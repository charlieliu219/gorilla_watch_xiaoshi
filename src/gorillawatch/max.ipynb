{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joscha Model (VIT Large Dinov2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from timm import create_model\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import torch\n",
    "from timm import create_model\n",
    "import timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to model and data\n",
    "\n",
    "joscha_checkpoint_path = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/models/vit_large_dinov2_ssl_joscha.ckpt\"\n",
    "robert_checkpoint_path = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/models/supervised_dinov2_large.ckpt\"\n",
    "vincent_checkpoint_path = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/models/ssl_vincent_vit_large.ckpt\"\n",
    "\n",
    "#checkpoint_path = robert_checkpoint_path\n",
    "test_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_all_split_60-25-15/test\"\n",
    "train_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_all_split_60-25-15/train\"\n",
    "val_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_all_split_60-25-15/val\"\n",
    "\n",
    "all_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_all_face\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joscha Laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_157409/2813075131.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda'))\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = joscha_checkpoint_path\n",
    "# Initialize the ViT model\n",
    "vit_model = create_model('vit_large_patch14_dinov2.lvd142m', pretrained=False, img_size=192)\n",
    "\n",
    "# Load checkpoint and extract state_dict\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda'))\n",
    "state_dict = checkpoint.get('state_dict', checkpoint)  # Get 'state_dict' or use checkpoint directly if no wrapper exists\n",
    "\n",
    "# Adjust the keys if necessary (remove any prefix like 'model.')\n",
    "new_state_dict = {k.replace('model_wrapper.', ''): v for k, v in state_dict.items()}\n",
    "new_state_dict2 = {k.replace('model.', ''): v for k, v in new_state_dict.items()}\n",
    "\n",
    "# Filter out unexpected keys from the state_dict\n",
    "model_keys = set(vit_model.state_dict().keys())\n",
    "filtered_state_dict = {k: v for k, v in new_state_dict2.items() if k in model_keys}\n",
    "\n",
    "# Load the filtered state_dict into the model\n",
    "vit_model.load_state_dict(filtered_state_dict, strict=True)\n",
    "vit_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Preprocessing function to resize and normalize images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((192, 192)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Adjust based on model pretraining\n",
    "])\n",
    "\n",
    "# Load images and extract class codes from file names\n",
    "def load_images_and_labels(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            image = cv2.imread(os.path.join(folder, filename))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = transform(image)\n",
    "            images.append(image)\n",
    "            labels.append(filename[:4])  # Assuming first 4 chars are label\n",
    "    return images, labels\n",
    "\n",
    "# Load data\n",
    "data_folder = test_folder\n",
    "images, labels = load_images_and_labels(data_folder)\n",
    "images_tensor = torch.stack(images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robert Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_157409/3682386049.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolating pos_embed from torch.Size([1, 257, 1024]) to torch.Size([1, 170, 1024])\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = robert_checkpoint_path\n",
    "# Initialize the ViT model\n",
    "vit_model = create_model('vit_large_patch14_dinov2.lvd142m', pretrained=False, img_size=192)\n",
    "\n",
    "# Load checkpoint and extract state_dict\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda'))\n",
    "state_dict = checkpoint.get('state_dict', checkpoint)  # Get 'state_dict' or use checkpoint directly if no wrapper exists\n",
    "\n",
    "# Adjust the keys if necessary (remove any prefix like 'model.')\n",
    "new_state_dict = {k.replace('model_wrapper.', ''): v for k, v in state_dict.items()}\n",
    "new_state_dict2 = {k.replace('model.', ''): v for k, v in new_state_dict.items()}\n",
    "\n",
    "# Interpolate positional embeddings if size mismatch\n",
    "if 'pos_embed' in new_state_dict2:\n",
    "    pos_embed_checkpoint = new_state_dict2['pos_embed']\n",
    "    pos_embed_model = vit_model.state_dict()['pos_embed']\n",
    "    if pos_embed_checkpoint.shape != pos_embed_model.shape:\n",
    "        print(f\"Interpolating pos_embed from {pos_embed_checkpoint.shape} to {pos_embed_model.shape}\")\n",
    "        num_patches = pos_embed_model.shape[1] - 1  # Exclude class token\n",
    "        class_pos_embed = pos_embed_checkpoint[:, :1, :]  # Class token\n",
    "        patch_pos_embed = pos_embed_checkpoint[:, 1:, :]  # Patch tokens\n",
    "\n",
    "        # Reshape and interpolate patch embeddings\n",
    "        patch_pos_embed = patch_pos_embed.reshape(1, int(patch_pos_embed.size(1)**0.5), -1, patch_pos_embed.size(-1))\n",
    "        patch_pos_embed = torch.nn.functional.interpolate(\n",
    "            patch_pos_embed.permute(0, 3, 1, 2),  # Convert to NCHW for interpolation\n",
    "            size=(int(num_patches**0.5), int(num_patches**0.5)),  # Target size\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        ).permute(0, 2, 3, 1).reshape(1, num_patches, -1)  # Back to NHWC\n",
    "\n",
    "        # Concatenate class token and interpolated patch embeddings\n",
    "        new_pos_embed = torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n",
    "        new_state_dict2['pos_embed'] = new_pos_embed\n",
    "\n",
    "\n",
    "# # Filter out unexpected keys from the state_dict\n",
    "# model_keys = set(vit_model.state_dict().keys())\n",
    "# filtered_state_dict = {k: v for k, v in new_state_dict2.items() if k in model_keys}\n",
    "\n",
    "# Load the filtered state_dict into the model\n",
    "vit_model.load_state_dict(filtered_state_dict, strict=True)\n",
    "vit_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Preprocessing function to resize and normalize images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((192, 192)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Adjust based on model pretraining\n",
    "])\n",
    "\n",
    "# Load images and extract class codes from file names\n",
    "def load_images_and_labels(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            image = cv2.imread(os.path.join(folder, filename))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = transform(image)\n",
    "            images.append(image)\n",
    "            labels.append(filename[:4])  # Assuming first 4 chars are label\n",
    "    return images, labels\n",
    "\n",
    "# Load data\n",
    "images, labels = load_images_and_labels(data_folder)\n",
    "images_tensor = torch.stack(images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Accuracy: 0.0909\n"
     ]
    }
   ],
   "source": [
    "# Function to generate embeddings from a model\n",
    "def generate_embeddings(model, images_tensor):\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.forward_features(images_tensor)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "# Generate and flatten embeddings for ViT\n",
    "vit_embeddings = generate_embeddings(vit_model, images_tensor)\n",
    "vit_embeddings_flat = vit_embeddings.view(vit_embeddings.size(0), -1).numpy()\n",
    "\n",
    "# Train-test split and train KNN on ViT embeddings\n",
    "vit_X_train, vit_X_test, vit_y_train, vit_y_test = train_test_split(vit_embeddings_flat, labels, test_size=0.2, random_state=100)\n",
    "vit_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "vit_knn.fit(vit_X_train, vit_y_train)\n",
    "vit_y_pred = vit_knn.predict(vit_X_test)\n",
    "vit_accuracy = accuracy_score(vit_y_test, vit_y_pred)\n",
    "print(f'ViT Accuracy: {vit_accuracy:.4f}')\n",
    "\n",
    "# Leave-One-Out Cross-Validation KNN Classification with progress bar\n",
    "def leave_one_out_knn_classification(model, model_name, images_tensor, labels):\n",
    "    print(f\"Using model: {model_name}\")\n",
    "    embeddings = generate_embeddings(model, images_tensor)\n",
    "    embeddings_flat = embeddings.view(embeddings.size(0), -1).numpy()\n",
    "    loo = LeaveOneOut()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for train_index, test_index in tqdm(loo.split(embeddings_flat), desc=\"Leave-One-Out CV\", total=len(embeddings_flat), unit=\"sample\"):\n",
    "        X_train, X_test = embeddings_flat[train_index], embeddings_flat[test_index]\n",
    "        y_train, y_test = np.array(labels)[train_index], np.array(labels)[test_index]\n",
    "        knn = KNeighborsClassifier(n_neighbors=5)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_test_pred = knn.predict(X_test)\n",
    "        y_true.append(y_test[0])\n",
    "        y_pred.append(y_test_pred[0])\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Leave-One-Out Cross-Validation Accuracy for {model_name}: {accuracy:.4f}')\n",
    "    return accuracy\n",
    "\n",
    "# Perform Leave-One-Out KNN Classification on ViT model\n",
    "#leave_one_out_knn_classification(vit_model, 'ViT', images_tensor, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letztes mal:\n",
    "\n",
    "ViT Accuracy: 0.6667 (Mit test_size 0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
