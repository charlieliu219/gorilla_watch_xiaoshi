{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are 2 approaches possible:\n",
    "\n",
    "1. Load the checkpoints into a standard ViT model and ignore all the extra layers \n",
    "\n",
    "2. Define a custom ViT model by wrapping a base ViT in layers corresponding to the checkpoints (weights that are named \"loss_module_*\" are ignored because they're mostly likely only for training and I don't know what kind of layers fit them)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from timm import create_model\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import torch\n",
    "from timm import create_model\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to model and data\n",
    "\n",
    "joscha_checkpoint_path = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/models/vit_large_dinov2_ssl_joscha.ckpt\"\n",
    "robert_checkpoint_path = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/models/supervised_dinov2_large.ckpt\"\n",
    "vincent_checkpoint_path = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/models/ssl_vincent_vit_large.ckpt\"\n",
    "\n",
    "model_name_joscha = \"vit_large_dinov2_ssl_joscha\"\n",
    "model_name_robert = \"supervised_dinov2_large\"\n",
    "model_name_vincent = \"ssl_vincent_vit_large\"\n",
    "\n",
    "#checkpoint_path = robert_checkpoint_path\n",
    "test_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_all_split_60-25-15/test\"\n",
    "train_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_all_split_60-25-15/train\"\n",
    "val_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_all_split_60-25-15/val\"\n",
    "\n",
    "all_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_all_face\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect checkpoints in comparison to a standard ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_231414/2987211072.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_joscha = torch.load(joscha_checkpoint_path, map_location=torch.device(device))\n",
      "/tmp/ipykernel_231414/2987211072.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_robert = torch.load(robert_checkpoint_path, map_location=torch.device(device))\n",
      "/tmp/ipykernel_231414/2987211072.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_vincent = torch.load(vincent_checkpoint_path, map_location=torch.device(device))\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint and extract state_dict\n",
    "checkpoint_joscha = torch.load(joscha_checkpoint_path, map_location=torch.device(device))\n",
    "checkpoint_robert = torch.load(robert_checkpoint_path, map_location=torch.device(device))\n",
    "checkpoint_vincent = torch.load(vincent_checkpoint_path, map_location=torch.device(device))\n",
    "\n",
    "# print(checkpoint[\"hyper_parameters\"])\n",
    "# print(checkpoint[\"hparams_name\"])\n",
    "# print(checkpoint_robert[\"state_dict\"].keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys in standard ViT (Joscha): ['embedding_layer.0.weight', 'embedding_layer.0.bias', 'embedding_layer.0.running_mean', 'embedding_layer.0.running_var', 'embedding_layer.0.num_batches_tracked', 'embedding_layer.2.weight', 'embedding_layer.2.bias', 'embedding_layer.3.weight', 'embedding_layer.3.bias', 'embedding_layer.3.running_mean', 'embedding_layer.3.running_var', 'embedding_layer.3.num_batches_tracked']\n",
      "Missing keys in standard ViT (Robert): ['embedding_layer.weight', 'embedding_layer.bias']\n",
      "Missing keys in standard ViT (Vincent): ['embedding_layer.0.weight', 'embedding_layer.0.bias', 'embedding_layer.0.running_mean', 'embedding_layer.0.running_var', 'embedding_layer.0.num_batches_tracked', 'embedding_layer.2.weight', 'embedding_layer.2.bias', 'embedding_layer.3.weight', 'embedding_layer.3.bias', 'embedding_layer.3.running_mean', 'embedding_layer.3.running_var', 'embedding_layer.3.num_batches_tracked']\n"
     ]
    }
   ],
   "source": [
    "# ### compare checkpoints' state_dict with a standard model's state_dict\n",
    "\n",
    "def extract_clean_state_dict(checkpoint, wrapper_key=\"model_wrapper.\", model_key=\"model.\"):\n",
    "    # Extract the state_dict from the checkpoint\n",
    "    state_dict = checkpoint.get('state_dict', checkpoint)  # Use 'state_dict' or checkpoint directly\n",
    "    # Remove wrapper key prefix\n",
    "    cleaned_state_dict = {k.replace(wrapper_key, ''): v for k, v in state_dict.items()}\n",
    "    # Remove model key prefix\n",
    "    cleaned_state_dict = {k.replace(model_key, ''): v for k, v in cleaned_state_dict.items()}\n",
    "    \n",
    "    return cleaned_state_dict\n",
    "    \n",
    "def compare_checkpoints_with_model(checkpoint, model_fn, model_name, img_size):\n",
    "    # # Extract the state_dict from the checkpoint\n",
    "    # state_dict = checkpoint.get('state_dict', checkpoint)  # Use 'state_dict' or checkpoint directly\n",
    "    # new_state_dict = {k.replace('model_wrapper.', ''): v for k, v in state_dict.items()}\n",
    "    # new_state_dict2 = {k.replace('model.', ''): v for k, v in new_state_dict.items()}\n",
    "    \n",
    "    cleaned_state_dict = extract_clean_state_dict(checkpoint)\n",
    "\n",
    "    # Initialize the model\n",
    "    vit_model = model_fn(model_name, pretrained=False, img_size=img_size)\n",
    "\n",
    "    # Compare keys\n",
    "    standard_vit_keys = vit_model.state_dict().keys()\n",
    "    custom_keys = cleaned_state_dict.keys()\n",
    "    missing_in_standard = [k for k in custom_keys if k not in standard_vit_keys]\n",
    "    extra_in_standard = [k for k in standard_vit_keys if k not in custom_keys]\n",
    "\n",
    "    # Collect information about embedding keys\n",
    "    embedding_keys = [key for key in cleaned_state_dict.keys() if \"embedding_layer\" in key]\n",
    "\n",
    "    # Print missing keys and their shapes\n",
    "    # for key in missing_in_standard:\n",
    "        # print(f\"{key}: {new_state_dict2[key].shape}\")\n",
    "\n",
    "    return {\n",
    "        \"missing_in_standard\": missing_in_standard,\n",
    "        \"extra_in_standard\": extra_in_standard,\n",
    "        \"embedding_keys\": embedding_keys,\n",
    "    }\n",
    "        \n",
    "    \n",
    "# Compare the checkpoint with the standard ViT model\n",
    "comparison_joscha = compare_checkpoints_with_model(\n",
    "    checkpoint=checkpoint_joscha,\n",
    "    model_fn=create_model,\n",
    "    model_name='vit_large_patch14_dinov2.lvd142m',\n",
    "    img_size=192\n",
    ")\n",
    "comparison_robert = compare_checkpoints_with_model(\n",
    "    checkpoint=checkpoint_robert,\n",
    "    model_fn=create_model,\n",
    "    model_name='vit_large_patch14_dinov2.lvd142m',\n",
    "    img_size=192\n",
    ")\n",
    "comparison_vincent = compare_checkpoints_with_model(\n",
    "    checkpoint=checkpoint_vincent,\n",
    "    model_fn=create_model,\n",
    "    model_name='vit_large_patch14_dinov2.lvd142m',\n",
    "    img_size=192\n",
    ")\n",
    "\n",
    "# print(\"Missing keys in standard ViT (Joscha):\", comparison_joscha)\n",
    "# print(\"Missing keys in standard ViT (Robert):\", comparison_robert)\n",
    "# print(\"Missing keys in standard ViT (Vincent):\", comparison_vincent)\n",
    "\n",
    "\n",
    "def filter_missing_keys(keys, filter_keywords):\n",
    "    return [key for key in keys if not any(keyword in key for keyword in filter_keywords)]\n",
    "\n",
    "\n",
    "# Define the keywords to filter out\n",
    "filter_keywords = [\"loss_module_val\", \"loss_module_train\"]\n",
    "\n",
    "# Apply filtering to each comparison\n",
    "comparison_joscha_missing_keys = filter_missing_keys(comparison_joscha[\"missing_in_standard\"], filter_keywords)\n",
    "comparison_robert_missing_keys = filter_missing_keys(comparison_robert[\"missing_in_standard\"], filter_keywords)\n",
    "comparison_vincent_missing_keys = filter_missing_keys(comparison_vincent[\"missing_in_standard\"], filter_keywords)\n",
    "\n",
    "# Print filtered results\n",
    "print(\"Missing keys in standard ViT (Joscha):\", comparison_joscha_missing_keys)\n",
    "print(\"Missing keys in standard ViT (Robert):\", comparison_robert_missing_keys)\n",
    "print(\"Missing keys in standard ViT (Vincent):\", comparison_vincent_missing_keys)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions used by both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix pos_emd mismatch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def interpolate_positional_embedding(pretrained_state_dict, current_model, patch_size):\n",
    "    # Get the pretrained positional embeddings\n",
    "    pretrained_pos_embed = pretrained_state_dict['pos_embed']\n",
    "    current_pos_embed = current_model.state_dict()['pos_embed']\n",
    "    \n",
    "    # Exclude the CLS token (first token) if present\n",
    "    cls_token = pretrained_pos_embed[:, :1, :]\n",
    "    pretrained_grid = pretrained_pos_embed[:, 1:, :]\n",
    "    \n",
    "    # Get the grid dimensions\n",
    "    num_patches = current_pos_embed.shape[1] - 1  # Exclude CLS token\n",
    "    grid_size_pretrained = int((pretrained_grid.shape[1])**0.5)  # sqrt(num_patches)\n",
    "    grid_size_current = int(num_patches**0.5)\n",
    "\n",
    "    # Reshape the positional embeddings to a grid\n",
    "    pretrained_grid = pretrained_grid.reshape(1, grid_size_pretrained, grid_size_pretrained, -1).permute(0, 3, 1, 2)\n",
    "    \n",
    "    # Interpolate to the current grid size\n",
    "    interpolated_grid = F.interpolate(pretrained_grid, size=(grid_size_current, grid_size_current), mode='bilinear', align_corners=False)\n",
    "    interpolated_grid = interpolated_grid.permute(0, 2, 3, 1).reshape(1, -1, interpolated_grid.shape[1])\n",
    "    \n",
    "    # Combine CLS token and the new grid\n",
    "    new_pos_embed = torch.cat([cls_token, interpolated_grid], dim=1)\n",
    "    pretrained_state_dict['pos_embed'] = new_pos_embed\n",
    "\n",
    "    return pretrained_state_dict\n",
    "\n",
    "\n",
    "# interpolate_positional_embedding(new_state_dict2, vit_model, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract cleaned state_dict from checkpoint\n",
    "cleaned_state_dict_joscha = extract_clean_state_dict(checkpoint_joscha, wrapper_key=\"model_wrapper.\", model_key=\"model.\")\n",
    "cleaned_state_dict_robert = extract_clean_state_dict(checkpoint_robert, wrapper_key=\"model_wrapper.\", model_key=\"model.\")\n",
    "cleaned_state_dict_vincent = extract_clean_state_dict(checkpoint_vincent, wrapper_key=\"model_wrapper.\", model_key=\"model.\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# Preprocessing function to resize and normalize images\n",
    "transform_custom = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Adjust based on model pretraining\n",
    "])\n",
    "\n",
    "transform_standard = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((192, 192)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Adjust based on model pretraining\n",
    "])\n",
    "\n",
    "# Load images and extract class codes from file names\n",
    "def load_images_and_labels(folder, transform):\n",
    "    images = []\n",
    "    labels = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            image = cv2.imread(os.path.join(folder, filename))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = transform(image)\n",
    "            images.append(image)\n",
    "            labels.append(filename[:4])  # Assuming first 4 chars are label\n",
    "            \n",
    "            filenames.append(filename)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to generate embeddings from a model\n",
    "def generate_embeddings(model, images_tensor):\n",
    "    with torch.no_grad():\n",
    "        # Call the forward method to extract embeddings\n",
    "        embeddings = model(images_tensor)\n",
    "        embeddings_flat = embeddings.view(embeddings.size(0), -1).numpy() # Flatten embeddings(optional?)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# KNN classifier and excluding the closest neighbor (self-matching)\n",
    "def knn_classifier(embeddings, labels):\n",
    "    for k in range(2, 6):\n",
    "        # Use k+1 neighbors to account for excluding self\n",
    "        vit_knn = KNeighborsClassifier(n_neighbors=k + 1)\n",
    "        vit_knn.fit(embeddings, labels)  # Fit KNN on all data\n",
    "\n",
    "        vit_y_pred = []\n",
    "        for idx, test_embedding in enumerate(embeddings):\n",
    "            # Find k+1 neighbors (including the test sample itself)\n",
    "            neighbors = vit_knn.kneighbors(test_embedding.reshape(1, -1), return_distance=False)[0]\n",
    "\n",
    "            # Exclude the test sample itself (assumed to be the closest neighbor)\n",
    "            filtered_neighbors = neighbors[1:]  # Exclude the first neighbor\n",
    "        \n",
    "            # Predict based on the remaining neighbors (majority vote)\n",
    "            filtered_neighbor_labels = [labels[n] for n in filtered_neighbors]\n",
    "            predicted_label = max(set(filtered_neighbor_labels), key=filtered_neighbor_labels.count)\n",
    "            vit_y_pred.append(predicted_label)\n",
    "\n",
    "        # Calculate accuracy for the current k\n",
    "        vit_accuracy = accuracy_score(labels, vit_y_pred)\n",
    "        print(f\"ViT Accuracy with {k}-nearest neighbors (excluding self): {vit_accuracy:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: load the chcekpoints into a standard ViT moel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Approach 1: load the chcekpoints into a standard ViT moel\n",
    "\n",
    "# vit_model_joscha = create_model('vit_large_patch14_dinov2.lvd142m', pretrained=False, img_size=192)\n",
    "# vit_model_robert = create_model('vit_large_patch14_dinov2.lvd142m', pretrained=False, img_size=192)\n",
    "# vit_model_vincent = create_model('vit_large_patch14_dinov2.lvd142m', pretrained=False, img_size=192)\n",
    "\n",
    "# # interpolate pos_embeddings\n",
    "# interpolated_state_dict_joscha = interpolate_positional_embedding(cleaned_state_dict_joscha, vit_model_joscha, 16)\n",
    "# interpolated_state_dict_robert = interpolate_positional_embedding(cleaned_state_dict_robert, vit_model_robert, 16)\n",
    "# interpolated_state_dict_vincent = interpolate_positional_embedding(cleaned_state_dict_vincent, vit_model_vincent, 16)\n",
    "\n",
    "# vit_model_joscha.load_state_dict(interpolated_state_dict_joscha, strict=False)\n",
    "# vit_model_robert.load_state_dict(interpolated_state_dict_robert, strict=False)\n",
    "# vit_model_vincent.load_state_dict(interpolated_state_dict_vincent, strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the data\n",
    "# # Load data\n",
    "# test_images, test_labels = load_images_and_labels(test_folder, transform_standard)\n",
    "# # train_images, train_labels = load_images_and_labels(train_folder)\n",
    "# # val_images, val_labels = load_images_and_labels(val_folder)\n",
    "# # all_images, all_labels = load_images_and_labels(all_folder)\n",
    "\n",
    "# test_images_tensor = torch.stack(test_images)\n",
    "# # train_images_tensor = torch.stack(train_images)\n",
    "# # val_images_tensor = torch.stack(val_images)\n",
    "# # all_images_tensor = torch.stack(all_images)\n",
    "\n",
    "# # train_val_images_tensor = torch.cat((train_images_tensor, val_images_tensor), 0)\n",
    "# # train_val_labels = train_labels + val_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate with standard models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vit_model_joscha.eval()\n",
    "# vit_model_robert.eval()\n",
    "# vit_model_vincent.eval()\n",
    "\n",
    "# standard_embeddings_joscha = generate_embeddings(vit_model_joscha, test_images_tensor)\n",
    "# stadard_embeddings_robert = generate_embeddings(vit_model_robert, test_images_tensor)\n",
    "# standard_embeddings_vincent = generate_embeddings(vit_model_vincent, test_images_tensor)\n",
    "\n",
    "# def classify_and_print(model_name, model, embeddings,labels):\n",
    "#     print(f\"{model_name}:\")\n",
    "#     knn_classifier(embeddings, labels)\n",
    "\n",
    "# classify_and_print(model_name_joscha, vit_model_joscha, standard_embeddings_joscha, test_labels)\n",
    "# classify_and_print(model_name_robert, vit_model_robert, stadard_embeddings_robert, test_labels)\n",
    "# classify_and_print(model_name_vincent, vit_model_vincent, standard_embeddings_vincent, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Define custom ViT models for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom ViT for supervised_dinov2_large\n",
    "\n",
    "class CustomViT_supervised(nn.Module):\n",
    "    def __init__(self, base_vit):\n",
    "        super(CustomViT_supervised, self).__init__()\n",
    "        self.base_vit = base_vit\n",
    "        \n",
    "        # Define embedding layer to match the missing keys\n",
    "        self.embedding_layer = nn.Linear(1024, 256)  # Assuming base_vit outputs 768-d embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the Vision Transformer backbone\n",
    "        x = self.base_vit.forward_features(x)  # Output size is [batch_size, num_tokens, 768]\n",
    "        print(f\"Output shape of base_vit: {x.shape}\")\n",
    "        \n",
    "        # Extract the [CLS] token embedding (assuming it's the first token)\n",
    "        x = x[:, 0, :]  # Shape: [batch_size, 768]\n",
    "        print(f\"Output shape after selecting CLS token: {x.shape}\")\n",
    "        \n",
    "        # Pass through the custom embedding layer\n",
    "        x = self.embedding_layer(x)  # Linear transformation to 1024\n",
    "        print(f\"Output shape of embedding_layer: {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_into_custom_vit_supervised(checkpoint_state_dict, custom_vit, patch_size=16):\n",
    "    \"\"\"\n",
    "    Loads a checkpoint's state_dict into a custom Vision Transformer (ViT) model.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_state_dict (dict): The state_dict from the checkpoint.\n",
    "        custom_vit (torch.nn.Module): The custom ViT model with backbone and embedding layers.\n",
    "        patch_size (int): Patch size for the ViT model, used for positional embedding interpolation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Separate backbone (base_vit) and embedding layer weights\n",
    "    backbone_state_dict = {k: v for k, v in checkpoint_state_dict.items() if \"embedding_layer\" not in k}\n",
    "    embedding_layer_state_dict = {k: v for k, v in checkpoint_state_dict.items() if \"embedding_layer\" in k}\n",
    "\n",
    "    # Interpolate positional embeddings if necessary\n",
    "    if \"pos_embed\" in backbone_state_dict:\n",
    "        num_patches = (custom_vit.base_vit.patch_embed.num_patches + 1)  # +1 for the CLS token\n",
    "        num_pos_tokens = backbone_state_dict[\"pos_embed\"].shape[1]\n",
    "        if num_pos_tokens != num_patches:\n",
    "            print(f\"Interpolating positional embeddings from {num_pos_tokens} to {num_patches}...\")\n",
    "            pos_embed = backbone_state_dict[\"pos_embed\"]\n",
    "            cls_token, pos_tokens = pos_embed[:, :1, :], pos_embed[:, 1:, :]  # Split CLS token and patch embeddings\n",
    "            pos_tokens = pos_tokens.view(1, int(num_pos_tokens**0.5), int(num_pos_tokens**0.5), -1)\n",
    "            new_size = int(num_patches**0.5)\n",
    "            pos_tokens = F.interpolate(pos_tokens, size=(new_size, new_size), mode=\"bicubic\", align_corners=False)\n",
    "            pos_tokens = pos_tokens.view(1, -1, pos_tokens.shape[-1])\n",
    "            backbone_state_dict[\"pos_embed\"] = torch.cat([cls_token, pos_tokens], dim=1)\n",
    "\n",
    "    # Load the backbone weights into base_vit\n",
    "    custom_vit.base_vit.load_state_dict(backbone_state_dict, strict=False)\n",
    "\n",
    "    # Load weights into the custom embedding layer\n",
    "    custom_vit.embedding_layer.load_state_dict({\n",
    "        \"weight\": embedding_layer_state_dict[\"embedding_layer.weight\"],\n",
    "        \"bias\": embedding_layer_state_dict[\"embedding_layer.bias\"],\n",
    "    })\n",
    "\n",
    "    print(\"Checkpoint loaded successfully into CustomViT_supervised!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom VIT for vit_large_dinov2_ssl_joscha & ssl_vincent_vit_large\n",
    "\n",
    "class CustomVisionTransformer_ssl(nn.Module):\n",
    "    def __init__(self, base_vit):\n",
    "        super(CustomVisionTransformer_ssl, self).__init__()\n",
    "        self.base_vit = base_vit\n",
    "        \n",
    "        # Define embedding layers based on checkpoint dimensions\n",
    "        self.embedding_layer_0 = nn.BatchNorm1d(1024)  # Normalize input size 1024\n",
    "        self.embedding_layer_2 = nn.Linear(1024, 256)  # Linear layer: 1024 -> 256\n",
    "        self.embedding_layer_3 = nn.BatchNorm1d(256)  # Normalize input size 256\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the Vision Transformer backbone\n",
    "        x = self.base_vit.forward_features(x)  # Output size is [batch_size, 1024]\n",
    "        print(f\"output shape of base_vit: {x.shape}\")\n",
    "        \n",
    "        # Flatten the patch and token dimensions into batch\n",
    "        # x = x.view(-1, x.size(-1))  # Shape: [batch_size * 257, 1024]\n",
    "        x = x[:, 0, :]  # Shape: [batch_size, 1024]\n",
    "\n",
    "        \n",
    "        # Pass through the additional embedding layers\n",
    "        x = self.embedding_layer_0(x)  # BatchNorm1d for 1024\n",
    "        print(f\"output shape of embedding_layer_0: {x.shape}\")\n",
    "        x = self.embedding_layer_2(x)  # Linear transformation to 256\n",
    "        print(f\"output shape of embedding_layer_2: {x.shape}\")\n",
    "        x = self.embedding_layer_3(x)  # BatchNorm1d for 256\n",
    "        print(f\"output shape of embedding_layer_3: {x.shape}\")\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_into_custom_vit_ssl(checkpoint_state_dict, custom_vit, patch_size=16):\n",
    "    \"\"\"\n",
    "    Loads a checkpoint's state_dict into a custom Vision Transformer (ViT) model.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_state_dict (dict): The state_dict from the checkpoint.\n",
    "        vit_model (torch.nn.Module): The base Vision Transformer model to adjust positional embeddings.\n",
    "        custom_vit (torch.nn.Module): The custom ViT model with backbone and embedding layers.\n",
    "        patch_size (int): Patch size for the ViT model, used for positional embedding interpolation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Separate backbone and embedding layers\n",
    "    backbone_state_dict = {k: v for k, v in checkpoint_state_dict.items() if \"embedding_layer\" not in k}\n",
    "    filtered_backbone_state_dict = {\n",
    "        k: v for k, v in backbone_state_dict.items() \n",
    "        if \"ls1.gamma\" not in k and \"ls2.gamma\" not in k\n",
    "    }\n",
    "\n",
    "    interpolated_backbone_state_dict = interpolate_positional_embedding(filtered_backbone_state_dict, custom_vit.base_vit, patch_size)\n",
    "\n",
    "    # Replace the key in the state_dict\n",
    "    filtered_backbone_state_dict[\"pos_embed\"] = interpolated_backbone_state_dict[\"pos_embed\"]\n",
    "\n",
    "    # Load weights into the backbone (base_vit)\n",
    "    custom_vit.base_vit.load_state_dict(filtered_backbone_state_dict, strict=False)\n",
    "\n",
    "    # Extract embedding layer weights\n",
    "    embedding_layer_state_dict = {k: v for k, v in checkpoint_state_dict.items() if \"embedding_layer\" in k}\n",
    "\n",
    "    # Load weights into the embedding layers\n",
    "    custom_vit.embedding_layer_0.load_state_dict({\n",
    "        \"weight\": embedding_layer_state_dict[\"embedding_layer.0.weight\"],\n",
    "        \"bias\": embedding_layer_state_dict[\"embedding_layer.0.bias\"],\n",
    "        \"running_mean\": embedding_layer_state_dict[\"embedding_layer.0.running_mean\"],\n",
    "        \"running_var\": embedding_layer_state_dict[\"embedding_layer.0.running_var\"],\n",
    "        \"num_batches_tracked\": embedding_layer_state_dict[\"embedding_layer.0.num_batches_tracked\"],\n",
    "    })\n",
    "    custom_vit.embedding_layer_2.load_state_dict({\n",
    "        \"weight\": embedding_layer_state_dict[\"embedding_layer.2.weight\"],\n",
    "        \"bias\": embedding_layer_state_dict[\"embedding_layer.2.bias\"],\n",
    "    })\n",
    "    custom_vit.embedding_layer_3.load_state_dict({\n",
    "        \"weight\": embedding_layer_state_dict[\"embedding_layer.3.weight\"],\n",
    "        \"bias\": embedding_layer_state_dict[\"embedding_layer.3.bias\"],\n",
    "        \"running_mean\": embedding_layer_state_dict[\"embedding_layer.3.running_mean\"],\n",
    "        \"running_var\": embedding_layer_state_dict[\"embedding_layer.3.running_var\"],\n",
    "        \"num_batches_tracked\": embedding_layer_state_dict[\"embedding_layer.3.num_batches_tracked\"],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the checkpoints' state_dict into the custom_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_vit = vit_model = create_model(\"vit_large_patch14_dinov2\", pretrained=False, img_size=192)\n",
    "\n",
    "custom_vit_joscha = CustomVisionTransformer_ssl(base_vit)\n",
    "custom_vit_vincent = CustomVisionTransformer_ssl(base_vit)\n",
    "custom_vit_robert = CustomViT_supervised(base_vit)\n",
    "\n",
    "### load the checkpoints' state_dict into the custom_vit\n",
    "load_checkpoint_into_custom_vit_ssl(cleaned_state_dict_joscha, custom_vit_joscha, patch_size=16)\n",
    "load_checkpoint_into_custom_vit_ssl(cleaned_state_dict_vincent, custom_vit_vincent, patch_size=16)\n",
    "# load_checkpoint_into_custom_vit_supervised(cleaned_state_dict_robert, custom_vit_robert, patch_size=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape of base_vit: torch.Size([54, 170, 1024])\n",
      "output shape of embedding_layer_0: torch.Size([54, 1024])\n",
      "output shape of embedding_layer_2: torch.Size([54, 256])\n",
      "output shape of embedding_layer_3: torch.Size([54, 256])\n",
      "output shape of base_vit: torch.Size([54, 170, 1024])\n",
      "output shape of embedding_layer_0: torch.Size([54, 1024])\n",
      "output shape of embedding_layer_2: torch.Size([54, 256])\n",
      "output shape of embedding_layer_3: torch.Size([54, 256])\n",
      "Output shape of base_vit: torch.Size([54, 170, 1024])\n",
      "Output shape after selecting CLS token: torch.Size([54, 1024])\n",
      "Output shape of embedding_layer: torch.Size([54, 256])\n",
      "ViT Accuracy with 2-nearest neighbors (excluding self): 0.3148\n",
      "ViT Accuracy with 3-nearest neighbors (excluding self): 0.3333\n",
      "ViT Accuracy with 4-nearest neighbors (excluding self): 0.3148\n",
      "ViT Accuracy with 5-nearest neighbors (excluding self): 0.3333\n",
      "ViT Accuracy with 2-nearest neighbors (excluding self): 0.3333\n",
      "ViT Accuracy with 3-nearest neighbors (excluding self): 0.3519\n",
      "ViT Accuracy with 4-nearest neighbors (excluding self): 0.3704\n",
      "ViT Accuracy with 5-nearest neighbors (excluding self): 0.3148\n"
     ]
    }
   ],
   "source": [
    "### generate embeddings from the custom_vit & KNN classification\n",
    "custom_vit_joscha.eval()\n",
    "custom_vit_vincent.eval()\n",
    "custom_vit_robert.eval()\n",
    "\n",
    "# create corresponding image tensors (224 instead of 192)\n",
    "test_images, test_labels = load_images_and_labels(test_folder, transform_standard)\n",
    "test_images_tensor = torch.stack(test_images)\n",
    "\n",
    "custom_embeddings_joscha = generate_embeddings(custom_vit_joscha, test_images_tensor)\n",
    "custom_embeddings_vincent = generate_embeddings(custom_vit_vincent, test_images_tensor)\n",
    "custom_embeddings_robert = generate_embeddings(custom_vit_robert, test_images_tensor)\n",
    "\n",
    "knn_classifier(custom_embeddings_joscha, test_labels)\n",
    "knn_classifier(custom_embeddings_vincent, test_labels)\n",
    "\n",
    "# classify_and_print(model_name_joscha, custom_vit_joscha, custom_embeddings_joscha, test_labels)\n",
    "# classify_and_print(model_name_vincent, custom_vit_vincent, custom_embeddings_vincent, test_labels)\n",
    "# classify_and_print(model_name_robert, custom_vit_robert, custom_embeddings_robert, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the data\n",
    "# # Preprocessing function to resize and normalize images\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.Resize((192, 192)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Adjust based on model pretraining\n",
    "# ])\n",
    "\n",
    "# # Load images and extract class codes from file names\n",
    "# def load_images_and_labels(folder):\n",
    "#     images = []\n",
    "#     labels = []\n",
    "#     filenames = []\n",
    "#     for filename in os.listdir(folder):\n",
    "#         if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "#             image = cv2.imread(os.path.join(folder, filename))\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#             image = transform(image)\n",
    "#             images.append(image)\n",
    "#             labels.append(filename[:4])  # Assuming first 4 chars are label\n",
    "            \n",
    "#             filenames.append(filename)\n",
    "    \n",
    "#     return images, labels\n",
    "\n",
    "# # Load data\n",
    "# test_images, test_labels = load_images_and_labels(test_folder)\n",
    "# # train_images, train_labels = load_images_and_labels(train_folder)\n",
    "# # val_images, val_labels = load_images_and_labels(val_folder)\n",
    "# # all_images, all_labels = load_images_and_labels(all_folder)\n",
    "\n",
    "# test_images_tensor = torch.stack(test_images)\n",
    "# # train_images_tensor = torch.stack(train_images)\n",
    "# # val_images_tensor = torch.stack(val_images)\n",
    "# # all_images_tensor = torch.stack(all_images)\n",
    "\n",
    "# # train_val_images_tensor = torch.cat((train_images_tensor, val_images_tensor), 0)\n",
    "# # train_val_labels = train_labels + val_labels\n",
    "\n",
    "\n",
    "# # robert_test_embeddings = generate_embeddings(vit_model_robert, test_images_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # try reducing smaples with insufficient data\n",
    "\n",
    "# # from collections import Counter\n",
    "# # threshold = 3\n",
    "\n",
    "# # # Count occurrences of each label\n",
    "# # label_counts = Counter(test_labels)\n",
    "# # print(f\"Number of images before filtering: {len(test_labels)}\")\n",
    "\n",
    "# # # \n",
    "# # # Filter embeddings and labels for individuals with >= 3 samples\n",
    "# # filtered_indices = [i for i, label in enumerate(test_labels) if label_counts[label] >= threshold]\n",
    "# # test_vit_embeddings_filtered = test_vit_embeddings_flat[filtered_indices]\n",
    "# # test_labels_filtered = [test_labels[i] for i in filtered_indices]\n",
    "\n",
    "# # # check the number of images after filtering\n",
    "# # print(f\"Number of images after filtering: {len(test_labels_filtered)}\")\n",
    "# # print(f\"Number of images after filtering: {len(test_vit_embeddings_filtered)}\")\n",
    "\n",
    "# from collections import Counter\n",
    "\n",
    "# def filter_samples_by_threshold(embeddings, labels, threshold=3):\n",
    "#     # Count occurrences of each label\n",
    "#     label_counts = Counter(labels)\n",
    "#     print(f\"Number of images before filtering: {len(labels)}\")\n",
    "\n",
    "#     # Filter embeddings and labels for labels with >= threshold occurrences\n",
    "#     filtered_indices = [i for i, label in enumerate(labels) if label_counts[label] >= threshold]\n",
    "#     filtered_embeddings = embeddings[filtered_indices]\n",
    "#     filtered_labels = [labels[i] for i in filtered_indices]\n",
    "\n",
    "#     # Log results\n",
    "#     print(f\"Number of images after filtering: {len(filtered_labels)}\")\n",
    "#     print(f\"Number of embeddings after filtering: {len(filtered_embeddings)}\")\n",
    "\n",
    "#     return filtered_embeddings, filtered_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# knn_classifier(all_vit_embeddings_reduced, all_labels, test_vit_embeddings_reduced, test_labels)\n",
    "# knn_classifier(all_vit_embeddings_filtered, all_labels_filtered, test_vit_embeddings_flat, test_labels)\n",
    "print(\"all test samples: joscha\")\n",
    "knn_classifier(test_vit_embeddings_flat, test_labels)\n",
    "print(\"all test samples: robert\")\n",
    "knn_classifier(robert_test_embeddings, test_labels)\n",
    "\n",
    "threshold = 3\n",
    "test_vit_embeddings_filtered, test_labels_filtered = filter_samples_by_threshold(\n",
    "    test_vit_embeddings_flat,\n",
    "    test_labels,\n",
    "    threshold=3\n",
    ")\n",
    "print(f\"filtered test samples (threshold = {threshold}):joscha\")\n",
    "knn_classifier(test_vit_embeddings_filtered, test_labels_filtered)\n",
    "\n",
    "robert_embeddings_filtered, robert_labels_filtered = filter_samples_by_threshold(\n",
    "    robert_test_embeddings,\n",
    "    test_labels,\n",
    "    threshold=3\n",
    ")\n",
    "print(f\"filtered test samples (threshold = {threshold}):robert\")\n",
    "knn_classifier(robert_embeddings_filtered, robert_labels_filtered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all test samples: joscha\n",
    "ViT Accuracy with 1-nearest neighbors (excluding self): 0.3704\n",
    "ViT Accuracy with 2-nearest neighbors (excluding self): 0.2963\n",
    "ViT Accuracy with 3-nearest neighbors (excluding self): 0.2222\n",
    "ViT Accuracy with 4-nearest neighbors (excluding self): 0.2407\n",
    "ViT Accuracy with 5-nearest neighbors (excluding self): 0.2222\n",
    "all test samples: robert\n",
    "ViT Accuracy with 1-nearest neighbors (excluding self): 0.8704\n",
    "ViT Accuracy with 2-nearest neighbors (excluding self): 0.7778\n",
    "ViT Accuracy with 3-nearest neighbors (excluding self): 0.7222\n",
    "ViT Accuracy with 4-nearest neighbors (excluding self): 0.6852\n",
    "ViT Accuracy with 5-nearest neighbors (excluding self): 0.6852\n",
    "Number of images before filtering: 54\n",
    "Number of images after filtering: 40\n",
    "Number of embeddings after filtering: 40\n",
    "filtered test samples (threshold = 3):joscha\n",
    "ViT Accuracy with 1-nearest neighbors (excluding self): 0.4000\n",
    "ViT Accuracy with 2-nearest neighbors (excluding self): 0.3250\n",
    "ViT Accuracy with 3-nearest neighbors (excluding self): 0.2750\n",
    "ViT Accuracy with 4-nearest neighbors (excluding self): 0.3250\n",
    "ViT Accuracy with 5-nearest neighbors (excluding self): 0.2500\n",
    "Number of images before filtering: 54\n",
    "Number of images after filtering: 40\n",
    "Number of embeddings after filtering: 40\n",
    "filtered test samples (threshold = 3):robert\n",
    "ViT Accuracy with 1-nearest neighbors (excluding self): 0.9750\n",
    "ViT Accuracy with 2-nearest neighbors (excluding self): 0.9500\n",
    "ViT Accuracy with 3-nearest neighbors (excluding self): 0.9750\n",
    "ViT Accuracy with 4-nearest neighbors (excluding self): 0.9250\n",
    "ViT Accuracy with 5-nearest neighbors (excluding self): 0.8750\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letztes mal:\n",
    "\n",
    "ViT Accuracy: 0.6667 (Mit test_size 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `all_vit_embeddings_flat` contains all embeddings\n",
    "# and `all_labels` contains corresponding labels\n",
    "\n",
    "# Using t-SNE to reduce dimensionality to 2D for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "# reduced_embeddings_tsne = tsne.fit_transform(all_vit_embeddings_flat)\n",
    "reduced_embeddings_tsne = tsne.fit_transform(test_vit_embeddings_flat)\n",
    "\n",
    "# Convert labels to numeric values for coloring\n",
    "unique_labels = list(set(test_labels))\n",
    "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "numeric_labels = [label_to_index[label] for label in test_labels]\n",
    "\n",
    "# Visualizing the reduced embeddings with a scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    reduced_embeddings_tsne[:, 0],\n",
    "    reduced_embeddings_tsne[:, 1],\n",
    "    c=numeric_labels,\n",
    "    cmap=\"tab10\",\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"k\"\n",
    ")\n",
    "plt.colorbar(scatter, ticks=range(len(unique_labels)), label=\"Classes\")\n",
    "plt.title(\"t-SNE Visualization of Embeddings\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
