{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from timm import create_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import defaultdict\n",
    "from wrappers_supervised import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8057/3892574982.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_best = torch.load(best_checkpoint_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# set the path to dataset to evaluate \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_checkpoint_path = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/models/should-be-the-best-model_vit_large_dinoV2.ckpt\"\n",
    "checkpoint_best = torch.load(best_checkpoint_path, map_location=device)\n",
    "\n",
    "# Path to folders\n",
    "test_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_faces_squared_openset_kfold-5/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For KNN-CV we only classify the filtered_filtered_images of individuals where there are at least 3 filtered_images of the same individual from a different video (Note that we still use images that do not fulfill this condition for classifying other images). When classifying we only classify an image using images from other videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and filter out the lsit of images that will be classified by cross-video KNN\n",
    "img_size = 224\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "class GorillaDataset(Dataset):\n",
    "    def __init__(self, folder, transform, threshold=4):\n",
    "        self.folder = folder\n",
    "        self.transform = transform\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        self.images = [] \n",
    "        self.labels = []\n",
    "        self.videos = []\n",
    "        \n",
    "        # keeps track of the idx of images for different KNN metric\n",
    "        self.images_for_standard_knn = []\n",
    "        self.images_for_cross_video_knn = []\n",
    "        \n",
    "        # the dataset of all images\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                image_path = os.path.join(folder, filename)\n",
    "                self.images.append(image_path)\n",
    "                label = filename.split(\"_\")[0] # Extract label from filename\n",
    "                self.labels.append(label)\n",
    "                video = filename.split(\"_\")[1] + \"_\" + filename.split(\"_\")[2]\n",
    "                self.videos.append(video)\n",
    "        \n",
    "        # Organize images by label and video for filtering\n",
    "        data_by_label = defaultdict(lambda: {\"images\": [], \"videos\": defaultdict(list)})\n",
    "        for image, label, video in zip(self.images, self.labels, self.videos):\n",
    "            data_by_label[label][\"images\"].append(image)\n",
    "            data_by_label[label][\"videos\"][video].append(image)\n",
    "        \n",
    "        # Find valid images for cross-video KNN and standard KNN\n",
    "        filtered_images = [] # to remove the classes with less than 4 images\n",
    "        filtered_labels = []\n",
    "        filtered_videos = []\n",
    "        for idx, (image, label, video) in enumerate(zip(self.images, self.labels, self.videos)):\n",
    "            # Check if the image's label has at least 3 images in other videos\n",
    "            videos_with_label = data_by_label[label][\"videos\"]\n",
    "            other_videos_count = sum(\n",
    "                len(images) for vid, images in videos_with_label.items() if vid != video\n",
    "            )\n",
    "            # if an image has more than 3 images in other videos under the same class, we will use it for cross-video KNN\n",
    "            if other_videos_count >= 3:\n",
    "                self.images_for_cross_video_knn.append(idx)\n",
    "                \n",
    "            # if a class has more than 4(threshold) images, we put it in filtered_images\n",
    "            if len(data_by_label[label][\"images\"]) >= threshold:\n",
    "                # self.images_for_standard_knn.append((idx, label))\n",
    "                self.images_for_standard_knn.append(idx)\n",
    "                filtered_images.append(image)\n",
    "                filtered_labels.append(label)\n",
    "                filtered_videos.append(video)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # generate embeddings for all the images (and only classify the ones that are in the valid_classes)\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        video = self.videos[idx]    \n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transform(image)\n",
    "        return image, label, video\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "test_dataset = GorillaDataset(test_folder, transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['loss_module_train.model.cls_token', 'loss_module_train.model.pos_embed', 'loss_module_train.model.patch_embed.proj.weight', 'loss_module_train.model.patch_embed.proj.bias', 'loss_module_train.model.blocks.0.norm1.weight', 'loss_module_train.model.blocks.0.norm1.bias', 'loss_module_train.model.blocks.0.attn.qkv.weight', 'loss_module_train.model.blocks.0.attn.qkv.bias', 'loss_module_train.model.blocks.0.attn.proj.weight', 'loss_module_train.model.blocks.0.attn.proj.bias', 'loss_module_train.model.blocks.0.ls1.gamma', 'loss_module_train.model.blocks.0.norm2.weight', 'loss_module_train.model.blocks.0.norm2.bias', 'loss_module_train.model.blocks.0.mlp.fc1.weight', 'loss_module_train.model.blocks.0.mlp.fc1.bias', 'loss_module_train.model.blocks.0.mlp.fc2.weight', 'loss_module_train.model.blocks.0.mlp.fc2.bias', 'loss_module_train.model.blocks.0.ls2.gamma', 'loss_module_train.model.blocks.1.norm1.weight', 'loss_module_train.model.blocks.1.norm1.bias', 'loss_module_train.model.blocks.1.attn.qkv.weight', 'loss_module_train.model.blocks.1.attn.qkv.bias', 'loss_module_train.model.blocks.1.attn.proj.weight', 'loss_module_train.model.blocks.1.attn.proj.bias', 'loss_module_train.model.blocks.1.ls1.gamma', 'loss_module_train.model.blocks.1.norm2.weight', 'loss_module_train.model.blocks.1.norm2.bias', 'loss_module_train.model.blocks.1.mlp.fc1.weight', 'loss_module_train.model.blocks.1.mlp.fc1.bias', 'loss_module_train.model.blocks.1.mlp.fc2.weight', 'loss_module_train.model.blocks.1.mlp.fc2.bias', 'loss_module_train.model.blocks.1.ls2.gamma', 'loss_module_train.model.blocks.2.norm1.weight', 'loss_module_train.model.blocks.2.norm1.bias', 'loss_module_train.model.blocks.2.attn.qkv.weight', 'loss_module_train.model.blocks.2.attn.qkv.bias', 'loss_module_train.model.blocks.2.attn.proj.weight', 'loss_module_train.model.blocks.2.attn.proj.bias', 'loss_module_train.model.blocks.2.ls1.gamma', 'loss_module_train.model.blocks.2.norm2.weight', 'loss_module_train.model.blocks.2.norm2.bias', 'loss_module_train.model.blocks.2.mlp.fc1.weight', 'loss_module_train.model.blocks.2.mlp.fc1.bias', 'loss_module_train.model.blocks.2.mlp.fc2.weight', 'loss_module_train.model.blocks.2.mlp.fc2.bias', 'loss_module_train.model.blocks.2.ls2.gamma', 'loss_module_train.model.blocks.3.norm1.weight', 'loss_module_train.model.blocks.3.norm1.bias', 'loss_module_train.model.blocks.3.attn.qkv.weight', 'loss_module_train.model.blocks.3.attn.qkv.bias', 'loss_module_train.model.blocks.3.attn.proj.weight', 'loss_module_train.model.blocks.3.attn.proj.bias', 'loss_module_train.model.blocks.3.ls1.gamma', 'loss_module_train.model.blocks.3.norm2.weight', 'loss_module_train.model.blocks.3.norm2.bias', 'loss_module_train.model.blocks.3.mlp.fc1.weight', 'loss_module_train.model.blocks.3.mlp.fc1.bias', 'loss_module_train.model.blocks.3.mlp.fc2.weight', 'loss_module_train.model.blocks.3.mlp.fc2.bias', 'loss_module_train.model.blocks.3.ls2.gamma', 'loss_module_train.model.blocks.4.norm1.weight', 'loss_module_train.model.blocks.4.norm1.bias', 'loss_module_train.model.blocks.4.attn.qkv.weight', 'loss_module_train.model.blocks.4.attn.qkv.bias', 'loss_module_train.model.blocks.4.attn.proj.weight', 'loss_module_train.model.blocks.4.attn.proj.bias', 'loss_module_train.model.blocks.4.ls1.gamma', 'loss_module_train.model.blocks.4.norm2.weight', 'loss_module_train.model.blocks.4.norm2.bias', 'loss_module_train.model.blocks.4.mlp.fc1.weight', 'loss_module_train.model.blocks.4.mlp.fc1.bias', 'loss_module_train.model.blocks.4.mlp.fc2.weight', 'loss_module_train.model.blocks.4.mlp.fc2.bias', 'loss_module_train.model.blocks.4.ls2.gamma', 'loss_module_train.model.blocks.5.norm1.weight', 'loss_module_train.model.blocks.5.norm1.bias', 'loss_module_train.model.blocks.5.attn.qkv.weight', 'loss_module_train.model.blocks.5.attn.qkv.bias', 'loss_module_train.model.blocks.5.attn.proj.weight', 'loss_module_train.model.blocks.5.attn.proj.bias', 'loss_module_train.model.blocks.5.ls1.gamma', 'loss_module_train.model.blocks.5.norm2.weight', 'loss_module_train.model.blocks.5.norm2.bias', 'loss_module_train.model.blocks.5.mlp.fc1.weight', 'loss_module_train.model.blocks.5.mlp.fc1.bias', 'loss_module_train.model.blocks.5.mlp.fc2.weight', 'loss_module_train.model.blocks.5.mlp.fc2.bias', 'loss_module_train.model.blocks.5.ls2.gamma', 'loss_module_train.model.blocks.6.norm1.weight', 'loss_module_train.model.blocks.6.norm1.bias', 'loss_module_train.model.blocks.6.attn.qkv.weight', 'loss_module_train.model.blocks.6.attn.qkv.bias', 'loss_module_train.model.blocks.6.attn.proj.weight', 'loss_module_train.model.blocks.6.attn.proj.bias', 'loss_module_train.model.blocks.6.ls1.gamma', 'loss_module_train.model.blocks.6.norm2.weight', 'loss_module_train.model.blocks.6.norm2.bias', 'loss_module_train.model.blocks.6.mlp.fc1.weight', 'loss_module_train.model.blocks.6.mlp.fc1.bias', 'loss_module_train.model.blocks.6.mlp.fc2.weight', 'loss_module_train.model.blocks.6.mlp.fc2.bias', 'loss_module_train.model.blocks.6.ls2.gamma', 'loss_module_train.model.blocks.7.norm1.weight', 'loss_module_train.model.blocks.7.norm1.bias', 'loss_module_train.model.blocks.7.attn.qkv.weight', 'loss_module_train.model.blocks.7.attn.qkv.bias', 'loss_module_train.model.blocks.7.attn.proj.weight', 'loss_module_train.model.blocks.7.attn.proj.bias', 'loss_module_train.model.blocks.7.ls1.gamma', 'loss_module_train.model.blocks.7.norm2.weight', 'loss_module_train.model.blocks.7.norm2.bias', 'loss_module_train.model.blocks.7.mlp.fc1.weight', 'loss_module_train.model.blocks.7.mlp.fc1.bias', 'loss_module_train.model.blocks.7.mlp.fc2.weight', 'loss_module_train.model.blocks.7.mlp.fc2.bias', 'loss_module_train.model.blocks.7.ls2.gamma', 'loss_module_train.model.blocks.8.norm1.weight', 'loss_module_train.model.blocks.8.norm1.bias', 'loss_module_train.model.blocks.8.attn.qkv.weight', 'loss_module_train.model.blocks.8.attn.qkv.bias', 'loss_module_train.model.blocks.8.attn.proj.weight', 'loss_module_train.model.blocks.8.attn.proj.bias', 'loss_module_train.model.blocks.8.ls1.gamma', 'loss_module_train.model.blocks.8.norm2.weight', 'loss_module_train.model.blocks.8.norm2.bias', 'loss_module_train.model.blocks.8.mlp.fc1.weight', 'loss_module_train.model.blocks.8.mlp.fc1.bias', 'loss_module_train.model.blocks.8.mlp.fc2.weight', 'loss_module_train.model.blocks.8.mlp.fc2.bias', 'loss_module_train.model.blocks.8.ls2.gamma', 'loss_module_train.model.blocks.9.norm1.weight', 'loss_module_train.model.blocks.9.norm1.bias', 'loss_module_train.model.blocks.9.attn.qkv.weight', 'loss_module_train.model.blocks.9.attn.qkv.bias', 'loss_module_train.model.blocks.9.attn.proj.weight', 'loss_module_train.model.blocks.9.attn.proj.bias', 'loss_module_train.model.blocks.9.ls1.gamma', 'loss_module_train.model.blocks.9.norm2.weight', 'loss_module_train.model.blocks.9.norm2.bias', 'loss_module_train.model.blocks.9.mlp.fc1.weight', 'loss_module_train.model.blocks.9.mlp.fc1.bias', 'loss_module_train.model.blocks.9.mlp.fc2.weight', 'loss_module_train.model.blocks.9.mlp.fc2.bias', 'loss_module_train.model.blocks.9.ls2.gamma', 'loss_module_train.model.blocks.10.norm1.weight', 'loss_module_train.model.blocks.10.norm1.bias', 'loss_module_train.model.blocks.10.attn.qkv.weight', 'loss_module_train.model.blocks.10.attn.qkv.bias', 'loss_module_train.model.blocks.10.attn.proj.weight', 'loss_module_train.model.blocks.10.attn.proj.bias', 'loss_module_train.model.blocks.10.ls1.gamma', 'loss_module_train.model.blocks.10.norm2.weight', 'loss_module_train.model.blocks.10.norm2.bias', 'loss_module_train.model.blocks.10.mlp.fc1.weight', 'loss_module_train.model.blocks.10.mlp.fc1.bias', 'loss_module_train.model.blocks.10.mlp.fc2.weight', 'loss_module_train.model.blocks.10.mlp.fc2.bias', 'loss_module_train.model.blocks.10.ls2.gamma', 'loss_module_train.model.blocks.11.norm1.weight', 'loss_module_train.model.blocks.11.norm1.bias', 'loss_module_train.model.blocks.11.attn.qkv.weight', 'loss_module_train.model.blocks.11.attn.qkv.bias', 'loss_module_train.model.blocks.11.attn.proj.weight', 'loss_module_train.model.blocks.11.attn.proj.bias', 'loss_module_train.model.blocks.11.ls1.gamma', 'loss_module_train.model.blocks.11.norm2.weight', 'loss_module_train.model.blocks.11.norm2.bias', 'loss_module_train.model.blocks.11.mlp.fc1.weight', 'loss_module_train.model.blocks.11.mlp.fc1.bias', 'loss_module_train.model.blocks.11.mlp.fc2.weight', 'loss_module_train.model.blocks.11.mlp.fc2.bias', 'loss_module_train.model.blocks.11.ls2.gamma', 'loss_module_train.model.blocks.12.norm1.weight', 'loss_module_train.model.blocks.12.norm1.bias', 'loss_module_train.model.blocks.12.attn.qkv.weight', 'loss_module_train.model.blocks.12.attn.qkv.bias', 'loss_module_train.model.blocks.12.attn.proj.weight', 'loss_module_train.model.blocks.12.attn.proj.bias', 'loss_module_train.model.blocks.12.ls1.gamma', 'loss_module_train.model.blocks.12.norm2.weight', 'loss_module_train.model.blocks.12.norm2.bias', 'loss_module_train.model.blocks.12.mlp.fc1.weight', 'loss_module_train.model.blocks.12.mlp.fc1.bias', 'loss_module_train.model.blocks.12.mlp.fc2.weight', 'loss_module_train.model.blocks.12.mlp.fc2.bias', 'loss_module_train.model.blocks.12.ls2.gamma', 'loss_module_train.model.blocks.13.norm1.weight', 'loss_module_train.model.blocks.13.norm1.bias', 'loss_module_train.model.blocks.13.attn.qkv.weight', 'loss_module_train.model.blocks.13.attn.qkv.bias', 'loss_module_train.model.blocks.13.attn.proj.weight', 'loss_module_train.model.blocks.13.attn.proj.bias', 'loss_module_train.model.blocks.13.ls1.gamma', 'loss_module_train.model.blocks.13.norm2.weight', 'loss_module_train.model.blocks.13.norm2.bias', 'loss_module_train.model.blocks.13.mlp.fc1.weight', 'loss_module_train.model.blocks.13.mlp.fc1.bias', 'loss_module_train.model.blocks.13.mlp.fc2.weight', 'loss_module_train.model.blocks.13.mlp.fc2.bias', 'loss_module_train.model.blocks.13.ls2.gamma', 'loss_module_train.model.blocks.14.norm1.weight', 'loss_module_train.model.blocks.14.norm1.bias', 'loss_module_train.model.blocks.14.attn.qkv.weight', 'loss_module_train.model.blocks.14.attn.qkv.bias', 'loss_module_train.model.blocks.14.attn.proj.weight', 'loss_module_train.model.blocks.14.attn.proj.bias', 'loss_module_train.model.blocks.14.ls1.gamma', 'loss_module_train.model.blocks.14.norm2.weight', 'loss_module_train.model.blocks.14.norm2.bias', 'loss_module_train.model.blocks.14.mlp.fc1.weight', 'loss_module_train.model.blocks.14.mlp.fc1.bias', 'loss_module_train.model.blocks.14.mlp.fc2.weight', 'loss_module_train.model.blocks.14.mlp.fc2.bias', 'loss_module_train.model.blocks.14.ls2.gamma', 'loss_module_train.model.blocks.15.norm1.weight', 'loss_module_train.model.blocks.15.norm1.bias', 'loss_module_train.model.blocks.15.attn.qkv.weight', 'loss_module_train.model.blocks.15.attn.qkv.bias', 'loss_module_train.model.blocks.15.attn.proj.weight', 'loss_module_train.model.blocks.15.attn.proj.bias', 'loss_module_train.model.blocks.15.ls1.gamma', 'loss_module_train.model.blocks.15.norm2.weight', 'loss_module_train.model.blocks.15.norm2.bias', 'loss_module_train.model.blocks.15.mlp.fc1.weight', 'loss_module_train.model.blocks.15.mlp.fc1.bias', 'loss_module_train.model.blocks.15.mlp.fc2.weight', 'loss_module_train.model.blocks.15.mlp.fc2.bias', 'loss_module_train.model.blocks.15.ls2.gamma', 'loss_module_train.model.blocks.16.norm1.weight', 'loss_module_train.model.blocks.16.norm1.bias', 'loss_module_train.model.blocks.16.attn.qkv.weight', 'loss_module_train.model.blocks.16.attn.qkv.bias', 'loss_module_train.model.blocks.16.attn.proj.weight', 'loss_module_train.model.blocks.16.attn.proj.bias', 'loss_module_train.model.blocks.16.ls1.gamma', 'loss_module_train.model.blocks.16.norm2.weight', 'loss_module_train.model.blocks.16.norm2.bias', 'loss_module_train.model.blocks.16.mlp.fc1.weight', 'loss_module_train.model.blocks.16.mlp.fc1.bias', 'loss_module_train.model.blocks.16.mlp.fc2.weight', 'loss_module_train.model.blocks.16.mlp.fc2.bias', 'loss_module_train.model.blocks.16.ls2.gamma', 'loss_module_train.model.blocks.17.norm1.weight', 'loss_module_train.model.blocks.17.norm1.bias', 'loss_module_train.model.blocks.17.attn.qkv.weight', 'loss_module_train.model.blocks.17.attn.qkv.bias', 'loss_module_train.model.blocks.17.attn.proj.weight', 'loss_module_train.model.blocks.17.attn.proj.bias', 'loss_module_train.model.blocks.17.ls1.gamma', 'loss_module_train.model.blocks.17.norm2.weight', 'loss_module_train.model.blocks.17.norm2.bias', 'loss_module_train.model.blocks.17.mlp.fc1.weight', 'loss_module_train.model.blocks.17.mlp.fc1.bias', 'loss_module_train.model.blocks.17.mlp.fc2.weight', 'loss_module_train.model.blocks.17.mlp.fc2.bias', 'loss_module_train.model.blocks.17.ls2.gamma', 'loss_module_train.model.blocks.18.norm1.weight', 'loss_module_train.model.blocks.18.norm1.bias', 'loss_module_train.model.blocks.18.attn.qkv.weight', 'loss_module_train.model.blocks.18.attn.qkv.bias', 'loss_module_train.model.blocks.18.attn.proj.weight', 'loss_module_train.model.blocks.18.attn.proj.bias', 'loss_module_train.model.blocks.18.ls1.gamma', 'loss_module_train.model.blocks.18.norm2.weight', 'loss_module_train.model.blocks.18.norm2.bias', 'loss_module_train.model.blocks.18.mlp.fc1.weight', 'loss_module_train.model.blocks.18.mlp.fc1.bias', 'loss_module_train.model.blocks.18.mlp.fc2.weight', 'loss_module_train.model.blocks.18.mlp.fc2.bias', 'loss_module_train.model.blocks.18.ls2.gamma', 'loss_module_train.model.blocks.19.norm1.weight', 'loss_module_train.model.blocks.19.norm1.bias', 'loss_module_train.model.blocks.19.attn.qkv.weight', 'loss_module_train.model.blocks.19.attn.qkv.bias', 'loss_module_train.model.blocks.19.attn.proj.weight', 'loss_module_train.model.blocks.19.attn.proj.bias', 'loss_module_train.model.blocks.19.ls1.gamma', 'loss_module_train.model.blocks.19.norm2.weight', 'loss_module_train.model.blocks.19.norm2.bias', 'loss_module_train.model.blocks.19.mlp.fc1.weight', 'loss_module_train.model.blocks.19.mlp.fc1.bias', 'loss_module_train.model.blocks.19.mlp.fc2.weight', 'loss_module_train.model.blocks.19.mlp.fc2.bias', 'loss_module_train.model.blocks.19.ls2.gamma', 'loss_module_train.model.blocks.20.norm1.weight', 'loss_module_train.model.blocks.20.norm1.bias', 'loss_module_train.model.blocks.20.attn.qkv.weight', 'loss_module_train.model.blocks.20.attn.qkv.bias', 'loss_module_train.model.blocks.20.attn.proj.weight', 'loss_module_train.model.blocks.20.attn.proj.bias', 'loss_module_train.model.blocks.20.ls1.gamma', 'loss_module_train.model.blocks.20.norm2.weight', 'loss_module_train.model.blocks.20.norm2.bias', 'loss_module_train.model.blocks.20.mlp.fc1.weight', 'loss_module_train.model.blocks.20.mlp.fc1.bias', 'loss_module_train.model.blocks.20.mlp.fc2.weight', 'loss_module_train.model.blocks.20.mlp.fc2.bias', 'loss_module_train.model.blocks.20.ls2.gamma', 'loss_module_train.model.blocks.21.norm1.weight', 'loss_module_train.model.blocks.21.norm1.bias', 'loss_module_train.model.blocks.21.attn.qkv.weight', 'loss_module_train.model.blocks.21.attn.qkv.bias', 'loss_module_train.model.blocks.21.attn.proj.weight', 'loss_module_train.model.blocks.21.attn.proj.bias', 'loss_module_train.model.blocks.21.ls1.gamma', 'loss_module_train.model.blocks.21.norm2.weight', 'loss_module_train.model.blocks.21.norm2.bias', 'loss_module_train.model.blocks.21.mlp.fc1.weight', 'loss_module_train.model.blocks.21.mlp.fc1.bias', 'loss_module_train.model.blocks.21.mlp.fc2.weight', 'loss_module_train.model.blocks.21.mlp.fc2.bias', 'loss_module_train.model.blocks.21.ls2.gamma', 'loss_module_train.model.blocks.22.norm1.weight', 'loss_module_train.model.blocks.22.norm1.bias', 'loss_module_train.model.blocks.22.attn.qkv.weight', 'loss_module_train.model.blocks.22.attn.qkv.bias', 'loss_module_train.model.blocks.22.attn.proj.weight', 'loss_module_train.model.blocks.22.attn.proj.bias', 'loss_module_train.model.blocks.22.ls1.gamma', 'loss_module_train.model.blocks.22.norm2.weight', 'loss_module_train.model.blocks.22.norm2.bias', 'loss_module_train.model.blocks.22.mlp.fc1.weight', 'loss_module_train.model.blocks.22.mlp.fc1.bias', 'loss_module_train.model.blocks.22.mlp.fc2.weight', 'loss_module_train.model.blocks.22.mlp.fc2.bias', 'loss_module_train.model.blocks.22.ls2.gamma', 'loss_module_train.model.blocks.23.norm1.weight', 'loss_module_train.model.blocks.23.norm1.bias', 'loss_module_train.model.blocks.23.attn.qkv.weight', 'loss_module_train.model.blocks.23.attn.qkv.bias', 'loss_module_train.model.blocks.23.attn.proj.weight', 'loss_module_train.model.blocks.23.attn.proj.bias', 'loss_module_train.model.blocks.23.ls1.gamma', 'loss_module_train.model.blocks.23.norm2.weight', 'loss_module_train.model.blocks.23.norm2.bias', 'loss_module_train.model.blocks.23.mlp.fc1.weight', 'loss_module_train.model.blocks.23.mlp.fc1.bias', 'loss_module_train.model.blocks.23.mlp.fc2.weight', 'loss_module_train.model.blocks.23.mlp.fc2.bias', 'loss_module_train.model.blocks.23.ls2.gamma', 'loss_module_train.model.norm.weight', 'loss_module_train.model.norm.bias', 'loss_module_train.l2sp_loss.cls_token', 'loss_module_train.l2sp_loss.patch_embed_proj_weight', 'loss_module_train.l2sp_loss.blocks_0_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_0_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_0_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_0_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_1_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_1_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_1_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_1_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_2_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_2_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_2_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_2_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_3_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_3_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_3_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_3_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_4_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_4_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_4_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_4_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_5_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_5_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_5_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_5_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_6_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_6_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_6_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_6_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_7_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_7_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_7_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_7_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_8_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_8_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_8_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_8_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_9_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_9_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_9_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_9_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_10_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_10_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_10_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_10_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_11_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_11_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_11_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_11_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_12_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_12_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_12_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_12_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_13_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_13_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_13_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_13_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_14_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_14_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_14_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_14_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_15_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_15_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_15_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_15_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_16_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_16_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_16_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_16_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_17_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_17_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_17_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_17_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_18_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_18_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_18_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_18_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_19_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_19_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_19_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_19_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_20_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_20_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_20_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_20_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_21_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_21_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_21_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_21_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_22_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_22_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_22_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_22_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_23_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_23_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_23_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_23_mlp_fc2_weight', 'loss_module_val.model.cls_token', 'loss_module_val.model.pos_embed', 'loss_module_val.model.patch_embed.proj.weight', 'loss_module_val.model.patch_embed.proj.bias', 'loss_module_val.model.blocks.0.norm1.weight', 'loss_module_val.model.blocks.0.norm1.bias', 'loss_module_val.model.blocks.0.attn.qkv.weight', 'loss_module_val.model.blocks.0.attn.qkv.bias', 'loss_module_val.model.blocks.0.attn.proj.weight', 'loss_module_val.model.blocks.0.attn.proj.bias', 'loss_module_val.model.blocks.0.ls1.gamma', 'loss_module_val.model.blocks.0.norm2.weight', 'loss_module_val.model.blocks.0.norm2.bias', 'loss_module_val.model.blocks.0.mlp.fc1.weight', 'loss_module_val.model.blocks.0.mlp.fc1.bias', 'loss_module_val.model.blocks.0.mlp.fc2.weight', 'loss_module_val.model.blocks.0.mlp.fc2.bias', 'loss_module_val.model.blocks.0.ls2.gamma', 'loss_module_val.model.blocks.1.norm1.weight', 'loss_module_val.model.blocks.1.norm1.bias', 'loss_module_val.model.blocks.1.attn.qkv.weight', 'loss_module_val.model.blocks.1.attn.qkv.bias', 'loss_module_val.model.blocks.1.attn.proj.weight', 'loss_module_val.model.blocks.1.attn.proj.bias', 'loss_module_val.model.blocks.1.ls1.gamma', 'loss_module_val.model.blocks.1.norm2.weight', 'loss_module_val.model.blocks.1.norm2.bias', 'loss_module_val.model.blocks.1.mlp.fc1.weight', 'loss_module_val.model.blocks.1.mlp.fc1.bias', 'loss_module_val.model.blocks.1.mlp.fc2.weight', 'loss_module_val.model.blocks.1.mlp.fc2.bias', 'loss_module_val.model.blocks.1.ls2.gamma', 'loss_module_val.model.blocks.2.norm1.weight', 'loss_module_val.model.blocks.2.norm1.bias', 'loss_module_val.model.blocks.2.attn.qkv.weight', 'loss_module_val.model.blocks.2.attn.qkv.bias', 'loss_module_val.model.blocks.2.attn.proj.weight', 'loss_module_val.model.blocks.2.attn.proj.bias', 'loss_module_val.model.blocks.2.ls1.gamma', 'loss_module_val.model.blocks.2.norm2.weight', 'loss_module_val.model.blocks.2.norm2.bias', 'loss_module_val.model.blocks.2.mlp.fc1.weight', 'loss_module_val.model.blocks.2.mlp.fc1.bias', 'loss_module_val.model.blocks.2.mlp.fc2.weight', 'loss_module_val.model.blocks.2.mlp.fc2.bias', 'loss_module_val.model.blocks.2.ls2.gamma', 'loss_module_val.model.blocks.3.norm1.weight', 'loss_module_val.model.blocks.3.norm1.bias', 'loss_module_val.model.blocks.3.attn.qkv.weight', 'loss_module_val.model.blocks.3.attn.qkv.bias', 'loss_module_val.model.blocks.3.attn.proj.weight', 'loss_module_val.model.blocks.3.attn.proj.bias', 'loss_module_val.model.blocks.3.ls1.gamma', 'loss_module_val.model.blocks.3.norm2.weight', 'loss_module_val.model.blocks.3.norm2.bias', 'loss_module_val.model.blocks.3.mlp.fc1.weight', 'loss_module_val.model.blocks.3.mlp.fc1.bias', 'loss_module_val.model.blocks.3.mlp.fc2.weight', 'loss_module_val.model.blocks.3.mlp.fc2.bias', 'loss_module_val.model.blocks.3.ls2.gamma', 'loss_module_val.model.blocks.4.norm1.weight', 'loss_module_val.model.blocks.4.norm1.bias', 'loss_module_val.model.blocks.4.attn.qkv.weight', 'loss_module_val.model.blocks.4.attn.qkv.bias', 'loss_module_val.model.blocks.4.attn.proj.weight', 'loss_module_val.model.blocks.4.attn.proj.bias', 'loss_module_val.model.blocks.4.ls1.gamma', 'loss_module_val.model.blocks.4.norm2.weight', 'loss_module_val.model.blocks.4.norm2.bias', 'loss_module_val.model.blocks.4.mlp.fc1.weight', 'loss_module_val.model.blocks.4.mlp.fc1.bias', 'loss_module_val.model.blocks.4.mlp.fc2.weight', 'loss_module_val.model.blocks.4.mlp.fc2.bias', 'loss_module_val.model.blocks.4.ls2.gamma', 'loss_module_val.model.blocks.5.norm1.weight', 'loss_module_val.model.blocks.5.norm1.bias', 'loss_module_val.model.blocks.5.attn.qkv.weight', 'loss_module_val.model.blocks.5.attn.qkv.bias', 'loss_module_val.model.blocks.5.attn.proj.weight', 'loss_module_val.model.blocks.5.attn.proj.bias', 'loss_module_val.model.blocks.5.ls1.gamma', 'loss_module_val.model.blocks.5.norm2.weight', 'loss_module_val.model.blocks.5.norm2.bias', 'loss_module_val.model.blocks.5.mlp.fc1.weight', 'loss_module_val.model.blocks.5.mlp.fc1.bias', 'loss_module_val.model.blocks.5.mlp.fc2.weight', 'loss_module_val.model.blocks.5.mlp.fc2.bias', 'loss_module_val.model.blocks.5.ls2.gamma', 'loss_module_val.model.blocks.6.norm1.weight', 'loss_module_val.model.blocks.6.norm1.bias', 'loss_module_val.model.blocks.6.attn.qkv.weight', 'loss_module_val.model.blocks.6.attn.qkv.bias', 'loss_module_val.model.blocks.6.attn.proj.weight', 'loss_module_val.model.blocks.6.attn.proj.bias', 'loss_module_val.model.blocks.6.ls1.gamma', 'loss_module_val.model.blocks.6.norm2.weight', 'loss_module_val.model.blocks.6.norm2.bias', 'loss_module_val.model.blocks.6.mlp.fc1.weight', 'loss_module_val.model.blocks.6.mlp.fc1.bias', 'loss_module_val.model.blocks.6.mlp.fc2.weight', 'loss_module_val.model.blocks.6.mlp.fc2.bias', 'loss_module_val.model.blocks.6.ls2.gamma', 'loss_module_val.model.blocks.7.norm1.weight', 'loss_module_val.model.blocks.7.norm1.bias', 'loss_module_val.model.blocks.7.attn.qkv.weight', 'loss_module_val.model.blocks.7.attn.qkv.bias', 'loss_module_val.model.blocks.7.attn.proj.weight', 'loss_module_val.model.blocks.7.attn.proj.bias', 'loss_module_val.model.blocks.7.ls1.gamma', 'loss_module_val.model.blocks.7.norm2.weight', 'loss_module_val.model.blocks.7.norm2.bias', 'loss_module_val.model.blocks.7.mlp.fc1.weight', 'loss_module_val.model.blocks.7.mlp.fc1.bias', 'loss_module_val.model.blocks.7.mlp.fc2.weight', 'loss_module_val.model.blocks.7.mlp.fc2.bias', 'loss_module_val.model.blocks.7.ls2.gamma', 'loss_module_val.model.blocks.8.norm1.weight', 'loss_module_val.model.blocks.8.norm1.bias', 'loss_module_val.model.blocks.8.attn.qkv.weight', 'loss_module_val.model.blocks.8.attn.qkv.bias', 'loss_module_val.model.blocks.8.attn.proj.weight', 'loss_module_val.model.blocks.8.attn.proj.bias', 'loss_module_val.model.blocks.8.ls1.gamma', 'loss_module_val.model.blocks.8.norm2.weight', 'loss_module_val.model.blocks.8.norm2.bias', 'loss_module_val.model.blocks.8.mlp.fc1.weight', 'loss_module_val.model.blocks.8.mlp.fc1.bias', 'loss_module_val.model.blocks.8.mlp.fc2.weight', 'loss_module_val.model.blocks.8.mlp.fc2.bias', 'loss_module_val.model.blocks.8.ls2.gamma', 'loss_module_val.model.blocks.9.norm1.weight', 'loss_module_val.model.blocks.9.norm1.bias', 'loss_module_val.model.blocks.9.attn.qkv.weight', 'loss_module_val.model.blocks.9.attn.qkv.bias', 'loss_module_val.model.blocks.9.attn.proj.weight', 'loss_module_val.model.blocks.9.attn.proj.bias', 'loss_module_val.model.blocks.9.ls1.gamma', 'loss_module_val.model.blocks.9.norm2.weight', 'loss_module_val.model.blocks.9.norm2.bias', 'loss_module_val.model.blocks.9.mlp.fc1.weight', 'loss_module_val.model.blocks.9.mlp.fc1.bias', 'loss_module_val.model.blocks.9.mlp.fc2.weight', 'loss_module_val.model.blocks.9.mlp.fc2.bias', 'loss_module_val.model.blocks.9.ls2.gamma', 'loss_module_val.model.blocks.10.norm1.weight', 'loss_module_val.model.blocks.10.norm1.bias', 'loss_module_val.model.blocks.10.attn.qkv.weight', 'loss_module_val.model.blocks.10.attn.qkv.bias', 'loss_module_val.model.blocks.10.attn.proj.weight', 'loss_module_val.model.blocks.10.attn.proj.bias', 'loss_module_val.model.blocks.10.ls1.gamma', 'loss_module_val.model.blocks.10.norm2.weight', 'loss_module_val.model.blocks.10.norm2.bias', 'loss_module_val.model.blocks.10.mlp.fc1.weight', 'loss_module_val.model.blocks.10.mlp.fc1.bias', 'loss_module_val.model.blocks.10.mlp.fc2.weight', 'loss_module_val.model.blocks.10.mlp.fc2.bias', 'loss_module_val.model.blocks.10.ls2.gamma', 'loss_module_val.model.blocks.11.norm1.weight', 'loss_module_val.model.blocks.11.norm1.bias', 'loss_module_val.model.blocks.11.attn.qkv.weight', 'loss_module_val.model.blocks.11.attn.qkv.bias', 'loss_module_val.model.blocks.11.attn.proj.weight', 'loss_module_val.model.blocks.11.attn.proj.bias', 'loss_module_val.model.blocks.11.ls1.gamma', 'loss_module_val.model.blocks.11.norm2.weight', 'loss_module_val.model.blocks.11.norm2.bias', 'loss_module_val.model.blocks.11.mlp.fc1.weight', 'loss_module_val.model.blocks.11.mlp.fc1.bias', 'loss_module_val.model.blocks.11.mlp.fc2.weight', 'loss_module_val.model.blocks.11.mlp.fc2.bias', 'loss_module_val.model.blocks.11.ls2.gamma', 'loss_module_val.model.blocks.12.norm1.weight', 'loss_module_val.model.blocks.12.norm1.bias', 'loss_module_val.model.blocks.12.attn.qkv.weight', 'loss_module_val.model.blocks.12.attn.qkv.bias', 'loss_module_val.model.blocks.12.attn.proj.weight', 'loss_module_val.model.blocks.12.attn.proj.bias', 'loss_module_val.model.blocks.12.ls1.gamma', 'loss_module_val.model.blocks.12.norm2.weight', 'loss_module_val.model.blocks.12.norm2.bias', 'loss_module_val.model.blocks.12.mlp.fc1.weight', 'loss_module_val.model.blocks.12.mlp.fc1.bias', 'loss_module_val.model.blocks.12.mlp.fc2.weight', 'loss_module_val.model.blocks.12.mlp.fc2.bias', 'loss_module_val.model.blocks.12.ls2.gamma', 'loss_module_val.model.blocks.13.norm1.weight', 'loss_module_val.model.blocks.13.norm1.bias', 'loss_module_val.model.blocks.13.attn.qkv.weight', 'loss_module_val.model.blocks.13.attn.qkv.bias', 'loss_module_val.model.blocks.13.attn.proj.weight', 'loss_module_val.model.blocks.13.attn.proj.bias', 'loss_module_val.model.blocks.13.ls1.gamma', 'loss_module_val.model.blocks.13.norm2.weight', 'loss_module_val.model.blocks.13.norm2.bias', 'loss_module_val.model.blocks.13.mlp.fc1.weight', 'loss_module_val.model.blocks.13.mlp.fc1.bias', 'loss_module_val.model.blocks.13.mlp.fc2.weight', 'loss_module_val.model.blocks.13.mlp.fc2.bias', 'loss_module_val.model.blocks.13.ls2.gamma', 'loss_module_val.model.blocks.14.norm1.weight', 'loss_module_val.model.blocks.14.norm1.bias', 'loss_module_val.model.blocks.14.attn.qkv.weight', 'loss_module_val.model.blocks.14.attn.qkv.bias', 'loss_module_val.model.blocks.14.attn.proj.weight', 'loss_module_val.model.blocks.14.attn.proj.bias', 'loss_module_val.model.blocks.14.ls1.gamma', 'loss_module_val.model.blocks.14.norm2.weight', 'loss_module_val.model.blocks.14.norm2.bias', 'loss_module_val.model.blocks.14.mlp.fc1.weight', 'loss_module_val.model.blocks.14.mlp.fc1.bias', 'loss_module_val.model.blocks.14.mlp.fc2.weight', 'loss_module_val.model.blocks.14.mlp.fc2.bias', 'loss_module_val.model.blocks.14.ls2.gamma', 'loss_module_val.model.blocks.15.norm1.weight', 'loss_module_val.model.blocks.15.norm1.bias', 'loss_module_val.model.blocks.15.attn.qkv.weight', 'loss_module_val.model.blocks.15.attn.qkv.bias', 'loss_module_val.model.blocks.15.attn.proj.weight', 'loss_module_val.model.blocks.15.attn.proj.bias', 'loss_module_val.model.blocks.15.ls1.gamma', 'loss_module_val.model.blocks.15.norm2.weight', 'loss_module_val.model.blocks.15.norm2.bias', 'loss_module_val.model.blocks.15.mlp.fc1.weight', 'loss_module_val.model.blocks.15.mlp.fc1.bias', 'loss_module_val.model.blocks.15.mlp.fc2.weight', 'loss_module_val.model.blocks.15.mlp.fc2.bias', 'loss_module_val.model.blocks.15.ls2.gamma', 'loss_module_val.model.blocks.16.norm1.weight', 'loss_module_val.model.blocks.16.norm1.bias', 'loss_module_val.model.blocks.16.attn.qkv.weight', 'loss_module_val.model.blocks.16.attn.qkv.bias', 'loss_module_val.model.blocks.16.attn.proj.weight', 'loss_module_val.model.blocks.16.attn.proj.bias', 'loss_module_val.model.blocks.16.ls1.gamma', 'loss_module_val.model.blocks.16.norm2.weight', 'loss_module_val.model.blocks.16.norm2.bias', 'loss_module_val.model.blocks.16.mlp.fc1.weight', 'loss_module_val.model.blocks.16.mlp.fc1.bias', 'loss_module_val.model.blocks.16.mlp.fc2.weight', 'loss_module_val.model.blocks.16.mlp.fc2.bias', 'loss_module_val.model.blocks.16.ls2.gamma', 'loss_module_val.model.blocks.17.norm1.weight', 'loss_module_val.model.blocks.17.norm1.bias', 'loss_module_val.model.blocks.17.attn.qkv.weight', 'loss_module_val.model.blocks.17.attn.qkv.bias', 'loss_module_val.model.blocks.17.attn.proj.weight', 'loss_module_val.model.blocks.17.attn.proj.bias', 'loss_module_val.model.blocks.17.ls1.gamma', 'loss_module_val.model.blocks.17.norm2.weight', 'loss_module_val.model.blocks.17.norm2.bias', 'loss_module_val.model.blocks.17.mlp.fc1.weight', 'loss_module_val.model.blocks.17.mlp.fc1.bias', 'loss_module_val.model.blocks.17.mlp.fc2.weight', 'loss_module_val.model.blocks.17.mlp.fc2.bias', 'loss_module_val.model.blocks.17.ls2.gamma', 'loss_module_val.model.blocks.18.norm1.weight', 'loss_module_val.model.blocks.18.norm1.bias', 'loss_module_val.model.blocks.18.attn.qkv.weight', 'loss_module_val.model.blocks.18.attn.qkv.bias', 'loss_module_val.model.blocks.18.attn.proj.weight', 'loss_module_val.model.blocks.18.attn.proj.bias', 'loss_module_val.model.blocks.18.ls1.gamma', 'loss_module_val.model.blocks.18.norm2.weight', 'loss_module_val.model.blocks.18.norm2.bias', 'loss_module_val.model.blocks.18.mlp.fc1.weight', 'loss_module_val.model.blocks.18.mlp.fc1.bias', 'loss_module_val.model.blocks.18.mlp.fc2.weight', 'loss_module_val.model.blocks.18.mlp.fc2.bias', 'loss_module_val.model.blocks.18.ls2.gamma', 'loss_module_val.model.blocks.19.norm1.weight', 'loss_module_val.model.blocks.19.norm1.bias', 'loss_module_val.model.blocks.19.attn.qkv.weight', 'loss_module_val.model.blocks.19.attn.qkv.bias', 'loss_module_val.model.blocks.19.attn.proj.weight', 'loss_module_val.model.blocks.19.attn.proj.bias', 'loss_module_val.model.blocks.19.ls1.gamma', 'loss_module_val.model.blocks.19.norm2.weight', 'loss_module_val.model.blocks.19.norm2.bias', 'loss_module_val.model.blocks.19.mlp.fc1.weight', 'loss_module_val.model.blocks.19.mlp.fc1.bias', 'loss_module_val.model.blocks.19.mlp.fc2.weight', 'loss_module_val.model.blocks.19.mlp.fc2.bias', 'loss_module_val.model.blocks.19.ls2.gamma', 'loss_module_val.model.blocks.20.norm1.weight', 'loss_module_val.model.blocks.20.norm1.bias', 'loss_module_val.model.blocks.20.attn.qkv.weight', 'loss_module_val.model.blocks.20.attn.qkv.bias', 'loss_module_val.model.blocks.20.attn.proj.weight', 'loss_module_val.model.blocks.20.attn.proj.bias', 'loss_module_val.model.blocks.20.ls1.gamma', 'loss_module_val.model.blocks.20.norm2.weight', 'loss_module_val.model.blocks.20.norm2.bias', 'loss_module_val.model.blocks.20.mlp.fc1.weight', 'loss_module_val.model.blocks.20.mlp.fc1.bias', 'loss_module_val.model.blocks.20.mlp.fc2.weight', 'loss_module_val.model.blocks.20.mlp.fc2.bias', 'loss_module_val.model.blocks.20.ls2.gamma', 'loss_module_val.model.blocks.21.norm1.weight', 'loss_module_val.model.blocks.21.norm1.bias', 'loss_module_val.model.blocks.21.attn.qkv.weight', 'loss_module_val.model.blocks.21.attn.qkv.bias', 'loss_module_val.model.blocks.21.attn.proj.weight', 'loss_module_val.model.blocks.21.attn.proj.bias', 'loss_module_val.model.blocks.21.ls1.gamma', 'loss_module_val.model.blocks.21.norm2.weight', 'loss_module_val.model.blocks.21.norm2.bias', 'loss_module_val.model.blocks.21.mlp.fc1.weight', 'loss_module_val.model.blocks.21.mlp.fc1.bias', 'loss_module_val.model.blocks.21.mlp.fc2.weight', 'loss_module_val.model.blocks.21.mlp.fc2.bias', 'loss_module_val.model.blocks.21.ls2.gamma', 'loss_module_val.model.blocks.22.norm1.weight', 'loss_module_val.model.blocks.22.norm1.bias', 'loss_module_val.model.blocks.22.attn.qkv.weight', 'loss_module_val.model.blocks.22.attn.qkv.bias', 'loss_module_val.model.blocks.22.attn.proj.weight', 'loss_module_val.model.blocks.22.attn.proj.bias', 'loss_module_val.model.blocks.22.ls1.gamma', 'loss_module_val.model.blocks.22.norm2.weight', 'loss_module_val.model.blocks.22.norm2.bias', 'loss_module_val.model.blocks.22.mlp.fc1.weight', 'loss_module_val.model.blocks.22.mlp.fc1.bias', 'loss_module_val.model.blocks.22.mlp.fc2.weight', 'loss_module_val.model.blocks.22.mlp.fc2.bias', 'loss_module_val.model.blocks.22.ls2.gamma', 'loss_module_val.model.blocks.23.norm1.weight', 'loss_module_val.model.blocks.23.norm1.bias', 'loss_module_val.model.blocks.23.attn.qkv.weight', 'loss_module_val.model.blocks.23.attn.qkv.bias', 'loss_module_val.model.blocks.23.attn.proj.weight', 'loss_module_val.model.blocks.23.attn.proj.bias', 'loss_module_val.model.blocks.23.ls1.gamma', 'loss_module_val.model.blocks.23.norm2.weight', 'loss_module_val.model.blocks.23.norm2.bias', 'loss_module_val.model.blocks.23.mlp.fc1.weight', 'loss_module_val.model.blocks.23.mlp.fc1.bias', 'loss_module_val.model.blocks.23.mlp.fc2.weight', 'loss_module_val.model.blocks.23.mlp.fc2.bias', 'loss_module_val.model.blocks.23.ls2.gamma', 'loss_module_val.model.norm.weight', 'loss_module_val.model.norm.bias', 'loss_module_val.l2sp_loss.cls_token', 'loss_module_val.l2sp_loss.patch_embed_proj_weight', 'loss_module_val.l2sp_loss.blocks_0_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_0_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_0_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_0_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_1_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_1_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_1_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_1_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_2_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_2_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_2_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_2_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_3_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_3_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_3_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_3_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_4_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_4_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_4_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_4_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_5_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_5_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_5_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_5_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_6_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_6_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_6_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_6_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_7_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_7_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_7_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_7_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_8_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_8_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_8_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_8_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_9_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_9_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_9_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_9_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_10_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_10_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_10_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_10_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_11_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_11_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_11_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_11_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_12_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_12_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_12_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_12_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_13_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_13_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_13_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_13_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_14_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_14_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_14_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_14_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_15_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_15_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_15_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_15_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_16_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_16_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_16_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_16_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_17_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_17_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_17_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_17_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_18_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_18_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_18_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_18_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_19_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_19_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_19_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_19_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_20_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_20_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_20_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_20_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_21_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_21_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_21_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_21_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_22_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_22_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_22_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_22_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_23_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_23_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_23_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_23_mlp_fc2_weight'])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load model from provided checkpoint\n",
    "def extract_clean_state_dict_for_wrapper(checkpoint, wrapper_key=\"model_wrapper.\", model_key=\"model.\"):\n",
    "    state_dict = checkpoint.get('state_dict', checkpoint)\n",
    "    cleaned_state_dict = {k.replace(wrapper_key, ''): v for k, v in state_dict.items()}\n",
    "    return cleaned_state_dict\n",
    "\n",
    "# using TimmWrapper provided by previous BP: embedding_id could be \"linear\" or \"\"\n",
    "model_wrapper = TimmWrapper(\n",
    "    backbone_name=\"vit_large_patch14_dinov2.lvd142m\",\n",
    "    embedding_size=256,\n",
    "    embedding_id=\"linear\", # possible values: \"linear\", \"\"\n",
    "    dropout_p=0.0,\n",
    "    pool_mode=\"none\",\n",
    "    img_size=224\n",
    ")\n",
    "\n",
    "cleaned_state_dict_wrapper = extract_clean_state_dict_for_wrapper(checkpoint_best)\n",
    "model_wrapper.load_state_dict(cleaned_state_dict_wrapper, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|| 4/4 [00:22<00:00,  5.73s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate embeddings for the test dataset\n",
    "model_wrapper.to(device)\n",
    "model_wrapper.eval()\n",
    "\n",
    "# Generate embeddings\n",
    "def generate_embeddings(model, data_loader):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    all_videos = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels, videos in tqdm(data_loader, desc=\"Generating Embeddings\"):\n",
    "            images = images.to(device)\n",
    "            embeddings = model(images)\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.extend(labels)\n",
    "            all_videos.extend(videos)\n",
    "    return torch.cat(all_embeddings), all_labels, all_videos\n",
    "\n",
    "embeddings, labels, video_ids = generate_embeddings(model_wrapper, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "Cross-Video KNN5 Accuracy: 0.8372\n"
     ]
    }
   ],
   "source": [
    "# KNN for cross-video classification\n",
    "\n",
    "# Function to calculate distance based on the chosen metric\n",
    "def calculate_distance(embeddings, test_embedding, metric):\n",
    "    if metric == \"euclidean\":\n",
    "        # Compute Euclidean distance\n",
    "        distances = np.linalg.norm(embeddings - test_embedding, axis=1)\n",
    "    elif metric == \"cosine\":\n",
    "        # Normalize embeddings to unit vectors for cosine similarity\n",
    "        normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        normalized_test_embedding = test_embedding / np.linalg.norm(test_embedding)\n",
    "        # Compute cosine similarity\n",
    "        cosine_similarity = np.dot(normalized_embeddings, normalized_test_embedding)\n",
    "        # Convert similarity to distance\n",
    "        distances = 1 - cosine_similarity\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distance metric: {metric}\")\n",
    "    return distances\n",
    "\n",
    "def KNN_CV(embeddings, images_to_check, labels, video_ids, distance_metric=\"euclidean\", num_neighbors=5):\n",
    "    embeddings = embeddings.numpy()\n",
    "    vit_y_pred = []\n",
    "    actual_labels = []\n",
    "\n",
    "    for idx, test_embedding in enumerate(embeddings):\n",
    "        # Only classify images that satisfies the condition for cross-video check\n",
    "        if idx not in images_to_check:\n",
    "            continue\n",
    "        \n",
    "        # Calculate distances using the chosen metric\n",
    "        distances = calculate_distance(embeddings, test_embedding, distance_metric)\n",
    "\n",
    "        # Get sorted indices of neighbors based on distance\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        sorted_indices = sorted_indices[sorted_indices != idx]  # Exclude self\n",
    "\n",
    "        # Get valid neighbors (i.e., neighbors from different videos)\n",
    "        valid_neighbors = []\n",
    "        for neighbor_idx in sorted_indices:\n",
    "            if video_ids[neighbor_idx] != video_ids[idx]:\n",
    "                valid_neighbors.append(neighbor_idx)\n",
    "            if len(valid_neighbors) == num_neighbors:\n",
    "                    break\n",
    "\n",
    "        if len(valid_neighbors) < num_neighbors:\n",
    "            print(f\"Warning: Less than {num_neighbors} valid neighbors for index {idx}.\")\n",
    "        \n",
    "        # Get labels for the valid neighbors\n",
    "        valid_neighbor_labels = [labels[i] for i in valid_neighbors]\n",
    "        \n",
    "        predicted_label = max(set(valid_neighbor_labels), key=valid_neighbor_labels.count)\n",
    "        actual_label = labels[idx]\n",
    "\n",
    "        vit_y_pred.append(predicted_label)\n",
    "        actual_labels.append(actual_label)\n",
    "    \n",
    "    accuracy = accuracy_score(actual_labels, vit_y_pred)\n",
    "    return accuracy\n",
    "\n",
    "print(len(test_dataset.images_for_cross_video_knn))\n",
    "KNN5_CV_accuracy = KNN_CV(embeddings, test_dataset.images_for_cross_video_knn, labels, video_ids, distance_metric=\"euclidean\")\n",
    "print(f\"Cross-Video KNN5 Accuracy: {KNN5_CV_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n",
      "Standard KNN5 Accuracy: 0.9560\n"
     ]
    }
   ],
   "source": [
    "# standard KNN classification (without cross-video check)\n",
    "def KNN_standard(embeddings, images_to_check, labels, distance_metric=\"euclidean\", num_neighbors=5):\n",
    "    embeddings = embeddings.numpy()\n",
    "    vit_y_pred = []\n",
    "    actual_labels = []\n",
    "\n",
    "    for idx, test_embedding in enumerate(embeddings):\n",
    "        # Only classify images that satisfies the condition for cross-video check\n",
    "        if idx not in images_to_check:\n",
    "            continue\n",
    "        \n",
    "        # Calculate distances using the chosen metric\n",
    "        distances = calculate_distance(embeddings, test_embedding, distance_metric)\n",
    "\n",
    "        # Get sorted indices of neighbors based on distance\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        sorted_indices = sorted_indices[sorted_indices != idx]  # Exclude self\n",
    "        neighbors = sorted_indices[:num_neighbors]\n",
    "        neighbor_labels = [labels[i] for i in neighbors]\n",
    "        \n",
    "        predicted_label = max(set(neighbor_labels), key=neighbor_labels.count)\n",
    "        actual_label = labels[idx]\n",
    "\n",
    "        vit_y_pred.append(predicted_label)\n",
    "        actual_labels.append(actual_label)\n",
    "    \n",
    "    accuracy = accuracy_score(actual_labels, vit_y_pred)\n",
    "    return accuracy\n",
    "\n",
    "print(len(test_dataset.images))\n",
    "KNN5_standard_accuracy = KNN_standard(embeddings, test_dataset.images_for_standard_knn, labels, distance_metric=\"euclidean\")\n",
    "print(f\"Standard KNN5 Accuracy: {KNN5_standard_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
