{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are 2 approaches possible:\n",
    "\n",
    "1. Load the checkpoints into a standard ViT model and ignore all the extra layers \n",
    "\n",
    "2. Define a custom ViT model by wrapping a base ViT in layers corresponding to the checkpoints (weights that are named \"loss_module_*\" are ignored because they're mostly likely only for training and I don't know what kind of layers fit them)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from timm import create_model\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import torch\n",
    "from timm import create_model\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from wrappers_supervised import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths to model and data\n",
    "\n",
    "joscha_checkpoint_path = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/models/vit_large_dinov2_ssl_joscha.ckpt\"\n",
    "robert_checkpoint_path = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/models/supervised_dinov2_large.ckpt\"\n",
    "vincent_checkpoint_path = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/models/ssl_vincent_vit_large.ckpt\"\n",
    "best_checkpoint_path = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/models/should-be-the-best-model_vit_large_dinoV2.ckpt\"\n",
    "\n",
    "model_name_joscha = \"vit_large_dinov2_ssl_joscha\"\n",
    "model_name_robert = \"supervised_dinov2_large\"\n",
    "model_name_vincent = \"ssl_vincent_vit_large\"\n",
    "model_name_best = \"best-model_vit_large_dinoV2\"\n",
    "\n",
    "wrapper_type = \"timmWrapper\"\n",
    "model_name_wrapper = f\"{model_name_best} in {wrapper_type}\"\n",
    "\n",
    "#checkpoint_path = robert_checkpoint_path\n",
    "test_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_all_split_60-25-15/test\"\n",
    "train_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_all_split_60-25-15/train\"\n",
    "val_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_all_split_60-25-15/val\"\n",
    "\n",
    "all_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_all_face\"\n",
    "\n",
    "bristol_folder = \"/workspaces/gorilla_watch/video_data/bristol/cropped_frames_square_filtered\"\n",
    "# filtered 372 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect checkpoints in comparison to a standard ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1177/66037643.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_joscha = torch.load(joscha_checkpoint_path, map_location=torch.device(device))\n",
      "/tmp/ipykernel_1177/66037643.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_robert = torch.load(robert_checkpoint_path, map_location=torch.device(device))\n",
      "/tmp/ipykernel_1177/66037643.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_vincent = torch.load(vincent_checkpoint_path, map_location=torch.device(device))\n",
      "/tmp/ipykernel_1177/66037643.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_best = torch.load(best_checkpoint_path, map_location=torch.device(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb_run <wandb.sdk.wandb_run.Run object at 0x7f7123031870>\n",
      "loss_mode online/hard/l2sp\n",
      "from_scratch False\n",
      "weight_decay 0.2\n",
      "lr_schedule cosine\n",
      "warmup_mode constant\n",
      "warmup_epochs 0\n",
      "max_epochs 100\n",
      "initial_lr 4.09891362474683e-06\n",
      "start_lr 5.497236565453763e-06\n",
      "end_lr 1e-07\n",
      "stepwise_schedule False\n",
      "lr_interval 1\n",
      "beta1 0.9\n",
      "beta2 0.999\n",
      "epsilon 1e-07\n",
      "embedding_size 256\n",
      "batch_size 16\n",
      "dataset_names ['cxlkfold', 'bristol', 'test']\n",
      "accelerator cuda\n",
      "dropout_p 0.0\n",
      "use_dist_term False\n",
      "use_inbatch_mixup False\n",
      "kfold_k None\n",
      "knn_with_train True\n",
      "use_quantization_aware_training False\n",
      "every_n_val_epochs 5\n",
      "fast_dev_run False\n",
      "margin 1.0\n",
      "s 64.0\n",
      "temperature 0.5\n",
      "memory_bank_size 0\n",
      "num_classes None\n",
      "class_distribution None\n",
      "l2_alpha 0.05901881729791581\n",
      "l2_beta 0.028149782260071254\n",
      "path_to_pretrained_weights ./pretrained_weights/vit_large_patch14_dinov2_lvd142m.pth\n",
      "use_wildme_model False\n",
      "k_subcenters 1\n",
      "use_focal_loss False\n",
      "label_smoothing 0.0\n",
      "use_class_weights False\n",
      "teacher_model_wandb_link \n",
      "loss_dist_term euclidean\n",
      "cross_video_masking False\n",
      "additive_margin 0.0\n",
      "margin_std 0.05\n",
      "model_name_or_path timm/vit_large_patch14_dinov2.lvd142m\n",
      "pool_mode none\n",
      "fix_img_size 224\n",
      "embedding_id linear\n",
      "freeze_backbone False\n",
      "kwargs\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint and extract state_dict\n",
    "checkpoint_joscha = torch.load(joscha_checkpoint_path, map_location=torch.device(device))\n",
    "checkpoint_robert = torch.load(robert_checkpoint_path, map_location=torch.device(device))\n",
    "checkpoint_vincent = torch.load(vincent_checkpoint_path, map_location=torch.device(device))\n",
    "checkpoint_best = torch.load(best_checkpoint_path, map_location=torch.device(device))\n",
    "\n",
    "# for k, v in checkpoint_best[\"hyper_parameters\"].items():\n",
    "#     print(k, v)\n",
    "# print(checkpoint_best[\"hparams_name\"])\n",
    "# print(checkpoint_best[\"state_dict\"].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys in standard ViT (best): ['embedding_layer.weight', 'embedding_layer.bias']\n"
     ]
    }
   ],
   "source": [
    "# ### compare checkpoints' state_dict with a standard model's state_dict\n",
    "\n",
    "def extract_clean_state_dict(checkpoint, wrapper_key=\"model_wrapper.\", model_key=\"model.\"):\n",
    "    # Extract the state_dict from the checkpoint\n",
    "    state_dict = checkpoint.get('state_dict', checkpoint)  # Use 'state_dict' or checkpoint directly\n",
    "    # Remove wrapper key prefix\n",
    "    cleaned_state_dict = {k.replace(wrapper_key, ''): v for k, v in state_dict.items()}\n",
    "    # Remove model key prefix\n",
    "    cleaned_state_dict = {k.replace(model_key, ''): v for k, v in cleaned_state_dict.items()}\n",
    "    \n",
    "    return cleaned_state_dict\n",
    "    \n",
    "def compare_checkpoints_with_model(checkpoint, model_fn, model_name, img_size):\n",
    "    # # Extract the state_dict from the checkpoint\n",
    "    # state_dict = checkpoint.get('state_dict', checkpoint)  # Use 'state_dict' or checkpoint directly\n",
    "    # new_state_dict = {k.replace('model_wrapper.', ''): v for k, v in state_dict.items()}\n",
    "    # new_state_dict2 = {k.replace('model.', ''): v for k, v in new_state_dict.items()}\n",
    "    \n",
    "    cleaned_state_dict = extract_clean_state_dict(checkpoint)\n",
    "\n",
    "    # Initialize the model\n",
    "    vit_model = model_fn(model_name, pretrained=False, img_size=img_size)\n",
    "\n",
    "    # Compare keys\n",
    "    standard_vit_keys = vit_model.state_dict().keys()\n",
    "    custom_keys = cleaned_state_dict.keys()\n",
    "    missing_in_standard = [k for k in custom_keys if k not in standard_vit_keys]\n",
    "    extra_in_standard = [k for k in standard_vit_keys if k not in custom_keys]\n",
    "\n",
    "    # Collect information about embedding keys\n",
    "    embedding_keys = [key for key in cleaned_state_dict.keys() if \"embedding_layer\" in key]\n",
    "\n",
    "    # Print missing keys and their shapes\n",
    "    # for key in missing_in_standard:\n",
    "        # print(f\"{key}: {new_state_dict2[key].shape}\")\n",
    "\n",
    "    return {\n",
    "        \"missing_in_standard\": missing_in_standard,\n",
    "        \"extra_in_standard\": extra_in_standard,\n",
    "        \"embedding_keys\": embedding_keys,\n",
    "    }\n",
    "        \n",
    "    \n",
    "# Compare the checkpoint with the standard ViT model\n",
    "# comparison_joscha = compare_checkpoints_with_model(\n",
    "#     checkpoint=checkpoint_joscha,\n",
    "#     model_fn=create_model,\n",
    "#     model_name='vit_large_patch14_dinov2.lvd142m',\n",
    "#     img_size=192\n",
    "# )\n",
    "# comparison_robert = compare_checkpoints_with_model(\n",
    "#     checkpoint=checkpoint_robert,\n",
    "#     model_fn=create_model,\n",
    "#     model_name='vit_large_patch14_dinov2.lvd142m',\n",
    "#     img_size=192\n",
    "# )\n",
    "# comparison_vincent = compare_checkpoints_with_model(\n",
    "#     checkpoint=checkpoint_vincent,\n",
    "#     model_fn=create_model,\n",
    "#     model_name='vit_large_patch14_dinov2.lvd142m',\n",
    "#     img_size=192\n",
    "# )\n",
    "comparison_best = compare_checkpoints_with_model(\n",
    "    checkpoint=checkpoint_best,\n",
    "    model_fn=create_model,\n",
    "    model_name='vit_large_patch14_dinov2.lvd142m',\n",
    "    img_size=224\n",
    ")\n",
    "\n",
    "# print(\"Missing keys in standard ViT (Joscha):\", comparison_joscha)\n",
    "# print(\"Missing keys in standard ViT (Robert):\", comparison_robert)\n",
    "# print(\"Missing keys in standard ViT (Vincent):\", comparison_vincent)\n",
    "\n",
    "\n",
    "def filter_missing_keys(keys, filter_keywords):\n",
    "    return [key for key in keys if not any(keyword in key for keyword in filter_keywords)]\n",
    "\n",
    "\n",
    "# Define the keywords to filter out\n",
    "filter_keywords = [\"loss_module_val\", \"loss_module_train\"]\n",
    "\n",
    "# # Apply filtering to each comparison\n",
    "# comparison_joscha_missing_keys = filter_missing_keys(comparison_joscha[\"missing_in_standard\"], filter_keywords)\n",
    "# comparison_robert_missing_keys = filter_missing_keys(comparison_robert[\"missing_in_standard\"], filter_keywords)\n",
    "# comparison_vincent_missing_keys = filter_missing_keys(comparison_vincent[\"missing_in_standard\"], filter_keywords)\n",
    "comparison_best_missing_keys = filter_missing_keys(comparison_best[\"missing_in_standard\"], filter_keywords)\n",
    "\n",
    "# # Print filtered results\n",
    "# print(\"Missing keys in standard ViT (Joscha):\", comparison_joscha_missing_keys)\n",
    "# print(\"Missing keys in standard ViT (Robert):\", comparison_robert_missing_keys)\n",
    "# print(\"Missing keys in standard ViT (Vincent):\", comparison_vincent_missing_keys)\n",
    "print(\"Missing keys in standard ViT (best):\", comparison_best_missing_keys)\n",
    "\n",
    "cleaned_state_dict = extract_clean_state_dict(checkpoint_best)\n",
    "# print(cleaned_state_dict.keys())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions used by both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix pos_emd mismatch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def interpolate_positional_embedding(pretrained_state_dict, current_model, patch_size):\n",
    "    # Get the pretrained positional embeddings\n",
    "    pretrained_pos_embed = pretrained_state_dict['pos_embed']\n",
    "    current_pos_embed = current_model.state_dict()['pos_embed']\n",
    "    \n",
    "    # Exclude the CLS token (first token) if present\n",
    "    cls_token = pretrained_pos_embed[:, :1, :]\n",
    "    pretrained_grid = pretrained_pos_embed[:, 1:, :]\n",
    "    \n",
    "    # Get the grid dimensions\n",
    "    num_patches = current_pos_embed.shape[1] - 1  # Exclude CLS token\n",
    "    grid_size_pretrained = int((pretrained_grid.shape[1])**0.5)  # sqrt(num_patches)\n",
    "    grid_size_current = int(num_patches**0.5)\n",
    "\n",
    "    # Reshape the positional embeddings to a grid\n",
    "    pretrained_grid = pretrained_grid.reshape(1, grid_size_pretrained, grid_size_pretrained, -1).permute(0, 3, 1, 2)\n",
    "    \n",
    "    # Interpolate to the current grid size\n",
    "    interpolated_grid = F.interpolate(pretrained_grid, size=(grid_size_current, grid_size_current), mode='bilinear', align_corners=False)\n",
    "    interpolated_grid = interpolated_grid.permute(0, 2, 3, 1).reshape(1, -1, interpolated_grid.shape[1])\n",
    "    \n",
    "    # Combine CLS token and the new grid\n",
    "    new_pos_embed = torch.cat([cls_token, interpolated_grid], dim=1)\n",
    "    pretrained_state_dict['pos_embed'] = new_pos_embed\n",
    "\n",
    "    return pretrained_state_dict\n",
    "\n",
    "\n",
    "# interpolate_positional_embedding(new_state_dict2, vit_model, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract cleaned state_dict from checkpoint\n",
    "cleaned_state_dict_joscha = extract_clean_state_dict(checkpoint_joscha, wrapper_key=\"model_wrapper.\", model_key=\"model.\")\n",
    "cleaned_state_dict_robert = extract_clean_state_dict(checkpoint_robert, wrapper_key=\"model_wrapper.\", model_key=\"model.\")\n",
    "cleaned_state_dict_vincent = extract_clean_state_dict(checkpoint_vincent, wrapper_key=\"model_wrapper.\", model_key=\"model.\")\n",
    "cleaned_state_dict_best = extract_clean_state_dict(checkpoint_best, wrapper_key=\"model_wrapper.\", model_key=\"model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# hyperparams\n",
    "img_size = 224\n",
    "\n",
    "# Preprocessing function to resize and normalize images\n",
    "transform_custom = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Adjust based on model pretraining\n",
    "])\n",
    "\n",
    "transform_standard = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Adjust based on model pretraining\n",
    "])\n",
    "\n",
    "# Load images and extract class codes from file names\n",
    "# def load_images_and_labels(folder, transform):\n",
    "#     images = []\n",
    "#     labels = []\n",
    "#     for filename in os.listdir(folder):\n",
    "#         if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "#             image = cv2.imread(os.path.join(folder, filename))\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#             image = transform(image)\n",
    "#             images.append(image)\n",
    "#             label = filename.split(\"_\")[0]\n",
    "#             # labels.append(filename[:4])  # Assuming first 4 chars are label\n",
    "#             labels.append(label)\n",
    "                \n",
    "#     return images, labels\n",
    "\n",
    "def load_images_and_labels(folder, transform, threshold=3):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Temporary storage for all data\n",
    "    temp_images = []\n",
    "    temp_labels = []\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            image = cv2.imread(os.path.join(folder, filename))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = transform(image)\n",
    "            temp_images.append(image)\n",
    "            label = filename.split(\"_\")[0]\n",
    "            temp_labels.append(label)\n",
    "    \n",
    "    # Count occurrences of each class\n",
    "    label_counts = Counter(temp_labels)\n",
    "    \n",
    "    # Filter out classes with fewer than 'threshold' images\n",
    "    valid_classes = {label for label, count in label_counts.items() if count >= threshold}\n",
    "    \n",
    "    for image, label in zip(temp_images, temp_labels):\n",
    "        if label in valid_classes:\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# unique_labels = set(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out samples with less occurrences\n",
    "from collections import Counter\n",
    "\n",
    "def filter_samples_by_threshold(embeddings, labels, threshold=3):\n",
    "    # Count occurrences of each label\n",
    "    label_counts = Counter(labels)\n",
    "    print(f\"Number of images before filtering: {len(labels)}\")\n",
    "\n",
    "    # Filter embeddings and labels for labels with >= threshold occurrences\n",
    "    filtered_indices = [i for i, label in enumerate(labels) if label_counts[label] >= threshold]\n",
    "    filtered_embeddings = embeddings[filtered_indices]\n",
    "    filtered_labels = [labels[i] for i in filtered_indices]\n",
    "\n",
    "    # Log results\n",
    "    print(f\"Number of embeddings after filtering: {len(filtered_embeddings)}\")\n",
    "\n",
    "    return filtered_embeddings, filtered_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to generate embeddings from a model\n",
    "def generate_embeddings(model, images_tensor):\n",
    "    with torch.no_grad():\n",
    "        # Call the forward method to extract embeddings\n",
    "        embeddings = model(images_tensor)\n",
    "        embeddings_flat = embeddings.view(embeddings.size(0), -1).numpy() # Flatten embeddings(optional?)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# KNN classifier and excluding the closest neighbor (self-matching)\n",
    "def knn_classifier(embeddings, labels):\n",
    "    for k in range(5, 6):\n",
    "        # Use k+1 neighbors to account for excluding self\n",
    "        vit_knn = KNeighborsClassifier(n_neighbors=k + 1)\n",
    "        vit_knn.fit(embeddings, labels)  # Fit KNN on all data\n",
    "\n",
    "        vit_y_pred = []\n",
    "        for idx, test_embedding in enumerate(embeddings):\n",
    "            # Find k+1 neighbors (including the test sample itself)\n",
    "            neighbors = vit_knn.kneighbors(test_embedding.reshape(1, -1), return_distance=False)[0]\n",
    "\n",
    "            # Exclude the test sample itself (assumed to be the closest neighbor)\n",
    "            filtered_neighbors = neighbors[1:]  # Exclude the first neighbor\n",
    "        \n",
    "            # Predict based on the remaining neighbors (majority vote)\n",
    "            filtered_neighbor_labels = [labels[n] for n in filtered_neighbors]\n",
    "            predicted_label = max(set(filtered_neighbor_labels), key=filtered_neighbor_labels.count)\n",
    "            vit_y_pred.append(predicted_label)\n",
    "\n",
    "        # Calculate accuracy and F1 for the current k\n",
    "        vit_accuracy = accuracy_score(labels, vit_y_pred)\n",
    "        vit_f1 = f1_score(labels, vit_y_pred, average='macro')\n",
    "        print(f\"ViT Accuracy with {k}-nearest neighbors: {vit_accuracy:.4f}, F1_score:{vit_f1: 4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data (choose the folder here!!!!)\n",
    "# Load data (SPAC or Bristol)\n",
    "# test_images, test_labels = load_images_and_labels(test_folder, transform_standard)\n",
    "\n",
    "test_images, test_labels = load_images_and_labels(bristol_folder, transform_standard)\n",
    "\n",
    "# test_images, test_labels = load_images_and_labels(val_folder, transform_standard)\n",
    "\n",
    "# train_images, train_labels = load_images_and_labels(train_folder)\n",
    "# val_images, val_labels = load_images_and_labels(val_folder)\n",
    "# all_images, all_labels = load_images_and_labels(all_folder)\n",
    "\n",
    "test_images_tensor = torch.stack(test_images)\n",
    "# train_images_tensor = torch.stack(train_images)\n",
    "# val_images_tensor = torch.stack(val_images)\n",
    "# all_images_tensor = torch.stack(all_images)\n",
    "\n",
    "# train_val_images_tensor = torch.cat((train_images_tensor, val_images_tensor), 0)\n",
    "# train_val_labels = train_labels + val_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: load the chcekpoints into a standard ViT moel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['embedding_layer.weight', 'embedding_layer.bias', 'loss_module_train.cls_token', 'loss_module_train.pos_embed', 'loss_module_train.patch_embed.proj.weight', 'loss_module_train.patch_embed.proj.bias', 'loss_module_train.blocks.0.norm1.weight', 'loss_module_train.blocks.0.norm1.bias', 'loss_module_train.blocks.0.attn.qkv.weight', 'loss_module_train.blocks.0.attn.qkv.bias', 'loss_module_train.blocks.0.attn.proj.weight', 'loss_module_train.blocks.0.attn.proj.bias', 'loss_module_train.blocks.0.ls1.gamma', 'loss_module_train.blocks.0.norm2.weight', 'loss_module_train.blocks.0.norm2.bias', 'loss_module_train.blocks.0.mlp.fc1.weight', 'loss_module_train.blocks.0.mlp.fc1.bias', 'loss_module_train.blocks.0.mlp.fc2.weight', 'loss_module_train.blocks.0.mlp.fc2.bias', 'loss_module_train.blocks.0.ls2.gamma', 'loss_module_train.blocks.1.norm1.weight', 'loss_module_train.blocks.1.norm1.bias', 'loss_module_train.blocks.1.attn.qkv.weight', 'loss_module_train.blocks.1.attn.qkv.bias', 'loss_module_train.blocks.1.attn.proj.weight', 'loss_module_train.blocks.1.attn.proj.bias', 'loss_module_train.blocks.1.ls1.gamma', 'loss_module_train.blocks.1.norm2.weight', 'loss_module_train.blocks.1.norm2.bias', 'loss_module_train.blocks.1.mlp.fc1.weight', 'loss_module_train.blocks.1.mlp.fc1.bias', 'loss_module_train.blocks.1.mlp.fc2.weight', 'loss_module_train.blocks.1.mlp.fc2.bias', 'loss_module_train.blocks.1.ls2.gamma', 'loss_module_train.blocks.2.norm1.weight', 'loss_module_train.blocks.2.norm1.bias', 'loss_module_train.blocks.2.attn.qkv.weight', 'loss_module_train.blocks.2.attn.qkv.bias', 'loss_module_train.blocks.2.attn.proj.weight', 'loss_module_train.blocks.2.attn.proj.bias', 'loss_module_train.blocks.2.ls1.gamma', 'loss_module_train.blocks.2.norm2.weight', 'loss_module_train.blocks.2.norm2.bias', 'loss_module_train.blocks.2.mlp.fc1.weight', 'loss_module_train.blocks.2.mlp.fc1.bias', 'loss_module_train.blocks.2.mlp.fc2.weight', 'loss_module_train.blocks.2.mlp.fc2.bias', 'loss_module_train.blocks.2.ls2.gamma', 'loss_module_train.blocks.3.norm1.weight', 'loss_module_train.blocks.3.norm1.bias', 'loss_module_train.blocks.3.attn.qkv.weight', 'loss_module_train.blocks.3.attn.qkv.bias', 'loss_module_train.blocks.3.attn.proj.weight', 'loss_module_train.blocks.3.attn.proj.bias', 'loss_module_train.blocks.3.ls1.gamma', 'loss_module_train.blocks.3.norm2.weight', 'loss_module_train.blocks.3.norm2.bias', 'loss_module_train.blocks.3.mlp.fc1.weight', 'loss_module_train.blocks.3.mlp.fc1.bias', 'loss_module_train.blocks.3.mlp.fc2.weight', 'loss_module_train.blocks.3.mlp.fc2.bias', 'loss_module_train.blocks.3.ls2.gamma', 'loss_module_train.blocks.4.norm1.weight', 'loss_module_train.blocks.4.norm1.bias', 'loss_module_train.blocks.4.attn.qkv.weight', 'loss_module_train.blocks.4.attn.qkv.bias', 'loss_module_train.blocks.4.attn.proj.weight', 'loss_module_train.blocks.4.attn.proj.bias', 'loss_module_train.blocks.4.ls1.gamma', 'loss_module_train.blocks.4.norm2.weight', 'loss_module_train.blocks.4.norm2.bias', 'loss_module_train.blocks.4.mlp.fc1.weight', 'loss_module_train.blocks.4.mlp.fc1.bias', 'loss_module_train.blocks.4.mlp.fc2.weight', 'loss_module_train.blocks.4.mlp.fc2.bias', 'loss_module_train.blocks.4.ls2.gamma', 'loss_module_train.blocks.5.norm1.weight', 'loss_module_train.blocks.5.norm1.bias', 'loss_module_train.blocks.5.attn.qkv.weight', 'loss_module_train.blocks.5.attn.qkv.bias', 'loss_module_train.blocks.5.attn.proj.weight', 'loss_module_train.blocks.5.attn.proj.bias', 'loss_module_train.blocks.5.ls1.gamma', 'loss_module_train.blocks.5.norm2.weight', 'loss_module_train.blocks.5.norm2.bias', 'loss_module_train.blocks.5.mlp.fc1.weight', 'loss_module_train.blocks.5.mlp.fc1.bias', 'loss_module_train.blocks.5.mlp.fc2.weight', 'loss_module_train.blocks.5.mlp.fc2.bias', 'loss_module_train.blocks.5.ls2.gamma', 'loss_module_train.blocks.6.norm1.weight', 'loss_module_train.blocks.6.norm1.bias', 'loss_module_train.blocks.6.attn.qkv.weight', 'loss_module_train.blocks.6.attn.qkv.bias', 'loss_module_train.blocks.6.attn.proj.weight', 'loss_module_train.blocks.6.attn.proj.bias', 'loss_module_train.blocks.6.ls1.gamma', 'loss_module_train.blocks.6.norm2.weight', 'loss_module_train.blocks.6.norm2.bias', 'loss_module_train.blocks.6.mlp.fc1.weight', 'loss_module_train.blocks.6.mlp.fc1.bias', 'loss_module_train.blocks.6.mlp.fc2.weight', 'loss_module_train.blocks.6.mlp.fc2.bias', 'loss_module_train.blocks.6.ls2.gamma', 'loss_module_train.blocks.7.norm1.weight', 'loss_module_train.blocks.7.norm1.bias', 'loss_module_train.blocks.7.attn.qkv.weight', 'loss_module_train.blocks.7.attn.qkv.bias', 'loss_module_train.blocks.7.attn.proj.weight', 'loss_module_train.blocks.7.attn.proj.bias', 'loss_module_train.blocks.7.ls1.gamma', 'loss_module_train.blocks.7.norm2.weight', 'loss_module_train.blocks.7.norm2.bias', 'loss_module_train.blocks.7.mlp.fc1.weight', 'loss_module_train.blocks.7.mlp.fc1.bias', 'loss_module_train.blocks.7.mlp.fc2.weight', 'loss_module_train.blocks.7.mlp.fc2.bias', 'loss_module_train.blocks.7.ls2.gamma', 'loss_module_train.blocks.8.norm1.weight', 'loss_module_train.blocks.8.norm1.bias', 'loss_module_train.blocks.8.attn.qkv.weight', 'loss_module_train.blocks.8.attn.qkv.bias', 'loss_module_train.blocks.8.attn.proj.weight', 'loss_module_train.blocks.8.attn.proj.bias', 'loss_module_train.blocks.8.ls1.gamma', 'loss_module_train.blocks.8.norm2.weight', 'loss_module_train.blocks.8.norm2.bias', 'loss_module_train.blocks.8.mlp.fc1.weight', 'loss_module_train.blocks.8.mlp.fc1.bias', 'loss_module_train.blocks.8.mlp.fc2.weight', 'loss_module_train.blocks.8.mlp.fc2.bias', 'loss_module_train.blocks.8.ls2.gamma', 'loss_module_train.blocks.9.norm1.weight', 'loss_module_train.blocks.9.norm1.bias', 'loss_module_train.blocks.9.attn.qkv.weight', 'loss_module_train.blocks.9.attn.qkv.bias', 'loss_module_train.blocks.9.attn.proj.weight', 'loss_module_train.blocks.9.attn.proj.bias', 'loss_module_train.blocks.9.ls1.gamma', 'loss_module_train.blocks.9.norm2.weight', 'loss_module_train.blocks.9.norm2.bias', 'loss_module_train.blocks.9.mlp.fc1.weight', 'loss_module_train.blocks.9.mlp.fc1.bias', 'loss_module_train.blocks.9.mlp.fc2.weight', 'loss_module_train.blocks.9.mlp.fc2.bias', 'loss_module_train.blocks.9.ls2.gamma', 'loss_module_train.blocks.10.norm1.weight', 'loss_module_train.blocks.10.norm1.bias', 'loss_module_train.blocks.10.attn.qkv.weight', 'loss_module_train.blocks.10.attn.qkv.bias', 'loss_module_train.blocks.10.attn.proj.weight', 'loss_module_train.blocks.10.attn.proj.bias', 'loss_module_train.blocks.10.ls1.gamma', 'loss_module_train.blocks.10.norm2.weight', 'loss_module_train.blocks.10.norm2.bias', 'loss_module_train.blocks.10.mlp.fc1.weight', 'loss_module_train.blocks.10.mlp.fc1.bias', 'loss_module_train.blocks.10.mlp.fc2.weight', 'loss_module_train.blocks.10.mlp.fc2.bias', 'loss_module_train.blocks.10.ls2.gamma', 'loss_module_train.blocks.11.norm1.weight', 'loss_module_train.blocks.11.norm1.bias', 'loss_module_train.blocks.11.attn.qkv.weight', 'loss_module_train.blocks.11.attn.qkv.bias', 'loss_module_train.blocks.11.attn.proj.weight', 'loss_module_train.blocks.11.attn.proj.bias', 'loss_module_train.blocks.11.ls1.gamma', 'loss_module_train.blocks.11.norm2.weight', 'loss_module_train.blocks.11.norm2.bias', 'loss_module_train.blocks.11.mlp.fc1.weight', 'loss_module_train.blocks.11.mlp.fc1.bias', 'loss_module_train.blocks.11.mlp.fc2.weight', 'loss_module_train.blocks.11.mlp.fc2.bias', 'loss_module_train.blocks.11.ls2.gamma', 'loss_module_train.blocks.12.norm1.weight', 'loss_module_train.blocks.12.norm1.bias', 'loss_module_train.blocks.12.attn.qkv.weight', 'loss_module_train.blocks.12.attn.qkv.bias', 'loss_module_train.blocks.12.attn.proj.weight', 'loss_module_train.blocks.12.attn.proj.bias', 'loss_module_train.blocks.12.ls1.gamma', 'loss_module_train.blocks.12.norm2.weight', 'loss_module_train.blocks.12.norm2.bias', 'loss_module_train.blocks.12.mlp.fc1.weight', 'loss_module_train.blocks.12.mlp.fc1.bias', 'loss_module_train.blocks.12.mlp.fc2.weight', 'loss_module_train.blocks.12.mlp.fc2.bias', 'loss_module_train.blocks.12.ls2.gamma', 'loss_module_train.blocks.13.norm1.weight', 'loss_module_train.blocks.13.norm1.bias', 'loss_module_train.blocks.13.attn.qkv.weight', 'loss_module_train.blocks.13.attn.qkv.bias', 'loss_module_train.blocks.13.attn.proj.weight', 'loss_module_train.blocks.13.attn.proj.bias', 'loss_module_train.blocks.13.ls1.gamma', 'loss_module_train.blocks.13.norm2.weight', 'loss_module_train.blocks.13.norm2.bias', 'loss_module_train.blocks.13.mlp.fc1.weight', 'loss_module_train.blocks.13.mlp.fc1.bias', 'loss_module_train.blocks.13.mlp.fc2.weight', 'loss_module_train.blocks.13.mlp.fc2.bias', 'loss_module_train.blocks.13.ls2.gamma', 'loss_module_train.blocks.14.norm1.weight', 'loss_module_train.blocks.14.norm1.bias', 'loss_module_train.blocks.14.attn.qkv.weight', 'loss_module_train.blocks.14.attn.qkv.bias', 'loss_module_train.blocks.14.attn.proj.weight', 'loss_module_train.blocks.14.attn.proj.bias', 'loss_module_train.blocks.14.ls1.gamma', 'loss_module_train.blocks.14.norm2.weight', 'loss_module_train.blocks.14.norm2.bias', 'loss_module_train.blocks.14.mlp.fc1.weight', 'loss_module_train.blocks.14.mlp.fc1.bias', 'loss_module_train.blocks.14.mlp.fc2.weight', 'loss_module_train.blocks.14.mlp.fc2.bias', 'loss_module_train.blocks.14.ls2.gamma', 'loss_module_train.blocks.15.norm1.weight', 'loss_module_train.blocks.15.norm1.bias', 'loss_module_train.blocks.15.attn.qkv.weight', 'loss_module_train.blocks.15.attn.qkv.bias', 'loss_module_train.blocks.15.attn.proj.weight', 'loss_module_train.blocks.15.attn.proj.bias', 'loss_module_train.blocks.15.ls1.gamma', 'loss_module_train.blocks.15.norm2.weight', 'loss_module_train.blocks.15.norm2.bias', 'loss_module_train.blocks.15.mlp.fc1.weight', 'loss_module_train.blocks.15.mlp.fc1.bias', 'loss_module_train.blocks.15.mlp.fc2.weight', 'loss_module_train.blocks.15.mlp.fc2.bias', 'loss_module_train.blocks.15.ls2.gamma', 'loss_module_train.blocks.16.norm1.weight', 'loss_module_train.blocks.16.norm1.bias', 'loss_module_train.blocks.16.attn.qkv.weight', 'loss_module_train.blocks.16.attn.qkv.bias', 'loss_module_train.blocks.16.attn.proj.weight', 'loss_module_train.blocks.16.attn.proj.bias', 'loss_module_train.blocks.16.ls1.gamma', 'loss_module_train.blocks.16.norm2.weight', 'loss_module_train.blocks.16.norm2.bias', 'loss_module_train.blocks.16.mlp.fc1.weight', 'loss_module_train.blocks.16.mlp.fc1.bias', 'loss_module_train.blocks.16.mlp.fc2.weight', 'loss_module_train.blocks.16.mlp.fc2.bias', 'loss_module_train.blocks.16.ls2.gamma', 'loss_module_train.blocks.17.norm1.weight', 'loss_module_train.blocks.17.norm1.bias', 'loss_module_train.blocks.17.attn.qkv.weight', 'loss_module_train.blocks.17.attn.qkv.bias', 'loss_module_train.blocks.17.attn.proj.weight', 'loss_module_train.blocks.17.attn.proj.bias', 'loss_module_train.blocks.17.ls1.gamma', 'loss_module_train.blocks.17.norm2.weight', 'loss_module_train.blocks.17.norm2.bias', 'loss_module_train.blocks.17.mlp.fc1.weight', 'loss_module_train.blocks.17.mlp.fc1.bias', 'loss_module_train.blocks.17.mlp.fc2.weight', 'loss_module_train.blocks.17.mlp.fc2.bias', 'loss_module_train.blocks.17.ls2.gamma', 'loss_module_train.blocks.18.norm1.weight', 'loss_module_train.blocks.18.norm1.bias', 'loss_module_train.blocks.18.attn.qkv.weight', 'loss_module_train.blocks.18.attn.qkv.bias', 'loss_module_train.blocks.18.attn.proj.weight', 'loss_module_train.blocks.18.attn.proj.bias', 'loss_module_train.blocks.18.ls1.gamma', 'loss_module_train.blocks.18.norm2.weight', 'loss_module_train.blocks.18.norm2.bias', 'loss_module_train.blocks.18.mlp.fc1.weight', 'loss_module_train.blocks.18.mlp.fc1.bias', 'loss_module_train.blocks.18.mlp.fc2.weight', 'loss_module_train.blocks.18.mlp.fc2.bias', 'loss_module_train.blocks.18.ls2.gamma', 'loss_module_train.blocks.19.norm1.weight', 'loss_module_train.blocks.19.norm1.bias', 'loss_module_train.blocks.19.attn.qkv.weight', 'loss_module_train.blocks.19.attn.qkv.bias', 'loss_module_train.blocks.19.attn.proj.weight', 'loss_module_train.blocks.19.attn.proj.bias', 'loss_module_train.blocks.19.ls1.gamma', 'loss_module_train.blocks.19.norm2.weight', 'loss_module_train.blocks.19.norm2.bias', 'loss_module_train.blocks.19.mlp.fc1.weight', 'loss_module_train.blocks.19.mlp.fc1.bias', 'loss_module_train.blocks.19.mlp.fc2.weight', 'loss_module_train.blocks.19.mlp.fc2.bias', 'loss_module_train.blocks.19.ls2.gamma', 'loss_module_train.blocks.20.norm1.weight', 'loss_module_train.blocks.20.norm1.bias', 'loss_module_train.blocks.20.attn.qkv.weight', 'loss_module_train.blocks.20.attn.qkv.bias', 'loss_module_train.blocks.20.attn.proj.weight', 'loss_module_train.blocks.20.attn.proj.bias', 'loss_module_train.blocks.20.ls1.gamma', 'loss_module_train.blocks.20.norm2.weight', 'loss_module_train.blocks.20.norm2.bias', 'loss_module_train.blocks.20.mlp.fc1.weight', 'loss_module_train.blocks.20.mlp.fc1.bias', 'loss_module_train.blocks.20.mlp.fc2.weight', 'loss_module_train.blocks.20.mlp.fc2.bias', 'loss_module_train.blocks.20.ls2.gamma', 'loss_module_train.blocks.21.norm1.weight', 'loss_module_train.blocks.21.norm1.bias', 'loss_module_train.blocks.21.attn.qkv.weight', 'loss_module_train.blocks.21.attn.qkv.bias', 'loss_module_train.blocks.21.attn.proj.weight', 'loss_module_train.blocks.21.attn.proj.bias', 'loss_module_train.blocks.21.ls1.gamma', 'loss_module_train.blocks.21.norm2.weight', 'loss_module_train.blocks.21.norm2.bias', 'loss_module_train.blocks.21.mlp.fc1.weight', 'loss_module_train.blocks.21.mlp.fc1.bias', 'loss_module_train.blocks.21.mlp.fc2.weight', 'loss_module_train.blocks.21.mlp.fc2.bias', 'loss_module_train.blocks.21.ls2.gamma', 'loss_module_train.blocks.22.norm1.weight', 'loss_module_train.blocks.22.norm1.bias', 'loss_module_train.blocks.22.attn.qkv.weight', 'loss_module_train.blocks.22.attn.qkv.bias', 'loss_module_train.blocks.22.attn.proj.weight', 'loss_module_train.blocks.22.attn.proj.bias', 'loss_module_train.blocks.22.ls1.gamma', 'loss_module_train.blocks.22.norm2.weight', 'loss_module_train.blocks.22.norm2.bias', 'loss_module_train.blocks.22.mlp.fc1.weight', 'loss_module_train.blocks.22.mlp.fc1.bias', 'loss_module_train.blocks.22.mlp.fc2.weight', 'loss_module_train.blocks.22.mlp.fc2.bias', 'loss_module_train.blocks.22.ls2.gamma', 'loss_module_train.blocks.23.norm1.weight', 'loss_module_train.blocks.23.norm1.bias', 'loss_module_train.blocks.23.attn.qkv.weight', 'loss_module_train.blocks.23.attn.qkv.bias', 'loss_module_train.blocks.23.attn.proj.weight', 'loss_module_train.blocks.23.attn.proj.bias', 'loss_module_train.blocks.23.ls1.gamma', 'loss_module_train.blocks.23.norm2.weight', 'loss_module_train.blocks.23.norm2.bias', 'loss_module_train.blocks.23.mlp.fc1.weight', 'loss_module_train.blocks.23.mlp.fc1.bias', 'loss_module_train.blocks.23.mlp.fc2.weight', 'loss_module_train.blocks.23.mlp.fc2.bias', 'loss_module_train.blocks.23.ls2.gamma', 'loss_module_train.norm.weight', 'loss_module_train.norm.bias', 'loss_module_train.l2sp_loss.cls_token', 'loss_module_train.l2sp_loss.patch_embed_proj_weight', 'loss_module_train.l2sp_loss.blocks_0_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_0_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_0_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_0_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_1_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_1_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_1_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_1_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_2_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_2_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_2_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_2_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_3_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_3_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_3_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_3_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_4_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_4_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_4_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_4_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_5_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_5_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_5_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_5_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_6_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_6_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_6_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_6_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_7_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_7_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_7_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_7_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_8_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_8_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_8_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_8_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_9_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_9_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_9_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_9_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_10_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_10_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_10_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_10_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_11_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_11_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_11_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_11_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_12_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_12_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_12_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_12_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_13_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_13_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_13_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_13_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_14_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_14_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_14_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_14_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_15_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_15_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_15_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_15_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_16_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_16_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_16_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_16_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_17_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_17_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_17_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_17_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_18_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_18_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_18_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_18_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_19_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_19_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_19_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_19_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_20_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_20_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_20_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_20_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_21_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_21_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_21_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_21_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_22_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_22_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_22_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_22_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_23_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_23_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_23_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_23_mlp_fc2_weight', 'loss_module_val.cls_token', 'loss_module_val.pos_embed', 'loss_module_val.patch_embed.proj.weight', 'loss_module_val.patch_embed.proj.bias', 'loss_module_val.blocks.0.norm1.weight', 'loss_module_val.blocks.0.norm1.bias', 'loss_module_val.blocks.0.attn.qkv.weight', 'loss_module_val.blocks.0.attn.qkv.bias', 'loss_module_val.blocks.0.attn.proj.weight', 'loss_module_val.blocks.0.attn.proj.bias', 'loss_module_val.blocks.0.ls1.gamma', 'loss_module_val.blocks.0.norm2.weight', 'loss_module_val.blocks.0.norm2.bias', 'loss_module_val.blocks.0.mlp.fc1.weight', 'loss_module_val.blocks.0.mlp.fc1.bias', 'loss_module_val.blocks.0.mlp.fc2.weight', 'loss_module_val.blocks.0.mlp.fc2.bias', 'loss_module_val.blocks.0.ls2.gamma', 'loss_module_val.blocks.1.norm1.weight', 'loss_module_val.blocks.1.norm1.bias', 'loss_module_val.blocks.1.attn.qkv.weight', 'loss_module_val.blocks.1.attn.qkv.bias', 'loss_module_val.blocks.1.attn.proj.weight', 'loss_module_val.blocks.1.attn.proj.bias', 'loss_module_val.blocks.1.ls1.gamma', 'loss_module_val.blocks.1.norm2.weight', 'loss_module_val.blocks.1.norm2.bias', 'loss_module_val.blocks.1.mlp.fc1.weight', 'loss_module_val.blocks.1.mlp.fc1.bias', 'loss_module_val.blocks.1.mlp.fc2.weight', 'loss_module_val.blocks.1.mlp.fc2.bias', 'loss_module_val.blocks.1.ls2.gamma', 'loss_module_val.blocks.2.norm1.weight', 'loss_module_val.blocks.2.norm1.bias', 'loss_module_val.blocks.2.attn.qkv.weight', 'loss_module_val.blocks.2.attn.qkv.bias', 'loss_module_val.blocks.2.attn.proj.weight', 'loss_module_val.blocks.2.attn.proj.bias', 'loss_module_val.blocks.2.ls1.gamma', 'loss_module_val.blocks.2.norm2.weight', 'loss_module_val.blocks.2.norm2.bias', 'loss_module_val.blocks.2.mlp.fc1.weight', 'loss_module_val.blocks.2.mlp.fc1.bias', 'loss_module_val.blocks.2.mlp.fc2.weight', 'loss_module_val.blocks.2.mlp.fc2.bias', 'loss_module_val.blocks.2.ls2.gamma', 'loss_module_val.blocks.3.norm1.weight', 'loss_module_val.blocks.3.norm1.bias', 'loss_module_val.blocks.3.attn.qkv.weight', 'loss_module_val.blocks.3.attn.qkv.bias', 'loss_module_val.blocks.3.attn.proj.weight', 'loss_module_val.blocks.3.attn.proj.bias', 'loss_module_val.blocks.3.ls1.gamma', 'loss_module_val.blocks.3.norm2.weight', 'loss_module_val.blocks.3.norm2.bias', 'loss_module_val.blocks.3.mlp.fc1.weight', 'loss_module_val.blocks.3.mlp.fc1.bias', 'loss_module_val.blocks.3.mlp.fc2.weight', 'loss_module_val.blocks.3.mlp.fc2.bias', 'loss_module_val.blocks.3.ls2.gamma', 'loss_module_val.blocks.4.norm1.weight', 'loss_module_val.blocks.4.norm1.bias', 'loss_module_val.blocks.4.attn.qkv.weight', 'loss_module_val.blocks.4.attn.qkv.bias', 'loss_module_val.blocks.4.attn.proj.weight', 'loss_module_val.blocks.4.attn.proj.bias', 'loss_module_val.blocks.4.ls1.gamma', 'loss_module_val.blocks.4.norm2.weight', 'loss_module_val.blocks.4.norm2.bias', 'loss_module_val.blocks.4.mlp.fc1.weight', 'loss_module_val.blocks.4.mlp.fc1.bias', 'loss_module_val.blocks.4.mlp.fc2.weight', 'loss_module_val.blocks.4.mlp.fc2.bias', 'loss_module_val.blocks.4.ls2.gamma', 'loss_module_val.blocks.5.norm1.weight', 'loss_module_val.blocks.5.norm1.bias', 'loss_module_val.blocks.5.attn.qkv.weight', 'loss_module_val.blocks.5.attn.qkv.bias', 'loss_module_val.blocks.5.attn.proj.weight', 'loss_module_val.blocks.5.attn.proj.bias', 'loss_module_val.blocks.5.ls1.gamma', 'loss_module_val.blocks.5.norm2.weight', 'loss_module_val.blocks.5.norm2.bias', 'loss_module_val.blocks.5.mlp.fc1.weight', 'loss_module_val.blocks.5.mlp.fc1.bias', 'loss_module_val.blocks.5.mlp.fc2.weight', 'loss_module_val.blocks.5.mlp.fc2.bias', 'loss_module_val.blocks.5.ls2.gamma', 'loss_module_val.blocks.6.norm1.weight', 'loss_module_val.blocks.6.norm1.bias', 'loss_module_val.blocks.6.attn.qkv.weight', 'loss_module_val.blocks.6.attn.qkv.bias', 'loss_module_val.blocks.6.attn.proj.weight', 'loss_module_val.blocks.6.attn.proj.bias', 'loss_module_val.blocks.6.ls1.gamma', 'loss_module_val.blocks.6.norm2.weight', 'loss_module_val.blocks.6.norm2.bias', 'loss_module_val.blocks.6.mlp.fc1.weight', 'loss_module_val.blocks.6.mlp.fc1.bias', 'loss_module_val.blocks.6.mlp.fc2.weight', 'loss_module_val.blocks.6.mlp.fc2.bias', 'loss_module_val.blocks.6.ls2.gamma', 'loss_module_val.blocks.7.norm1.weight', 'loss_module_val.blocks.7.norm1.bias', 'loss_module_val.blocks.7.attn.qkv.weight', 'loss_module_val.blocks.7.attn.qkv.bias', 'loss_module_val.blocks.7.attn.proj.weight', 'loss_module_val.blocks.7.attn.proj.bias', 'loss_module_val.blocks.7.ls1.gamma', 'loss_module_val.blocks.7.norm2.weight', 'loss_module_val.blocks.7.norm2.bias', 'loss_module_val.blocks.7.mlp.fc1.weight', 'loss_module_val.blocks.7.mlp.fc1.bias', 'loss_module_val.blocks.7.mlp.fc2.weight', 'loss_module_val.blocks.7.mlp.fc2.bias', 'loss_module_val.blocks.7.ls2.gamma', 'loss_module_val.blocks.8.norm1.weight', 'loss_module_val.blocks.8.norm1.bias', 'loss_module_val.blocks.8.attn.qkv.weight', 'loss_module_val.blocks.8.attn.qkv.bias', 'loss_module_val.blocks.8.attn.proj.weight', 'loss_module_val.blocks.8.attn.proj.bias', 'loss_module_val.blocks.8.ls1.gamma', 'loss_module_val.blocks.8.norm2.weight', 'loss_module_val.blocks.8.norm2.bias', 'loss_module_val.blocks.8.mlp.fc1.weight', 'loss_module_val.blocks.8.mlp.fc1.bias', 'loss_module_val.blocks.8.mlp.fc2.weight', 'loss_module_val.blocks.8.mlp.fc2.bias', 'loss_module_val.blocks.8.ls2.gamma', 'loss_module_val.blocks.9.norm1.weight', 'loss_module_val.blocks.9.norm1.bias', 'loss_module_val.blocks.9.attn.qkv.weight', 'loss_module_val.blocks.9.attn.qkv.bias', 'loss_module_val.blocks.9.attn.proj.weight', 'loss_module_val.blocks.9.attn.proj.bias', 'loss_module_val.blocks.9.ls1.gamma', 'loss_module_val.blocks.9.norm2.weight', 'loss_module_val.blocks.9.norm2.bias', 'loss_module_val.blocks.9.mlp.fc1.weight', 'loss_module_val.blocks.9.mlp.fc1.bias', 'loss_module_val.blocks.9.mlp.fc2.weight', 'loss_module_val.blocks.9.mlp.fc2.bias', 'loss_module_val.blocks.9.ls2.gamma', 'loss_module_val.blocks.10.norm1.weight', 'loss_module_val.blocks.10.norm1.bias', 'loss_module_val.blocks.10.attn.qkv.weight', 'loss_module_val.blocks.10.attn.qkv.bias', 'loss_module_val.blocks.10.attn.proj.weight', 'loss_module_val.blocks.10.attn.proj.bias', 'loss_module_val.blocks.10.ls1.gamma', 'loss_module_val.blocks.10.norm2.weight', 'loss_module_val.blocks.10.norm2.bias', 'loss_module_val.blocks.10.mlp.fc1.weight', 'loss_module_val.blocks.10.mlp.fc1.bias', 'loss_module_val.blocks.10.mlp.fc2.weight', 'loss_module_val.blocks.10.mlp.fc2.bias', 'loss_module_val.blocks.10.ls2.gamma', 'loss_module_val.blocks.11.norm1.weight', 'loss_module_val.blocks.11.norm1.bias', 'loss_module_val.blocks.11.attn.qkv.weight', 'loss_module_val.blocks.11.attn.qkv.bias', 'loss_module_val.blocks.11.attn.proj.weight', 'loss_module_val.blocks.11.attn.proj.bias', 'loss_module_val.blocks.11.ls1.gamma', 'loss_module_val.blocks.11.norm2.weight', 'loss_module_val.blocks.11.norm2.bias', 'loss_module_val.blocks.11.mlp.fc1.weight', 'loss_module_val.blocks.11.mlp.fc1.bias', 'loss_module_val.blocks.11.mlp.fc2.weight', 'loss_module_val.blocks.11.mlp.fc2.bias', 'loss_module_val.blocks.11.ls2.gamma', 'loss_module_val.blocks.12.norm1.weight', 'loss_module_val.blocks.12.norm1.bias', 'loss_module_val.blocks.12.attn.qkv.weight', 'loss_module_val.blocks.12.attn.qkv.bias', 'loss_module_val.blocks.12.attn.proj.weight', 'loss_module_val.blocks.12.attn.proj.bias', 'loss_module_val.blocks.12.ls1.gamma', 'loss_module_val.blocks.12.norm2.weight', 'loss_module_val.blocks.12.norm2.bias', 'loss_module_val.blocks.12.mlp.fc1.weight', 'loss_module_val.blocks.12.mlp.fc1.bias', 'loss_module_val.blocks.12.mlp.fc2.weight', 'loss_module_val.blocks.12.mlp.fc2.bias', 'loss_module_val.blocks.12.ls2.gamma', 'loss_module_val.blocks.13.norm1.weight', 'loss_module_val.blocks.13.norm1.bias', 'loss_module_val.blocks.13.attn.qkv.weight', 'loss_module_val.blocks.13.attn.qkv.bias', 'loss_module_val.blocks.13.attn.proj.weight', 'loss_module_val.blocks.13.attn.proj.bias', 'loss_module_val.blocks.13.ls1.gamma', 'loss_module_val.blocks.13.norm2.weight', 'loss_module_val.blocks.13.norm2.bias', 'loss_module_val.blocks.13.mlp.fc1.weight', 'loss_module_val.blocks.13.mlp.fc1.bias', 'loss_module_val.blocks.13.mlp.fc2.weight', 'loss_module_val.blocks.13.mlp.fc2.bias', 'loss_module_val.blocks.13.ls2.gamma', 'loss_module_val.blocks.14.norm1.weight', 'loss_module_val.blocks.14.norm1.bias', 'loss_module_val.blocks.14.attn.qkv.weight', 'loss_module_val.blocks.14.attn.qkv.bias', 'loss_module_val.blocks.14.attn.proj.weight', 'loss_module_val.blocks.14.attn.proj.bias', 'loss_module_val.blocks.14.ls1.gamma', 'loss_module_val.blocks.14.norm2.weight', 'loss_module_val.blocks.14.norm2.bias', 'loss_module_val.blocks.14.mlp.fc1.weight', 'loss_module_val.blocks.14.mlp.fc1.bias', 'loss_module_val.blocks.14.mlp.fc2.weight', 'loss_module_val.blocks.14.mlp.fc2.bias', 'loss_module_val.blocks.14.ls2.gamma', 'loss_module_val.blocks.15.norm1.weight', 'loss_module_val.blocks.15.norm1.bias', 'loss_module_val.blocks.15.attn.qkv.weight', 'loss_module_val.blocks.15.attn.qkv.bias', 'loss_module_val.blocks.15.attn.proj.weight', 'loss_module_val.blocks.15.attn.proj.bias', 'loss_module_val.blocks.15.ls1.gamma', 'loss_module_val.blocks.15.norm2.weight', 'loss_module_val.blocks.15.norm2.bias', 'loss_module_val.blocks.15.mlp.fc1.weight', 'loss_module_val.blocks.15.mlp.fc1.bias', 'loss_module_val.blocks.15.mlp.fc2.weight', 'loss_module_val.blocks.15.mlp.fc2.bias', 'loss_module_val.blocks.15.ls2.gamma', 'loss_module_val.blocks.16.norm1.weight', 'loss_module_val.blocks.16.norm1.bias', 'loss_module_val.blocks.16.attn.qkv.weight', 'loss_module_val.blocks.16.attn.qkv.bias', 'loss_module_val.blocks.16.attn.proj.weight', 'loss_module_val.blocks.16.attn.proj.bias', 'loss_module_val.blocks.16.ls1.gamma', 'loss_module_val.blocks.16.norm2.weight', 'loss_module_val.blocks.16.norm2.bias', 'loss_module_val.blocks.16.mlp.fc1.weight', 'loss_module_val.blocks.16.mlp.fc1.bias', 'loss_module_val.blocks.16.mlp.fc2.weight', 'loss_module_val.blocks.16.mlp.fc2.bias', 'loss_module_val.blocks.16.ls2.gamma', 'loss_module_val.blocks.17.norm1.weight', 'loss_module_val.blocks.17.norm1.bias', 'loss_module_val.blocks.17.attn.qkv.weight', 'loss_module_val.blocks.17.attn.qkv.bias', 'loss_module_val.blocks.17.attn.proj.weight', 'loss_module_val.blocks.17.attn.proj.bias', 'loss_module_val.blocks.17.ls1.gamma', 'loss_module_val.blocks.17.norm2.weight', 'loss_module_val.blocks.17.norm2.bias', 'loss_module_val.blocks.17.mlp.fc1.weight', 'loss_module_val.blocks.17.mlp.fc1.bias', 'loss_module_val.blocks.17.mlp.fc2.weight', 'loss_module_val.blocks.17.mlp.fc2.bias', 'loss_module_val.blocks.17.ls2.gamma', 'loss_module_val.blocks.18.norm1.weight', 'loss_module_val.blocks.18.norm1.bias', 'loss_module_val.blocks.18.attn.qkv.weight', 'loss_module_val.blocks.18.attn.qkv.bias', 'loss_module_val.blocks.18.attn.proj.weight', 'loss_module_val.blocks.18.attn.proj.bias', 'loss_module_val.blocks.18.ls1.gamma', 'loss_module_val.blocks.18.norm2.weight', 'loss_module_val.blocks.18.norm2.bias', 'loss_module_val.blocks.18.mlp.fc1.weight', 'loss_module_val.blocks.18.mlp.fc1.bias', 'loss_module_val.blocks.18.mlp.fc2.weight', 'loss_module_val.blocks.18.mlp.fc2.bias', 'loss_module_val.blocks.18.ls2.gamma', 'loss_module_val.blocks.19.norm1.weight', 'loss_module_val.blocks.19.norm1.bias', 'loss_module_val.blocks.19.attn.qkv.weight', 'loss_module_val.blocks.19.attn.qkv.bias', 'loss_module_val.blocks.19.attn.proj.weight', 'loss_module_val.blocks.19.attn.proj.bias', 'loss_module_val.blocks.19.ls1.gamma', 'loss_module_val.blocks.19.norm2.weight', 'loss_module_val.blocks.19.norm2.bias', 'loss_module_val.blocks.19.mlp.fc1.weight', 'loss_module_val.blocks.19.mlp.fc1.bias', 'loss_module_val.blocks.19.mlp.fc2.weight', 'loss_module_val.blocks.19.mlp.fc2.bias', 'loss_module_val.blocks.19.ls2.gamma', 'loss_module_val.blocks.20.norm1.weight', 'loss_module_val.blocks.20.norm1.bias', 'loss_module_val.blocks.20.attn.qkv.weight', 'loss_module_val.blocks.20.attn.qkv.bias', 'loss_module_val.blocks.20.attn.proj.weight', 'loss_module_val.blocks.20.attn.proj.bias', 'loss_module_val.blocks.20.ls1.gamma', 'loss_module_val.blocks.20.norm2.weight', 'loss_module_val.blocks.20.norm2.bias', 'loss_module_val.blocks.20.mlp.fc1.weight', 'loss_module_val.blocks.20.mlp.fc1.bias', 'loss_module_val.blocks.20.mlp.fc2.weight', 'loss_module_val.blocks.20.mlp.fc2.bias', 'loss_module_val.blocks.20.ls2.gamma', 'loss_module_val.blocks.21.norm1.weight', 'loss_module_val.blocks.21.norm1.bias', 'loss_module_val.blocks.21.attn.qkv.weight', 'loss_module_val.blocks.21.attn.qkv.bias', 'loss_module_val.blocks.21.attn.proj.weight', 'loss_module_val.blocks.21.attn.proj.bias', 'loss_module_val.blocks.21.ls1.gamma', 'loss_module_val.blocks.21.norm2.weight', 'loss_module_val.blocks.21.norm2.bias', 'loss_module_val.blocks.21.mlp.fc1.weight', 'loss_module_val.blocks.21.mlp.fc1.bias', 'loss_module_val.blocks.21.mlp.fc2.weight', 'loss_module_val.blocks.21.mlp.fc2.bias', 'loss_module_val.blocks.21.ls2.gamma', 'loss_module_val.blocks.22.norm1.weight', 'loss_module_val.blocks.22.norm1.bias', 'loss_module_val.blocks.22.attn.qkv.weight', 'loss_module_val.blocks.22.attn.qkv.bias', 'loss_module_val.blocks.22.attn.proj.weight', 'loss_module_val.blocks.22.attn.proj.bias', 'loss_module_val.blocks.22.ls1.gamma', 'loss_module_val.blocks.22.norm2.weight', 'loss_module_val.blocks.22.norm2.bias', 'loss_module_val.blocks.22.mlp.fc1.weight', 'loss_module_val.blocks.22.mlp.fc1.bias', 'loss_module_val.blocks.22.mlp.fc2.weight', 'loss_module_val.blocks.22.mlp.fc2.bias', 'loss_module_val.blocks.22.ls2.gamma', 'loss_module_val.blocks.23.norm1.weight', 'loss_module_val.blocks.23.norm1.bias', 'loss_module_val.blocks.23.attn.qkv.weight', 'loss_module_val.blocks.23.attn.qkv.bias', 'loss_module_val.blocks.23.attn.proj.weight', 'loss_module_val.blocks.23.attn.proj.bias', 'loss_module_val.blocks.23.ls1.gamma', 'loss_module_val.blocks.23.norm2.weight', 'loss_module_val.blocks.23.norm2.bias', 'loss_module_val.blocks.23.mlp.fc1.weight', 'loss_module_val.blocks.23.mlp.fc1.bias', 'loss_module_val.blocks.23.mlp.fc2.weight', 'loss_module_val.blocks.23.mlp.fc2.bias', 'loss_module_val.blocks.23.ls2.gamma', 'loss_module_val.norm.weight', 'loss_module_val.norm.bias', 'loss_module_val.l2sp_loss.cls_token', 'loss_module_val.l2sp_loss.patch_embed_proj_weight', 'loss_module_val.l2sp_loss.blocks_0_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_0_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_0_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_0_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_1_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_1_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_1_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_1_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_2_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_2_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_2_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_2_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_3_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_3_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_3_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_3_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_4_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_4_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_4_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_4_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_5_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_5_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_5_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_5_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_6_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_6_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_6_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_6_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_7_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_7_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_7_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_7_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_8_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_8_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_8_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_8_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_9_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_9_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_9_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_9_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_10_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_10_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_10_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_10_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_11_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_11_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_11_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_11_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_12_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_12_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_12_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_12_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_13_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_13_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_13_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_13_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_14_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_14_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_14_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_14_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_15_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_15_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_15_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_15_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_16_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_16_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_16_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_16_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_17_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_17_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_17_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_17_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_18_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_18_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_18_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_18_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_19_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_19_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_19_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_19_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_20_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_20_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_20_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_20_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_21_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_21_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_21_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_21_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_22_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_22_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_22_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_22_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_23_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_23_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_23_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_23_mlp_fc2_weight'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Approach 1: load the chcekpoints into a standard ViT moel\n",
    "\n",
    "vit_model_joscha = create_model('vit_large_patch14_dinov2.lvd142m', pretrained=False, img_size=img_size)\n",
    "vit_model_robert = create_model('vit_large_patch14_dinov2.lvd142m', pretrained=False, img_size=img_size)\n",
    "vit_model_vincent = create_model('vit_large_patch14_dinov2.lvd142m', pretrained=False, img_size=img_size)\n",
    "vit_model_best = create_model('vit_large_patch14_dinov2.lvd142m', pretrained=False, img_size=img_size)\n",
    "\n",
    "# interpolate pos_embeddings\n",
    "interpolated_state_dict_joscha = interpolate_positional_embedding(cleaned_state_dict_joscha, vit_model_joscha, 16)\n",
    "interpolated_state_dict_robert = interpolate_positional_embedding(cleaned_state_dict_robert, vit_model_robert, 16)\n",
    "interpolated_state_dict_vincent = interpolate_positional_embedding(cleaned_state_dict_vincent, vit_model_vincent, 16)\n",
    "interpolated_state_dict_best = interpolate_positional_embedding(cleaned_state_dict_best, vit_model_best, 16)\n",
    "\n",
    "vit_model_joscha.load_state_dict(interpolated_state_dict_joscha, strict=False)\n",
    "vit_model_robert.load_state_dict(interpolated_state_dict_robert, strict=False)\n",
    "vit_model_vincent.load_state_dict(interpolated_state_dict_vincent, strict=False)\n",
    "vit_model_best.load_state_dict(interpolated_state_dict_best, strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the data\n",
    "# # Load data (SPAC or Bristol)\n",
    "# # test_images, test_labels = load_images_and_labels(test_folder, transform_standard)\n",
    "# # test_images, test_labels = load_images_and_labels(bristol_folder, transform_standard)\n",
    "# test_images, test_labels = load_images_and_labels(val_folder, transform_standard)\n",
    "# print(test_labels[:5])\n",
    "\n",
    "# # train_images, train_labels = load_images_and_labels(train_folder)\n",
    "# # val_images, val_labels = load_images_and_labels(val_folder)\n",
    "# # all_images, all_labels = load_images_and_labels(all_folder)\n",
    "\n",
    "# test_images_tensor = torch.stack(test_images)\n",
    "# # train_images_tensor = torch.stack(train_images)\n",
    "# # val_images_tensor = torch.stack(val_images)\n",
    "# # all_images_tensor = torch.stack(all_images)\n",
    "\n",
    "# # train_val_images_tensor = torch.cat((train_images_tensor, val_images_tensor), 0)\n",
    "# # train_val_labels = train_labels + val_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate with standard models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit_large_dinov2_ssl_joscha:\n",
      "ViT Accuracy with 5-nearest neighbors: 0.7124, F1_score: 0.496258\n",
      "supervised_dinov2_large:\n",
      "ViT Accuracy with 5-nearest neighbors: 0.8170, F1_score: 0.648936\n",
      "ssl_vincent_vit_large:\n",
      "ViT Accuracy with 5-nearest neighbors: 0.6993, F1_score: 0.514996\n",
      "best-model_vit_large_dinoV2:\n",
      "ViT Accuracy with 5-nearest neighbors: 0.8431, F1_score: 0.687583\n"
     ]
    }
   ],
   "source": [
    "vit_model_joscha.eval()\n",
    "vit_model_robert.eval()\n",
    "vit_model_vincent.eval()\n",
    "vit_model_best.eval()\n",
    "\n",
    "standard_embeddings_joscha = generate_embeddings(vit_model_joscha, test_images_tensor)\n",
    "stadard_embeddings_robert = generate_embeddings(vit_model_robert, test_images_tensor)\n",
    "standard_embeddings_vincent = generate_embeddings(vit_model_vincent, test_images_tensor)\n",
    "standard_embeddings_best = generate_embeddings(vit_model_best, test_images_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit_large_dinov2_ssl_joscha:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [153, 372]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     knn_classifier(embeddings, labels)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mclassify_and_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_joscha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvit_model_joscha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstandard_embeddings_joscha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m classify_and_print(model_name_robert, vit_model_robert, stadard_embeddings_robert, test_labels)\n\u001b[1;32m      8\u001b[0m classify_and_print(model_name_vincent, vit_model_vincent, standard_embeddings_vincent, test_labels)\n",
      "Cell \u001b[0;32mIn[34], line 4\u001b[0m, in \u001b[0;36mclassify_and_print\u001b[0;34m(model_name, model, embeddings, labels)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassify_and_print\u001b[39m(model_name, model, embeddings,labels):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mknn_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m, in \u001b[0;36mknn_classifier\u001b[0;34m(embeddings, labels)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Use k+1 neighbors to account for excluding self\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     vit_knn \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39mk \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mvit_knn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fit KNN on all data\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     vit_y_pred \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, test_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(embeddings):\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m# Find k+1 neighbors (including the test sample itself)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/research/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/research/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:238\u001b[0m, in \u001b[0;36mKNeighborsClassifier.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# KNeighborsClassifier.metric is not validated yet\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    219\u001b[0m )\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the k-nearest neighbors classifier from the training dataset.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m        The fitted k-nearest neighbors classifier.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/research/lib/python3.10/site-packages/sklearn/neighbors/_base.py:475\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (KDTree, BallTree, NeighborsBase)):\n\u001b[0;32m--> 475\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;66;03m# Classification targets require a specific format\u001b[39;00m\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/research/lib/python3.10/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/envs/research/lib/python3.10/site-packages/sklearn/utils/validation.py:1320\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1301\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1302\u001b[0m     X,\n\u001b[1;32m   1303\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1316\u001b[0m )\n\u001b[1;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m-> 1320\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/opt/conda/envs/research/lib/python3.10/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [153, 372]"
     ]
    }
   ],
   "source": [
    "# evaluate with unfiltered embeddings\n",
    "def classify_and_print(model_name, model, embeddings,labels):\n",
    "    print(f\"{model_name}:\")\n",
    "    knn_classifier(embeddings, labels)\n",
    "    \n",
    "classify_and_print(model_name_joscha, vit_model_joscha, standard_embeddings_joscha, test_labels)\n",
    "classify_and_print(model_name_robert, vit_model_robert, stadard_embeddings_robert, test_labels)\n",
    "classify_and_print(model_name_vincent, vit_model_vincent, standard_embeddings_vincent, test_labels)\n",
    "classify_and_print(model_name_best, vit_model_best, standard_embeddings_best, test_labels)\n",
    "\n",
    "# evaluate with filtered embeddings\n",
    "filtered_embeddings_joscha, filtered_labels_joscha = filter_samples_by_threshold(standard_embeddings_joscha, test_labels, threshold=3)\n",
    "filtered_embeddings_robert, filtered_labels_robert = filter_samples_by_threshold(stadard_embeddings_robert, test_labels, threshold=3)\n",
    "filtered_embeddings_vincent, filtered_labels_vincent = filter_samples_by_threshold(standard_embeddings_vincent, test_labels, threshold=3)\n",
    "filtered_embeddings_best, filtered_labels_best = filter_samples_by_threshold(standard_embeddings_best, test_labels, threshold=3)\n",
    "\n",
    "print(\"Evaluate with filtered embeddings\")\n",
    "classify_and_print(model_name_joscha, vit_model_joscha, filtered_embeddings_joscha, filtered_labels_joscha)\n",
    "classify_and_print(model_name_robert, vit_model_robert, filtered_embeddings_robert, filtered_labels_robert)\n",
    "classify_and_print(model_name_vincent, vit_model_vincent, filtered_embeddings_vincent, filtered_labels_vincent)\n",
    "classify_and_print(model_name_best, vit_model_best, filtered_embeddings_best, filtered_labels_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Define custom ViT models for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom ViT for supervised_dinov2_large\n",
    "\n",
    "class CustomViT_supervised(nn.Module):\n",
    "    def __init__(self, base_vit):\n",
    "        super(CustomViT_supervised, self).__init__()\n",
    "        self.base_vit = base_vit\n",
    "        # Define embedding layer to match the missing keys\n",
    "        self.embedding_layer = nn.Linear(1024, 256)  # Assuming base_vit outputs 768-d embeddings\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the Vision Transformer backbone\n",
    "        x = self.base_vit.forward_features(x)  # Output size is [batch_size, num_tokens, 768]\n",
    "        print(f\"Output shape of base_vit: {x.shape}\")\n",
    "        \n",
    "        # Extract the [CLS] token embedding (assuming it's the first token)\n",
    "        x = x[:, 0, :]  # Shape: [batch_size, 768]\n",
    "        # print(f\"Output shape after selecting CLS token: {x.shape}\")\n",
    "        \n",
    "        # Pass through the custom embedding layer\n",
    "        x = self.embedding_layer(x)  # Linear transformation to 1024\n",
    "        # print(f\"Output shape of embedding_layer: {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test with the model wrapper only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original model wrapper\n",
    "embedding_id = \"linear\"\n",
    "# embedding_id = \"mlp\"\n",
    "# embedding_id = \"mlp_norm_dropout\"\n",
    "# embedding_id = \"linear_norm_dropout\"\n",
    "# embedding_id = \"\"\n",
    "# Custom ViT for supervised_dinov2_large\n",
    "model_wrapper = TimmWrapper(\n",
    "    backbone_name=\"vit_large_patch14_dinov2.lvd142m\",\n",
    "    embedding_size=256,  # Set based on your checkpoint\n",
    "    embedding_id=embedding_id,  # Ensure this matches the checkpoint (OG)\n",
    "    dropout_p=0.0,  # Set dropout probability\n",
    "    pool_mode=\"none\",  # Assuming no global pooling for Vision Transformers\n",
    "    img_size=224,  # Match the input size expected by the model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['loss_module_train.model.cls_token', 'loss_module_train.model.pos_embed', 'loss_module_train.model.patch_embed.proj.weight', 'loss_module_train.model.patch_embed.proj.bias', 'loss_module_train.model.blocks.0.norm1.weight', 'loss_module_train.model.blocks.0.norm1.bias', 'loss_module_train.model.blocks.0.attn.qkv.weight', 'loss_module_train.model.blocks.0.attn.qkv.bias', 'loss_module_train.model.blocks.0.attn.proj.weight', 'loss_module_train.model.blocks.0.attn.proj.bias', 'loss_module_train.model.blocks.0.ls1.gamma', 'loss_module_train.model.blocks.0.norm2.weight', 'loss_module_train.model.blocks.0.norm2.bias', 'loss_module_train.model.blocks.0.mlp.fc1.weight', 'loss_module_train.model.blocks.0.mlp.fc1.bias', 'loss_module_train.model.blocks.0.mlp.fc2.weight', 'loss_module_train.model.blocks.0.mlp.fc2.bias', 'loss_module_train.model.blocks.0.ls2.gamma', 'loss_module_train.model.blocks.1.norm1.weight', 'loss_module_train.model.blocks.1.norm1.bias', 'loss_module_train.model.blocks.1.attn.qkv.weight', 'loss_module_train.model.blocks.1.attn.qkv.bias', 'loss_module_train.model.blocks.1.attn.proj.weight', 'loss_module_train.model.blocks.1.attn.proj.bias', 'loss_module_train.model.blocks.1.ls1.gamma', 'loss_module_train.model.blocks.1.norm2.weight', 'loss_module_train.model.blocks.1.norm2.bias', 'loss_module_train.model.blocks.1.mlp.fc1.weight', 'loss_module_train.model.blocks.1.mlp.fc1.bias', 'loss_module_train.model.blocks.1.mlp.fc2.weight', 'loss_module_train.model.blocks.1.mlp.fc2.bias', 'loss_module_train.model.blocks.1.ls2.gamma', 'loss_module_train.model.blocks.2.norm1.weight', 'loss_module_train.model.blocks.2.norm1.bias', 'loss_module_train.model.blocks.2.attn.qkv.weight', 'loss_module_train.model.blocks.2.attn.qkv.bias', 'loss_module_train.model.blocks.2.attn.proj.weight', 'loss_module_train.model.blocks.2.attn.proj.bias', 'loss_module_train.model.blocks.2.ls1.gamma', 'loss_module_train.model.blocks.2.norm2.weight', 'loss_module_train.model.blocks.2.norm2.bias', 'loss_module_train.model.blocks.2.mlp.fc1.weight', 'loss_module_train.model.blocks.2.mlp.fc1.bias', 'loss_module_train.model.blocks.2.mlp.fc2.weight', 'loss_module_train.model.blocks.2.mlp.fc2.bias', 'loss_module_train.model.blocks.2.ls2.gamma', 'loss_module_train.model.blocks.3.norm1.weight', 'loss_module_train.model.blocks.3.norm1.bias', 'loss_module_train.model.blocks.3.attn.qkv.weight', 'loss_module_train.model.blocks.3.attn.qkv.bias', 'loss_module_train.model.blocks.3.attn.proj.weight', 'loss_module_train.model.blocks.3.attn.proj.bias', 'loss_module_train.model.blocks.3.ls1.gamma', 'loss_module_train.model.blocks.3.norm2.weight', 'loss_module_train.model.blocks.3.norm2.bias', 'loss_module_train.model.blocks.3.mlp.fc1.weight', 'loss_module_train.model.blocks.3.mlp.fc1.bias', 'loss_module_train.model.blocks.3.mlp.fc2.weight', 'loss_module_train.model.blocks.3.mlp.fc2.bias', 'loss_module_train.model.blocks.3.ls2.gamma', 'loss_module_train.model.blocks.4.norm1.weight', 'loss_module_train.model.blocks.4.norm1.bias', 'loss_module_train.model.blocks.4.attn.qkv.weight', 'loss_module_train.model.blocks.4.attn.qkv.bias', 'loss_module_train.model.blocks.4.attn.proj.weight', 'loss_module_train.model.blocks.4.attn.proj.bias', 'loss_module_train.model.blocks.4.ls1.gamma', 'loss_module_train.model.blocks.4.norm2.weight', 'loss_module_train.model.blocks.4.norm2.bias', 'loss_module_train.model.blocks.4.mlp.fc1.weight', 'loss_module_train.model.blocks.4.mlp.fc1.bias', 'loss_module_train.model.blocks.4.mlp.fc2.weight', 'loss_module_train.model.blocks.4.mlp.fc2.bias', 'loss_module_train.model.blocks.4.ls2.gamma', 'loss_module_train.model.blocks.5.norm1.weight', 'loss_module_train.model.blocks.5.norm1.bias', 'loss_module_train.model.blocks.5.attn.qkv.weight', 'loss_module_train.model.blocks.5.attn.qkv.bias', 'loss_module_train.model.blocks.5.attn.proj.weight', 'loss_module_train.model.blocks.5.attn.proj.bias', 'loss_module_train.model.blocks.5.ls1.gamma', 'loss_module_train.model.blocks.5.norm2.weight', 'loss_module_train.model.blocks.5.norm2.bias', 'loss_module_train.model.blocks.5.mlp.fc1.weight', 'loss_module_train.model.blocks.5.mlp.fc1.bias', 'loss_module_train.model.blocks.5.mlp.fc2.weight', 'loss_module_train.model.blocks.5.mlp.fc2.bias', 'loss_module_train.model.blocks.5.ls2.gamma', 'loss_module_train.model.blocks.6.norm1.weight', 'loss_module_train.model.blocks.6.norm1.bias', 'loss_module_train.model.blocks.6.attn.qkv.weight', 'loss_module_train.model.blocks.6.attn.qkv.bias', 'loss_module_train.model.blocks.6.attn.proj.weight', 'loss_module_train.model.blocks.6.attn.proj.bias', 'loss_module_train.model.blocks.6.ls1.gamma', 'loss_module_train.model.blocks.6.norm2.weight', 'loss_module_train.model.blocks.6.norm2.bias', 'loss_module_train.model.blocks.6.mlp.fc1.weight', 'loss_module_train.model.blocks.6.mlp.fc1.bias', 'loss_module_train.model.blocks.6.mlp.fc2.weight', 'loss_module_train.model.blocks.6.mlp.fc2.bias', 'loss_module_train.model.blocks.6.ls2.gamma', 'loss_module_train.model.blocks.7.norm1.weight', 'loss_module_train.model.blocks.7.norm1.bias', 'loss_module_train.model.blocks.7.attn.qkv.weight', 'loss_module_train.model.blocks.7.attn.qkv.bias', 'loss_module_train.model.blocks.7.attn.proj.weight', 'loss_module_train.model.blocks.7.attn.proj.bias', 'loss_module_train.model.blocks.7.ls1.gamma', 'loss_module_train.model.blocks.7.norm2.weight', 'loss_module_train.model.blocks.7.norm2.bias', 'loss_module_train.model.blocks.7.mlp.fc1.weight', 'loss_module_train.model.blocks.7.mlp.fc1.bias', 'loss_module_train.model.blocks.7.mlp.fc2.weight', 'loss_module_train.model.blocks.7.mlp.fc2.bias', 'loss_module_train.model.blocks.7.ls2.gamma', 'loss_module_train.model.blocks.8.norm1.weight', 'loss_module_train.model.blocks.8.norm1.bias', 'loss_module_train.model.blocks.8.attn.qkv.weight', 'loss_module_train.model.blocks.8.attn.qkv.bias', 'loss_module_train.model.blocks.8.attn.proj.weight', 'loss_module_train.model.blocks.8.attn.proj.bias', 'loss_module_train.model.blocks.8.ls1.gamma', 'loss_module_train.model.blocks.8.norm2.weight', 'loss_module_train.model.blocks.8.norm2.bias', 'loss_module_train.model.blocks.8.mlp.fc1.weight', 'loss_module_train.model.blocks.8.mlp.fc1.bias', 'loss_module_train.model.blocks.8.mlp.fc2.weight', 'loss_module_train.model.blocks.8.mlp.fc2.bias', 'loss_module_train.model.blocks.8.ls2.gamma', 'loss_module_train.model.blocks.9.norm1.weight', 'loss_module_train.model.blocks.9.norm1.bias', 'loss_module_train.model.blocks.9.attn.qkv.weight', 'loss_module_train.model.blocks.9.attn.qkv.bias', 'loss_module_train.model.blocks.9.attn.proj.weight', 'loss_module_train.model.blocks.9.attn.proj.bias', 'loss_module_train.model.blocks.9.ls1.gamma', 'loss_module_train.model.blocks.9.norm2.weight', 'loss_module_train.model.blocks.9.norm2.bias', 'loss_module_train.model.blocks.9.mlp.fc1.weight', 'loss_module_train.model.blocks.9.mlp.fc1.bias', 'loss_module_train.model.blocks.9.mlp.fc2.weight', 'loss_module_train.model.blocks.9.mlp.fc2.bias', 'loss_module_train.model.blocks.9.ls2.gamma', 'loss_module_train.model.blocks.10.norm1.weight', 'loss_module_train.model.blocks.10.norm1.bias', 'loss_module_train.model.blocks.10.attn.qkv.weight', 'loss_module_train.model.blocks.10.attn.qkv.bias', 'loss_module_train.model.blocks.10.attn.proj.weight', 'loss_module_train.model.blocks.10.attn.proj.bias', 'loss_module_train.model.blocks.10.ls1.gamma', 'loss_module_train.model.blocks.10.norm2.weight', 'loss_module_train.model.blocks.10.norm2.bias', 'loss_module_train.model.blocks.10.mlp.fc1.weight', 'loss_module_train.model.blocks.10.mlp.fc1.bias', 'loss_module_train.model.blocks.10.mlp.fc2.weight', 'loss_module_train.model.blocks.10.mlp.fc2.bias', 'loss_module_train.model.blocks.10.ls2.gamma', 'loss_module_train.model.blocks.11.norm1.weight', 'loss_module_train.model.blocks.11.norm1.bias', 'loss_module_train.model.blocks.11.attn.qkv.weight', 'loss_module_train.model.blocks.11.attn.qkv.bias', 'loss_module_train.model.blocks.11.attn.proj.weight', 'loss_module_train.model.blocks.11.attn.proj.bias', 'loss_module_train.model.blocks.11.ls1.gamma', 'loss_module_train.model.blocks.11.norm2.weight', 'loss_module_train.model.blocks.11.norm2.bias', 'loss_module_train.model.blocks.11.mlp.fc1.weight', 'loss_module_train.model.blocks.11.mlp.fc1.bias', 'loss_module_train.model.blocks.11.mlp.fc2.weight', 'loss_module_train.model.blocks.11.mlp.fc2.bias', 'loss_module_train.model.blocks.11.ls2.gamma', 'loss_module_train.model.blocks.12.norm1.weight', 'loss_module_train.model.blocks.12.norm1.bias', 'loss_module_train.model.blocks.12.attn.qkv.weight', 'loss_module_train.model.blocks.12.attn.qkv.bias', 'loss_module_train.model.blocks.12.attn.proj.weight', 'loss_module_train.model.blocks.12.attn.proj.bias', 'loss_module_train.model.blocks.12.ls1.gamma', 'loss_module_train.model.blocks.12.norm2.weight', 'loss_module_train.model.blocks.12.norm2.bias', 'loss_module_train.model.blocks.12.mlp.fc1.weight', 'loss_module_train.model.blocks.12.mlp.fc1.bias', 'loss_module_train.model.blocks.12.mlp.fc2.weight', 'loss_module_train.model.blocks.12.mlp.fc2.bias', 'loss_module_train.model.blocks.12.ls2.gamma', 'loss_module_train.model.blocks.13.norm1.weight', 'loss_module_train.model.blocks.13.norm1.bias', 'loss_module_train.model.blocks.13.attn.qkv.weight', 'loss_module_train.model.blocks.13.attn.qkv.bias', 'loss_module_train.model.blocks.13.attn.proj.weight', 'loss_module_train.model.blocks.13.attn.proj.bias', 'loss_module_train.model.blocks.13.ls1.gamma', 'loss_module_train.model.blocks.13.norm2.weight', 'loss_module_train.model.blocks.13.norm2.bias', 'loss_module_train.model.blocks.13.mlp.fc1.weight', 'loss_module_train.model.blocks.13.mlp.fc1.bias', 'loss_module_train.model.blocks.13.mlp.fc2.weight', 'loss_module_train.model.blocks.13.mlp.fc2.bias', 'loss_module_train.model.blocks.13.ls2.gamma', 'loss_module_train.model.blocks.14.norm1.weight', 'loss_module_train.model.blocks.14.norm1.bias', 'loss_module_train.model.blocks.14.attn.qkv.weight', 'loss_module_train.model.blocks.14.attn.qkv.bias', 'loss_module_train.model.blocks.14.attn.proj.weight', 'loss_module_train.model.blocks.14.attn.proj.bias', 'loss_module_train.model.blocks.14.ls1.gamma', 'loss_module_train.model.blocks.14.norm2.weight', 'loss_module_train.model.blocks.14.norm2.bias', 'loss_module_train.model.blocks.14.mlp.fc1.weight', 'loss_module_train.model.blocks.14.mlp.fc1.bias', 'loss_module_train.model.blocks.14.mlp.fc2.weight', 'loss_module_train.model.blocks.14.mlp.fc2.bias', 'loss_module_train.model.blocks.14.ls2.gamma', 'loss_module_train.model.blocks.15.norm1.weight', 'loss_module_train.model.blocks.15.norm1.bias', 'loss_module_train.model.blocks.15.attn.qkv.weight', 'loss_module_train.model.blocks.15.attn.qkv.bias', 'loss_module_train.model.blocks.15.attn.proj.weight', 'loss_module_train.model.blocks.15.attn.proj.bias', 'loss_module_train.model.blocks.15.ls1.gamma', 'loss_module_train.model.blocks.15.norm2.weight', 'loss_module_train.model.blocks.15.norm2.bias', 'loss_module_train.model.blocks.15.mlp.fc1.weight', 'loss_module_train.model.blocks.15.mlp.fc1.bias', 'loss_module_train.model.blocks.15.mlp.fc2.weight', 'loss_module_train.model.blocks.15.mlp.fc2.bias', 'loss_module_train.model.blocks.15.ls2.gamma', 'loss_module_train.model.blocks.16.norm1.weight', 'loss_module_train.model.blocks.16.norm1.bias', 'loss_module_train.model.blocks.16.attn.qkv.weight', 'loss_module_train.model.blocks.16.attn.qkv.bias', 'loss_module_train.model.blocks.16.attn.proj.weight', 'loss_module_train.model.blocks.16.attn.proj.bias', 'loss_module_train.model.blocks.16.ls1.gamma', 'loss_module_train.model.blocks.16.norm2.weight', 'loss_module_train.model.blocks.16.norm2.bias', 'loss_module_train.model.blocks.16.mlp.fc1.weight', 'loss_module_train.model.blocks.16.mlp.fc1.bias', 'loss_module_train.model.blocks.16.mlp.fc2.weight', 'loss_module_train.model.blocks.16.mlp.fc2.bias', 'loss_module_train.model.blocks.16.ls2.gamma', 'loss_module_train.model.blocks.17.norm1.weight', 'loss_module_train.model.blocks.17.norm1.bias', 'loss_module_train.model.blocks.17.attn.qkv.weight', 'loss_module_train.model.blocks.17.attn.qkv.bias', 'loss_module_train.model.blocks.17.attn.proj.weight', 'loss_module_train.model.blocks.17.attn.proj.bias', 'loss_module_train.model.blocks.17.ls1.gamma', 'loss_module_train.model.blocks.17.norm2.weight', 'loss_module_train.model.blocks.17.norm2.bias', 'loss_module_train.model.blocks.17.mlp.fc1.weight', 'loss_module_train.model.blocks.17.mlp.fc1.bias', 'loss_module_train.model.blocks.17.mlp.fc2.weight', 'loss_module_train.model.blocks.17.mlp.fc2.bias', 'loss_module_train.model.blocks.17.ls2.gamma', 'loss_module_train.model.blocks.18.norm1.weight', 'loss_module_train.model.blocks.18.norm1.bias', 'loss_module_train.model.blocks.18.attn.qkv.weight', 'loss_module_train.model.blocks.18.attn.qkv.bias', 'loss_module_train.model.blocks.18.attn.proj.weight', 'loss_module_train.model.blocks.18.attn.proj.bias', 'loss_module_train.model.blocks.18.ls1.gamma', 'loss_module_train.model.blocks.18.norm2.weight', 'loss_module_train.model.blocks.18.norm2.bias', 'loss_module_train.model.blocks.18.mlp.fc1.weight', 'loss_module_train.model.blocks.18.mlp.fc1.bias', 'loss_module_train.model.blocks.18.mlp.fc2.weight', 'loss_module_train.model.blocks.18.mlp.fc2.bias', 'loss_module_train.model.blocks.18.ls2.gamma', 'loss_module_train.model.blocks.19.norm1.weight', 'loss_module_train.model.blocks.19.norm1.bias', 'loss_module_train.model.blocks.19.attn.qkv.weight', 'loss_module_train.model.blocks.19.attn.qkv.bias', 'loss_module_train.model.blocks.19.attn.proj.weight', 'loss_module_train.model.blocks.19.attn.proj.bias', 'loss_module_train.model.blocks.19.ls1.gamma', 'loss_module_train.model.blocks.19.norm2.weight', 'loss_module_train.model.blocks.19.norm2.bias', 'loss_module_train.model.blocks.19.mlp.fc1.weight', 'loss_module_train.model.blocks.19.mlp.fc1.bias', 'loss_module_train.model.blocks.19.mlp.fc2.weight', 'loss_module_train.model.blocks.19.mlp.fc2.bias', 'loss_module_train.model.blocks.19.ls2.gamma', 'loss_module_train.model.blocks.20.norm1.weight', 'loss_module_train.model.blocks.20.norm1.bias', 'loss_module_train.model.blocks.20.attn.qkv.weight', 'loss_module_train.model.blocks.20.attn.qkv.bias', 'loss_module_train.model.blocks.20.attn.proj.weight', 'loss_module_train.model.blocks.20.attn.proj.bias', 'loss_module_train.model.blocks.20.ls1.gamma', 'loss_module_train.model.blocks.20.norm2.weight', 'loss_module_train.model.blocks.20.norm2.bias', 'loss_module_train.model.blocks.20.mlp.fc1.weight', 'loss_module_train.model.blocks.20.mlp.fc1.bias', 'loss_module_train.model.blocks.20.mlp.fc2.weight', 'loss_module_train.model.blocks.20.mlp.fc2.bias', 'loss_module_train.model.blocks.20.ls2.gamma', 'loss_module_train.model.blocks.21.norm1.weight', 'loss_module_train.model.blocks.21.norm1.bias', 'loss_module_train.model.blocks.21.attn.qkv.weight', 'loss_module_train.model.blocks.21.attn.qkv.bias', 'loss_module_train.model.blocks.21.attn.proj.weight', 'loss_module_train.model.blocks.21.attn.proj.bias', 'loss_module_train.model.blocks.21.ls1.gamma', 'loss_module_train.model.blocks.21.norm2.weight', 'loss_module_train.model.blocks.21.norm2.bias', 'loss_module_train.model.blocks.21.mlp.fc1.weight', 'loss_module_train.model.blocks.21.mlp.fc1.bias', 'loss_module_train.model.blocks.21.mlp.fc2.weight', 'loss_module_train.model.blocks.21.mlp.fc2.bias', 'loss_module_train.model.blocks.21.ls2.gamma', 'loss_module_train.model.blocks.22.norm1.weight', 'loss_module_train.model.blocks.22.norm1.bias', 'loss_module_train.model.blocks.22.attn.qkv.weight', 'loss_module_train.model.blocks.22.attn.qkv.bias', 'loss_module_train.model.blocks.22.attn.proj.weight', 'loss_module_train.model.blocks.22.attn.proj.bias', 'loss_module_train.model.blocks.22.ls1.gamma', 'loss_module_train.model.blocks.22.norm2.weight', 'loss_module_train.model.blocks.22.norm2.bias', 'loss_module_train.model.blocks.22.mlp.fc1.weight', 'loss_module_train.model.blocks.22.mlp.fc1.bias', 'loss_module_train.model.blocks.22.mlp.fc2.weight', 'loss_module_train.model.blocks.22.mlp.fc2.bias', 'loss_module_train.model.blocks.22.ls2.gamma', 'loss_module_train.model.blocks.23.norm1.weight', 'loss_module_train.model.blocks.23.norm1.bias', 'loss_module_train.model.blocks.23.attn.qkv.weight', 'loss_module_train.model.blocks.23.attn.qkv.bias', 'loss_module_train.model.blocks.23.attn.proj.weight', 'loss_module_train.model.blocks.23.attn.proj.bias', 'loss_module_train.model.blocks.23.ls1.gamma', 'loss_module_train.model.blocks.23.norm2.weight', 'loss_module_train.model.blocks.23.norm2.bias', 'loss_module_train.model.blocks.23.mlp.fc1.weight', 'loss_module_train.model.blocks.23.mlp.fc1.bias', 'loss_module_train.model.blocks.23.mlp.fc2.weight', 'loss_module_train.model.blocks.23.mlp.fc2.bias', 'loss_module_train.model.blocks.23.ls2.gamma', 'loss_module_train.model.norm.weight', 'loss_module_train.model.norm.bias', 'loss_module_train.l2sp_loss.cls_token', 'loss_module_train.l2sp_loss.patch_embed_proj_weight', 'loss_module_train.l2sp_loss.blocks_0_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_0_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_0_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_0_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_1_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_1_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_1_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_1_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_2_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_2_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_2_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_2_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_3_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_3_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_3_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_3_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_4_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_4_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_4_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_4_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_5_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_5_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_5_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_5_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_6_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_6_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_6_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_6_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_7_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_7_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_7_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_7_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_8_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_8_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_8_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_8_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_9_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_9_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_9_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_9_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_10_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_10_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_10_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_10_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_11_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_11_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_11_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_11_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_12_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_12_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_12_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_12_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_13_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_13_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_13_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_13_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_14_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_14_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_14_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_14_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_15_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_15_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_15_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_15_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_16_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_16_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_16_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_16_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_17_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_17_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_17_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_17_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_18_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_18_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_18_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_18_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_19_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_19_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_19_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_19_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_20_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_20_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_20_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_20_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_21_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_21_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_21_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_21_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_22_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_22_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_22_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_22_mlp_fc2_weight', 'loss_module_train.l2sp_loss.blocks_23_attn_qkv_weight', 'loss_module_train.l2sp_loss.blocks_23_attn_proj_weight', 'loss_module_train.l2sp_loss.blocks_23_mlp_fc1_weight', 'loss_module_train.l2sp_loss.blocks_23_mlp_fc2_weight', 'loss_module_val.model.cls_token', 'loss_module_val.model.pos_embed', 'loss_module_val.model.patch_embed.proj.weight', 'loss_module_val.model.patch_embed.proj.bias', 'loss_module_val.model.blocks.0.norm1.weight', 'loss_module_val.model.blocks.0.norm1.bias', 'loss_module_val.model.blocks.0.attn.qkv.weight', 'loss_module_val.model.blocks.0.attn.qkv.bias', 'loss_module_val.model.blocks.0.attn.proj.weight', 'loss_module_val.model.blocks.0.attn.proj.bias', 'loss_module_val.model.blocks.0.ls1.gamma', 'loss_module_val.model.blocks.0.norm2.weight', 'loss_module_val.model.blocks.0.norm2.bias', 'loss_module_val.model.blocks.0.mlp.fc1.weight', 'loss_module_val.model.blocks.0.mlp.fc1.bias', 'loss_module_val.model.blocks.0.mlp.fc2.weight', 'loss_module_val.model.blocks.0.mlp.fc2.bias', 'loss_module_val.model.blocks.0.ls2.gamma', 'loss_module_val.model.blocks.1.norm1.weight', 'loss_module_val.model.blocks.1.norm1.bias', 'loss_module_val.model.blocks.1.attn.qkv.weight', 'loss_module_val.model.blocks.1.attn.qkv.bias', 'loss_module_val.model.blocks.1.attn.proj.weight', 'loss_module_val.model.blocks.1.attn.proj.bias', 'loss_module_val.model.blocks.1.ls1.gamma', 'loss_module_val.model.blocks.1.norm2.weight', 'loss_module_val.model.blocks.1.norm2.bias', 'loss_module_val.model.blocks.1.mlp.fc1.weight', 'loss_module_val.model.blocks.1.mlp.fc1.bias', 'loss_module_val.model.blocks.1.mlp.fc2.weight', 'loss_module_val.model.blocks.1.mlp.fc2.bias', 'loss_module_val.model.blocks.1.ls2.gamma', 'loss_module_val.model.blocks.2.norm1.weight', 'loss_module_val.model.blocks.2.norm1.bias', 'loss_module_val.model.blocks.2.attn.qkv.weight', 'loss_module_val.model.blocks.2.attn.qkv.bias', 'loss_module_val.model.blocks.2.attn.proj.weight', 'loss_module_val.model.blocks.2.attn.proj.bias', 'loss_module_val.model.blocks.2.ls1.gamma', 'loss_module_val.model.blocks.2.norm2.weight', 'loss_module_val.model.blocks.2.norm2.bias', 'loss_module_val.model.blocks.2.mlp.fc1.weight', 'loss_module_val.model.blocks.2.mlp.fc1.bias', 'loss_module_val.model.blocks.2.mlp.fc2.weight', 'loss_module_val.model.blocks.2.mlp.fc2.bias', 'loss_module_val.model.blocks.2.ls2.gamma', 'loss_module_val.model.blocks.3.norm1.weight', 'loss_module_val.model.blocks.3.norm1.bias', 'loss_module_val.model.blocks.3.attn.qkv.weight', 'loss_module_val.model.blocks.3.attn.qkv.bias', 'loss_module_val.model.blocks.3.attn.proj.weight', 'loss_module_val.model.blocks.3.attn.proj.bias', 'loss_module_val.model.blocks.3.ls1.gamma', 'loss_module_val.model.blocks.3.norm2.weight', 'loss_module_val.model.blocks.3.norm2.bias', 'loss_module_val.model.blocks.3.mlp.fc1.weight', 'loss_module_val.model.blocks.3.mlp.fc1.bias', 'loss_module_val.model.blocks.3.mlp.fc2.weight', 'loss_module_val.model.blocks.3.mlp.fc2.bias', 'loss_module_val.model.blocks.3.ls2.gamma', 'loss_module_val.model.blocks.4.norm1.weight', 'loss_module_val.model.blocks.4.norm1.bias', 'loss_module_val.model.blocks.4.attn.qkv.weight', 'loss_module_val.model.blocks.4.attn.qkv.bias', 'loss_module_val.model.blocks.4.attn.proj.weight', 'loss_module_val.model.blocks.4.attn.proj.bias', 'loss_module_val.model.blocks.4.ls1.gamma', 'loss_module_val.model.blocks.4.norm2.weight', 'loss_module_val.model.blocks.4.norm2.bias', 'loss_module_val.model.blocks.4.mlp.fc1.weight', 'loss_module_val.model.blocks.4.mlp.fc1.bias', 'loss_module_val.model.blocks.4.mlp.fc2.weight', 'loss_module_val.model.blocks.4.mlp.fc2.bias', 'loss_module_val.model.blocks.4.ls2.gamma', 'loss_module_val.model.blocks.5.norm1.weight', 'loss_module_val.model.blocks.5.norm1.bias', 'loss_module_val.model.blocks.5.attn.qkv.weight', 'loss_module_val.model.blocks.5.attn.qkv.bias', 'loss_module_val.model.blocks.5.attn.proj.weight', 'loss_module_val.model.blocks.5.attn.proj.bias', 'loss_module_val.model.blocks.5.ls1.gamma', 'loss_module_val.model.blocks.5.norm2.weight', 'loss_module_val.model.blocks.5.norm2.bias', 'loss_module_val.model.blocks.5.mlp.fc1.weight', 'loss_module_val.model.blocks.5.mlp.fc1.bias', 'loss_module_val.model.blocks.5.mlp.fc2.weight', 'loss_module_val.model.blocks.5.mlp.fc2.bias', 'loss_module_val.model.blocks.5.ls2.gamma', 'loss_module_val.model.blocks.6.norm1.weight', 'loss_module_val.model.blocks.6.norm1.bias', 'loss_module_val.model.blocks.6.attn.qkv.weight', 'loss_module_val.model.blocks.6.attn.qkv.bias', 'loss_module_val.model.blocks.6.attn.proj.weight', 'loss_module_val.model.blocks.6.attn.proj.bias', 'loss_module_val.model.blocks.6.ls1.gamma', 'loss_module_val.model.blocks.6.norm2.weight', 'loss_module_val.model.blocks.6.norm2.bias', 'loss_module_val.model.blocks.6.mlp.fc1.weight', 'loss_module_val.model.blocks.6.mlp.fc1.bias', 'loss_module_val.model.blocks.6.mlp.fc2.weight', 'loss_module_val.model.blocks.6.mlp.fc2.bias', 'loss_module_val.model.blocks.6.ls2.gamma', 'loss_module_val.model.blocks.7.norm1.weight', 'loss_module_val.model.blocks.7.norm1.bias', 'loss_module_val.model.blocks.7.attn.qkv.weight', 'loss_module_val.model.blocks.7.attn.qkv.bias', 'loss_module_val.model.blocks.7.attn.proj.weight', 'loss_module_val.model.blocks.7.attn.proj.bias', 'loss_module_val.model.blocks.7.ls1.gamma', 'loss_module_val.model.blocks.7.norm2.weight', 'loss_module_val.model.blocks.7.norm2.bias', 'loss_module_val.model.blocks.7.mlp.fc1.weight', 'loss_module_val.model.blocks.7.mlp.fc1.bias', 'loss_module_val.model.blocks.7.mlp.fc2.weight', 'loss_module_val.model.blocks.7.mlp.fc2.bias', 'loss_module_val.model.blocks.7.ls2.gamma', 'loss_module_val.model.blocks.8.norm1.weight', 'loss_module_val.model.blocks.8.norm1.bias', 'loss_module_val.model.blocks.8.attn.qkv.weight', 'loss_module_val.model.blocks.8.attn.qkv.bias', 'loss_module_val.model.blocks.8.attn.proj.weight', 'loss_module_val.model.blocks.8.attn.proj.bias', 'loss_module_val.model.blocks.8.ls1.gamma', 'loss_module_val.model.blocks.8.norm2.weight', 'loss_module_val.model.blocks.8.norm2.bias', 'loss_module_val.model.blocks.8.mlp.fc1.weight', 'loss_module_val.model.blocks.8.mlp.fc1.bias', 'loss_module_val.model.blocks.8.mlp.fc2.weight', 'loss_module_val.model.blocks.8.mlp.fc2.bias', 'loss_module_val.model.blocks.8.ls2.gamma', 'loss_module_val.model.blocks.9.norm1.weight', 'loss_module_val.model.blocks.9.norm1.bias', 'loss_module_val.model.blocks.9.attn.qkv.weight', 'loss_module_val.model.blocks.9.attn.qkv.bias', 'loss_module_val.model.blocks.9.attn.proj.weight', 'loss_module_val.model.blocks.9.attn.proj.bias', 'loss_module_val.model.blocks.9.ls1.gamma', 'loss_module_val.model.blocks.9.norm2.weight', 'loss_module_val.model.blocks.9.norm2.bias', 'loss_module_val.model.blocks.9.mlp.fc1.weight', 'loss_module_val.model.blocks.9.mlp.fc1.bias', 'loss_module_val.model.blocks.9.mlp.fc2.weight', 'loss_module_val.model.blocks.9.mlp.fc2.bias', 'loss_module_val.model.blocks.9.ls2.gamma', 'loss_module_val.model.blocks.10.norm1.weight', 'loss_module_val.model.blocks.10.norm1.bias', 'loss_module_val.model.blocks.10.attn.qkv.weight', 'loss_module_val.model.blocks.10.attn.qkv.bias', 'loss_module_val.model.blocks.10.attn.proj.weight', 'loss_module_val.model.blocks.10.attn.proj.bias', 'loss_module_val.model.blocks.10.ls1.gamma', 'loss_module_val.model.blocks.10.norm2.weight', 'loss_module_val.model.blocks.10.norm2.bias', 'loss_module_val.model.blocks.10.mlp.fc1.weight', 'loss_module_val.model.blocks.10.mlp.fc1.bias', 'loss_module_val.model.blocks.10.mlp.fc2.weight', 'loss_module_val.model.blocks.10.mlp.fc2.bias', 'loss_module_val.model.blocks.10.ls2.gamma', 'loss_module_val.model.blocks.11.norm1.weight', 'loss_module_val.model.blocks.11.norm1.bias', 'loss_module_val.model.blocks.11.attn.qkv.weight', 'loss_module_val.model.blocks.11.attn.qkv.bias', 'loss_module_val.model.blocks.11.attn.proj.weight', 'loss_module_val.model.blocks.11.attn.proj.bias', 'loss_module_val.model.blocks.11.ls1.gamma', 'loss_module_val.model.blocks.11.norm2.weight', 'loss_module_val.model.blocks.11.norm2.bias', 'loss_module_val.model.blocks.11.mlp.fc1.weight', 'loss_module_val.model.blocks.11.mlp.fc1.bias', 'loss_module_val.model.blocks.11.mlp.fc2.weight', 'loss_module_val.model.blocks.11.mlp.fc2.bias', 'loss_module_val.model.blocks.11.ls2.gamma', 'loss_module_val.model.blocks.12.norm1.weight', 'loss_module_val.model.blocks.12.norm1.bias', 'loss_module_val.model.blocks.12.attn.qkv.weight', 'loss_module_val.model.blocks.12.attn.qkv.bias', 'loss_module_val.model.blocks.12.attn.proj.weight', 'loss_module_val.model.blocks.12.attn.proj.bias', 'loss_module_val.model.blocks.12.ls1.gamma', 'loss_module_val.model.blocks.12.norm2.weight', 'loss_module_val.model.blocks.12.norm2.bias', 'loss_module_val.model.blocks.12.mlp.fc1.weight', 'loss_module_val.model.blocks.12.mlp.fc1.bias', 'loss_module_val.model.blocks.12.mlp.fc2.weight', 'loss_module_val.model.blocks.12.mlp.fc2.bias', 'loss_module_val.model.blocks.12.ls2.gamma', 'loss_module_val.model.blocks.13.norm1.weight', 'loss_module_val.model.blocks.13.norm1.bias', 'loss_module_val.model.blocks.13.attn.qkv.weight', 'loss_module_val.model.blocks.13.attn.qkv.bias', 'loss_module_val.model.blocks.13.attn.proj.weight', 'loss_module_val.model.blocks.13.attn.proj.bias', 'loss_module_val.model.blocks.13.ls1.gamma', 'loss_module_val.model.blocks.13.norm2.weight', 'loss_module_val.model.blocks.13.norm2.bias', 'loss_module_val.model.blocks.13.mlp.fc1.weight', 'loss_module_val.model.blocks.13.mlp.fc1.bias', 'loss_module_val.model.blocks.13.mlp.fc2.weight', 'loss_module_val.model.blocks.13.mlp.fc2.bias', 'loss_module_val.model.blocks.13.ls2.gamma', 'loss_module_val.model.blocks.14.norm1.weight', 'loss_module_val.model.blocks.14.norm1.bias', 'loss_module_val.model.blocks.14.attn.qkv.weight', 'loss_module_val.model.blocks.14.attn.qkv.bias', 'loss_module_val.model.blocks.14.attn.proj.weight', 'loss_module_val.model.blocks.14.attn.proj.bias', 'loss_module_val.model.blocks.14.ls1.gamma', 'loss_module_val.model.blocks.14.norm2.weight', 'loss_module_val.model.blocks.14.norm2.bias', 'loss_module_val.model.blocks.14.mlp.fc1.weight', 'loss_module_val.model.blocks.14.mlp.fc1.bias', 'loss_module_val.model.blocks.14.mlp.fc2.weight', 'loss_module_val.model.blocks.14.mlp.fc2.bias', 'loss_module_val.model.blocks.14.ls2.gamma', 'loss_module_val.model.blocks.15.norm1.weight', 'loss_module_val.model.blocks.15.norm1.bias', 'loss_module_val.model.blocks.15.attn.qkv.weight', 'loss_module_val.model.blocks.15.attn.qkv.bias', 'loss_module_val.model.blocks.15.attn.proj.weight', 'loss_module_val.model.blocks.15.attn.proj.bias', 'loss_module_val.model.blocks.15.ls1.gamma', 'loss_module_val.model.blocks.15.norm2.weight', 'loss_module_val.model.blocks.15.norm2.bias', 'loss_module_val.model.blocks.15.mlp.fc1.weight', 'loss_module_val.model.blocks.15.mlp.fc1.bias', 'loss_module_val.model.blocks.15.mlp.fc2.weight', 'loss_module_val.model.blocks.15.mlp.fc2.bias', 'loss_module_val.model.blocks.15.ls2.gamma', 'loss_module_val.model.blocks.16.norm1.weight', 'loss_module_val.model.blocks.16.norm1.bias', 'loss_module_val.model.blocks.16.attn.qkv.weight', 'loss_module_val.model.blocks.16.attn.qkv.bias', 'loss_module_val.model.blocks.16.attn.proj.weight', 'loss_module_val.model.blocks.16.attn.proj.bias', 'loss_module_val.model.blocks.16.ls1.gamma', 'loss_module_val.model.blocks.16.norm2.weight', 'loss_module_val.model.blocks.16.norm2.bias', 'loss_module_val.model.blocks.16.mlp.fc1.weight', 'loss_module_val.model.blocks.16.mlp.fc1.bias', 'loss_module_val.model.blocks.16.mlp.fc2.weight', 'loss_module_val.model.blocks.16.mlp.fc2.bias', 'loss_module_val.model.blocks.16.ls2.gamma', 'loss_module_val.model.blocks.17.norm1.weight', 'loss_module_val.model.blocks.17.norm1.bias', 'loss_module_val.model.blocks.17.attn.qkv.weight', 'loss_module_val.model.blocks.17.attn.qkv.bias', 'loss_module_val.model.blocks.17.attn.proj.weight', 'loss_module_val.model.blocks.17.attn.proj.bias', 'loss_module_val.model.blocks.17.ls1.gamma', 'loss_module_val.model.blocks.17.norm2.weight', 'loss_module_val.model.blocks.17.norm2.bias', 'loss_module_val.model.blocks.17.mlp.fc1.weight', 'loss_module_val.model.blocks.17.mlp.fc1.bias', 'loss_module_val.model.blocks.17.mlp.fc2.weight', 'loss_module_val.model.blocks.17.mlp.fc2.bias', 'loss_module_val.model.blocks.17.ls2.gamma', 'loss_module_val.model.blocks.18.norm1.weight', 'loss_module_val.model.blocks.18.norm1.bias', 'loss_module_val.model.blocks.18.attn.qkv.weight', 'loss_module_val.model.blocks.18.attn.qkv.bias', 'loss_module_val.model.blocks.18.attn.proj.weight', 'loss_module_val.model.blocks.18.attn.proj.bias', 'loss_module_val.model.blocks.18.ls1.gamma', 'loss_module_val.model.blocks.18.norm2.weight', 'loss_module_val.model.blocks.18.norm2.bias', 'loss_module_val.model.blocks.18.mlp.fc1.weight', 'loss_module_val.model.blocks.18.mlp.fc1.bias', 'loss_module_val.model.blocks.18.mlp.fc2.weight', 'loss_module_val.model.blocks.18.mlp.fc2.bias', 'loss_module_val.model.blocks.18.ls2.gamma', 'loss_module_val.model.blocks.19.norm1.weight', 'loss_module_val.model.blocks.19.norm1.bias', 'loss_module_val.model.blocks.19.attn.qkv.weight', 'loss_module_val.model.blocks.19.attn.qkv.bias', 'loss_module_val.model.blocks.19.attn.proj.weight', 'loss_module_val.model.blocks.19.attn.proj.bias', 'loss_module_val.model.blocks.19.ls1.gamma', 'loss_module_val.model.blocks.19.norm2.weight', 'loss_module_val.model.blocks.19.norm2.bias', 'loss_module_val.model.blocks.19.mlp.fc1.weight', 'loss_module_val.model.blocks.19.mlp.fc1.bias', 'loss_module_val.model.blocks.19.mlp.fc2.weight', 'loss_module_val.model.blocks.19.mlp.fc2.bias', 'loss_module_val.model.blocks.19.ls2.gamma', 'loss_module_val.model.blocks.20.norm1.weight', 'loss_module_val.model.blocks.20.norm1.bias', 'loss_module_val.model.blocks.20.attn.qkv.weight', 'loss_module_val.model.blocks.20.attn.qkv.bias', 'loss_module_val.model.blocks.20.attn.proj.weight', 'loss_module_val.model.blocks.20.attn.proj.bias', 'loss_module_val.model.blocks.20.ls1.gamma', 'loss_module_val.model.blocks.20.norm2.weight', 'loss_module_val.model.blocks.20.norm2.bias', 'loss_module_val.model.blocks.20.mlp.fc1.weight', 'loss_module_val.model.blocks.20.mlp.fc1.bias', 'loss_module_val.model.blocks.20.mlp.fc2.weight', 'loss_module_val.model.blocks.20.mlp.fc2.bias', 'loss_module_val.model.blocks.20.ls2.gamma', 'loss_module_val.model.blocks.21.norm1.weight', 'loss_module_val.model.blocks.21.norm1.bias', 'loss_module_val.model.blocks.21.attn.qkv.weight', 'loss_module_val.model.blocks.21.attn.qkv.bias', 'loss_module_val.model.blocks.21.attn.proj.weight', 'loss_module_val.model.blocks.21.attn.proj.bias', 'loss_module_val.model.blocks.21.ls1.gamma', 'loss_module_val.model.blocks.21.norm2.weight', 'loss_module_val.model.blocks.21.norm2.bias', 'loss_module_val.model.blocks.21.mlp.fc1.weight', 'loss_module_val.model.blocks.21.mlp.fc1.bias', 'loss_module_val.model.blocks.21.mlp.fc2.weight', 'loss_module_val.model.blocks.21.mlp.fc2.bias', 'loss_module_val.model.blocks.21.ls2.gamma', 'loss_module_val.model.blocks.22.norm1.weight', 'loss_module_val.model.blocks.22.norm1.bias', 'loss_module_val.model.blocks.22.attn.qkv.weight', 'loss_module_val.model.blocks.22.attn.qkv.bias', 'loss_module_val.model.blocks.22.attn.proj.weight', 'loss_module_val.model.blocks.22.attn.proj.bias', 'loss_module_val.model.blocks.22.ls1.gamma', 'loss_module_val.model.blocks.22.norm2.weight', 'loss_module_val.model.blocks.22.norm2.bias', 'loss_module_val.model.blocks.22.mlp.fc1.weight', 'loss_module_val.model.blocks.22.mlp.fc1.bias', 'loss_module_val.model.blocks.22.mlp.fc2.weight', 'loss_module_val.model.blocks.22.mlp.fc2.bias', 'loss_module_val.model.blocks.22.ls2.gamma', 'loss_module_val.model.blocks.23.norm1.weight', 'loss_module_val.model.blocks.23.norm1.bias', 'loss_module_val.model.blocks.23.attn.qkv.weight', 'loss_module_val.model.blocks.23.attn.qkv.bias', 'loss_module_val.model.blocks.23.attn.proj.weight', 'loss_module_val.model.blocks.23.attn.proj.bias', 'loss_module_val.model.blocks.23.ls1.gamma', 'loss_module_val.model.blocks.23.norm2.weight', 'loss_module_val.model.blocks.23.norm2.bias', 'loss_module_val.model.blocks.23.mlp.fc1.weight', 'loss_module_val.model.blocks.23.mlp.fc1.bias', 'loss_module_val.model.blocks.23.mlp.fc2.weight', 'loss_module_val.model.blocks.23.mlp.fc2.bias', 'loss_module_val.model.blocks.23.ls2.gamma', 'loss_module_val.model.norm.weight', 'loss_module_val.model.norm.bias', 'loss_module_val.l2sp_loss.cls_token', 'loss_module_val.l2sp_loss.patch_embed_proj_weight', 'loss_module_val.l2sp_loss.blocks_0_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_0_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_0_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_0_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_1_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_1_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_1_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_1_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_2_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_2_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_2_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_2_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_3_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_3_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_3_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_3_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_4_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_4_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_4_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_4_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_5_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_5_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_5_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_5_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_6_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_6_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_6_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_6_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_7_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_7_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_7_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_7_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_8_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_8_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_8_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_8_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_9_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_9_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_9_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_9_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_10_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_10_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_10_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_10_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_11_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_11_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_11_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_11_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_12_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_12_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_12_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_12_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_13_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_13_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_13_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_13_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_14_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_14_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_14_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_14_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_15_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_15_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_15_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_15_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_16_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_16_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_16_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_16_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_17_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_17_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_17_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_17_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_18_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_18_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_18_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_18_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_19_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_19_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_19_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_19_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_20_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_20_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_20_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_20_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_21_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_21_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_21_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_21_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_22_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_22_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_22_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_22_mlp_fc2_weight', 'loss_module_val.l2sp_loss.blocks_23_attn_qkv_weight', 'loss_module_val.l2sp_loss.blocks_23_attn_proj_weight', 'loss_module_val.l2sp_loss.blocks_23_mlp_fc1_weight', 'loss_module_val.l2sp_loss.blocks_23_mlp_fc2_weight'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weights into the model\n",
    "\n",
    "def extract_clean_state_dict_for_wrapper(checkpoint, wrapper_key=\"model_wrapper.\", model_key=\"model.\"):\n",
    "    # Extract the state_dict from the checkpoint\n",
    "    state_dict = checkpoint.get('state_dict', checkpoint)  # Use 'state_dict' or checkpoint directly\n",
    "    # Remove wrapper key prefix\n",
    "    cleaned_state_dict = {k.replace(wrapper_key, ''): v for k, v in state_dict.items()}\n",
    "    \n",
    "    return cleaned_state_dict\n",
    "\n",
    "# only wrapper. prefix are removed\n",
    "# only loss_module. keys are ignored\n",
    "cleaned_state_dict_wrapper = extract_clean_state_dict_for_wrapper(checkpoint_best)\n",
    "model_wrapper.load_state_dict(cleaned_state_dict_wrapper, strict=False)\n",
    "# print(\"Checkpoint loaded successfully!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best-model_vit_large_dinoV2 in timmWrapper with custom model:\n",
      "ViT Accuracy with 5-nearest neighbors: 0.8441, F1_score: 0.825991\n",
      "Number of images before filtering: 372\n",
      "Number of embeddings after filtering: 372\n",
      "Evaluate with filtered embeddings: linear\n",
      "best-model_vit_large_dinoV2 in timmWrapper with custom model:\n",
      "ViT Accuracy with 5-nearest neighbors: 0.8441, F1_score: 0.825991\n"
     ]
    }
   ],
   "source": [
    "### test only the model wrapper\n",
    "### generate embeddings from the custom_vit & KNN classification\n",
    "model_wrapper.eval()\n",
    "\n",
    "# create corresponding image tensors (224 instead of 192)\n",
    "# test_images, test_labels = load_images_and_labels(test_folder, transform_standard)\n",
    "# test_images_tensor = torch.stack(test_images)\n",
    "\n",
    "custom_embeddings_wrapper = generate_embeddings(model_wrapper, test_images_tensor)\n",
    "\n",
    "def classify_and_print_custom(model_name, model, embeddings, labels):\n",
    "    print(f\"{model_name} with custom model:\")\n",
    "    knn_classifier(embeddings, labels)\n",
    "    \n",
    "\n",
    "    \n",
    "classify_and_print_custom(model_name_wrapper, model_wrapper, custom_embeddings_wrapper, test_labels)\n",
    "# knn_classifier(custom_embeddings_joscha, test_labels)\n",
    "\n",
    "# evaluate with filtered embeddings\n",
    "\n",
    "filtered_embeddings_wrapper, filtered_labels_wrapper = filter_samples_by_threshold(custom_embeddings_wrapper, test_labels, threshold=3)\n",
    "\n",
    "print(f\"Evaluate with filtered embeddings: {embedding_id}\")\n",
    "classify_and_print_custom(model_name_wrapper, model_wrapper, filtered_embeddings_wrapper, filtered_labels_wrapper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_into_custom_vit_supervised(checkpoint_state_dict, custom_vit, patch_size=16):\n",
    "    \"\"\"\n",
    "    Loads a checkpoint's state_dict into a custom Vision Transformer (ViT) model.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_state_dict (dict): The state_dict from the checkpoint.\n",
    "        custom_vit (torch.nn.Module): The custom ViT model with backbone and embedding layers.\n",
    "        patch_size (int): Patch size for the ViT model, used for positional embedding interpolation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Separate backbone (base_vit) and embedding layer weights\n",
    "    backbone_state_dict = {k: v for k, v in checkpoint_state_dict.items() if \"embedding_layer\" not in k}\n",
    "    embedding_layer_state_dict = {k: v for k, v in checkpoint_state_dict.items() if \"embedding_layer\" in k}\n",
    "\n",
    "    # Interpolate positional embeddings if necessary\n",
    "    interpolated_state_dict = interpolate_positional_embedding(backbone_state_dict, custom_vit.base_vit, patch_size)\n",
    "    \n",
    "    backbone_state_dict[\"pos_embed\"] = interpolated_state_dict[\"pos_embed\"]\n",
    "    \n",
    "    # Load the backbone weights into base_vit\n",
    "    custom_vit.base_vit.load_state_dict(backbone_state_dict, strict=False)\n",
    "\n",
    "    # Load weights into the custom embedding layer\n",
    "    custom_vit.embedding_layer.load_state_dict({\n",
    "        \"weight\": embedding_layer_state_dict[\"embedding_layer.weight\"],\n",
    "        \"bias\": embedding_layer_state_dict[\"embedding_layer.bias\"],\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom VIT for vit_large_dinov2_ssl_joscha & ssl_vincent_vit_large\n",
    "# check thesis for best embedding layer\n",
    "\n",
    "class CustomVisionTransformer_ssl(nn.Module):\n",
    "    def __init__(self, base_vit):\n",
    "        super(CustomVisionTransformer_ssl, self).__init__()\n",
    "        self.base_vit = base_vit\n",
    "        \n",
    "        # Define embedding layers based on checkpoint dimensions\n",
    "        self.embedding_layer_0 = nn.BatchNorm1d(1024)  # Normalize input size 1024\n",
    "        self.embedding_layer_2 = nn.Linear(1024, 256)  # Linear layer: 1024 -> 256\n",
    "        self.embedding_layer_3 = nn.BatchNorm1d(256)  # Normalize input size 256\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the Vision Transformer backbone\n",
    "        x = self.base_vit.forward_features(x)  # Output size is [batch_size, 1024]\n",
    "        print(f\"output shape of base_vit: {x.shape}\")\n",
    "        \n",
    "        # Flatten the patch and token dimensions into batch\n",
    "        # x = x.view(-1, x.size(-1))  # Shape: [batch_size * 257, 1024]\n",
    "        x = x[:, 0, :]  # Shape: [batch_size, 1024]\n",
    "\n",
    "        \n",
    "        # Pass through the additional embedding layers\n",
    "        x = self.embedding_layer_0(x)  # BatchNorm1d for 1024\n",
    "        # print(f\"output shape of embedding_layer_0: {x.shape}\")\n",
    "        \n",
    "        x = self.embedding_layer_2(x)  # Linear transformation to 256\n",
    "        # print(f\"output shape of embedding_layer_2: {x.shape}\")\n",
    "        \n",
    "        x = self.embedding_layer_3(x)  # BatchNorm1d for 256\n",
    "        # print(f\"output shape of embedding_layer_3: {x.shape}\")\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_into_custom_vit_ssl(checkpoint_state_dict, custom_vit, patch_size=16):\n",
    "    \"\"\"\n",
    "    Loads a checkpoint's state_dict into a custom Vision Transformer (ViT) model.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_state_dict (dict): The state_dict from the checkpoint.\n",
    "        vit_model (torch.nn.Module): The base Vision Transformer model to adjust positional embeddings.\n",
    "        custom_vit (torch.nn.Module): The custom ViT model with backbone and embedding layers.\n",
    "        patch_size (int): Patch size for the ViT model, used for positional embedding interpolation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Separate backbone and embedding layers\n",
    "    backbone_state_dict = {k: v for k, v in checkpoint_state_dict.items() if \"embedding_layer\" not in k}\n",
    "    filtered_backbone_state_dict = {\n",
    "        k: v for k, v in backbone_state_dict.items() \n",
    "        if \"ls1.gamma\" not in k and \"ls2.gamma\" not in k\n",
    "    }\n",
    "\n",
    "    interpolated_backbone_state_dict = interpolate_positional_embedding(filtered_backbone_state_dict, custom_vit.base_vit, patch_size)\n",
    "\n",
    "    # Replace the key in the state_dict\n",
    "    filtered_backbone_state_dict[\"pos_embed\"] = interpolated_backbone_state_dict[\"pos_embed\"]\n",
    "\n",
    "    # Load weights into the backbone (base_vit)\n",
    "    custom_vit.base_vit.load_state_dict(filtered_backbone_state_dict, strict=False)\n",
    "\n",
    "    # Extract embedding layer weights\n",
    "    embedding_layer_state_dict = {k: v for k, v in checkpoint_state_dict.items() if \"embedding_layer\" in k}\n",
    "\n",
    "    # Load weights into the embedding layers\n",
    "    custom_vit.embedding_layer_0.load_state_dict({\n",
    "        \"weight\": embedding_layer_state_dict[\"embedding_layer.0.weight\"],\n",
    "        \"bias\": embedding_layer_state_dict[\"embedding_layer.0.bias\"],\n",
    "        \"running_mean\": embedding_layer_state_dict[\"embedding_layer.0.running_mean\"],\n",
    "        \"running_var\": embedding_layer_state_dict[\"embedding_layer.0.running_var\"],\n",
    "        \"num_batches_tracked\": embedding_layer_state_dict[\"embedding_layer.0.num_batches_tracked\"],\n",
    "    })\n",
    "    custom_vit.embedding_layer_2.load_state_dict({\n",
    "        \"weight\": embedding_layer_state_dict[\"embedding_layer.2.weight\"],\n",
    "        \"bias\": embedding_layer_state_dict[\"embedding_layer.2.bias\"],\n",
    "    })\n",
    "    custom_vit.embedding_layer_3.load_state_dict({\n",
    "        \"weight\": embedding_layer_state_dict[\"embedding_layer.3.weight\"],\n",
    "        \"bias\": embedding_layer_state_dict[\"embedding_layer.3.bias\"],\n",
    "        \"running_mean\": embedding_layer_state_dict[\"embedding_layer.3.running_mean\"],\n",
    "        \"running_var\": embedding_layer_state_dict[\"embedding_layer.3.running_var\"],\n",
    "        \"num_batches_tracked\": embedding_layer_state_dict[\"embedding_layer.3.num_batches_tracked\"],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create custom models and load the checkpoints' state_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_vit_joscha = create_model(\"vit_large_patch14_dinov2\", pretrained=False, img_size=img_size)\n",
    "base_vit_vincent = create_model(\"vit_large_patch14_dinov2\", pretrained=False, img_size=img_size)\n",
    "base_vit_robert = create_model(\"vit_large_patch14_dinov2\", pretrained=False, img_size=img_size)\n",
    "base_vit_best = create_model(\"vit_large_patch14_dinov2\", pretrained=False, img_size=img_size)\n",
    "\n",
    "custom_vit_joscha = CustomVisionTransformer_ssl(base_vit_joscha)\n",
    "custom_vit_vincent = CustomVisionTransformer_ssl(base_vit_vincent)\n",
    "custom_vit_robert = CustomViT_supervised(base_vit_robert)\n",
    "custom_vit_best = CustomViT_supervised(base_vit_robert)\n",
    "\n",
    "### load the checkpoints' state_dict into the custom_vit\n",
    "load_checkpoint_into_custom_vit_ssl(cleaned_state_dict_joscha, custom_vit_joscha, patch_size=16)\n",
    "load_checkpoint_into_custom_vit_ssl(cleaned_state_dict_vincent, custom_vit_vincent, patch_size=16)\n",
    "load_checkpoint_into_custom_vit_supervised(cleaned_state_dict_robert, custom_vit_robert, patch_size=16)\n",
    "load_checkpoint_into_custom_vit_supervised(cleaned_state_dict_best, custom_vit_best, patch_size=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape of base_vit: torch.Size([153, 257, 1024])\n",
      "output shape of base_vit: torch.Size([153, 257, 1024])\n",
      "Output shape of base_vit: torch.Size([153, 257, 1024])\n",
      "Output shape of base_vit: torch.Size([153, 257, 1024])\n",
      "vit_large_dinov2_ssl_joscha with custom model:\n",
      "ViT Accuracy with 5-nearest neighbors: 0.3595, F1_score: 0.204865\n",
      "supervised_dinov2_large with custom model:\n",
      "ViT Accuracy with 5-nearest neighbors: 0.8105, F1_score: 0.641030\n",
      "ssl_vincent_vit_large with custom model:\n",
      "ViT Accuracy with 5-nearest neighbors: 0.4510, F1_score: 0.261567\n",
      "best-model_vit_large_dinoV2 with custom model:\n",
      "ViT Accuracy with 5-nearest neighbors: 0.8039, F1_score: 0.631577\n",
      "best-model_vit_large_dinoV2 in timmWrapper with custom model:\n",
      "ViT Accuracy with 5-nearest neighbors: 0.8039, F1_score: 0.631577\n"
     ]
    }
   ],
   "source": [
    "### generate embeddings from the custom_vit & KNN classification\n",
    "custom_vit_joscha.eval()\n",
    "custom_vit_vincent.eval()\n",
    "custom_vit_robert.eval()\n",
    "custom_vit_best.eval()\n",
    "model_wrapper.eval()\n",
    "\n",
    "\n",
    "# create corresponding image tensors (224 instead of 192)\n",
    "# test_images, test_labels = load_images_and_labels(test_folder, transform_standard)\n",
    "# test_images, test_labels = load_images_and_labels(bristol_folder, transform_standard)\n",
    "\n",
    "# test_images_tensor = torch.stack(test_images)\n",
    "\n",
    "custom_embeddings_joscha = generate_embeddings(custom_vit_joscha, test_images_tensor)\n",
    "custom_embeddings_vincent = generate_embeddings(custom_vit_vincent, test_images_tensor)\n",
    "custom_embeddings_robert = generate_embeddings(custom_vit_robert, test_images_tensor)\n",
    "custom_embeddings_best = generate_embeddings(custom_vit_best, test_images_tensor)\n",
    "custom_embeddings_wrapper = generate_embeddings(model_wrapper, test_images_tensor)\n",
    "\n",
    "def classify_and_print_custom(model_name, model, embeddings, labels):\n",
    "    print(f\"{model_name} with custom model:\")\n",
    "    knn_classifier(embeddings, labels)\n",
    "    \n",
    "classify_and_print_custom(model_name_joscha, custom_vit_joscha, custom_embeddings_joscha, test_labels)\n",
    "classify_and_print_custom(model_name_robert, custom_vit_robert, custom_embeddings_robert, test_labels)\n",
    "classify_and_print_custom(model_name_vincent, custom_vit_vincent, custom_embeddings_vincent, test_labels)\n",
    "classify_and_print_custom(model_name_best, custom_vit_best, custom_embeddings_best, test_labels)\n",
    "classify_and_print_custom(model_name_wrapper, model_wrapper, custom_embeddings_wrapper, test_labels)\n",
    "# knn_classifier(custom_embeddings_joscha, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images before filtering: 153\n",
      "Number of embeddings after filtering: 144\n",
      "Number of images before filtering: 153\n",
      "Number of embeddings after filtering: 144\n",
      "Number of images before filtering: 153\n",
      "Number of embeddings after filtering: 144\n",
      "Number of images before filtering: 153\n",
      "Number of embeddings after filtering: 144\n",
      "Number of images before filtering: 153\n",
      "Number of embeddings after filtering: 144\n",
      "Evaluate with filtered embeddings\n",
      "vit_large_dinov2_ssl_joscha with custom model:\n",
      "ViT Accuracy with 5-nearest neighbors: 0.3889, F1_score: 0.249918\n",
      "supervised_dinov2_large with custom model:\n",
      "ViT Accuracy with 5-nearest neighbors: 0.8681, F1_score: 0.789346\n",
      "ssl_vincent_vit_large with custom model:\n",
      "ViT Accuracy with 5-nearest neighbors: 0.4792, F1_score: 0.318877\n",
      "best-model_vit_large_dinoV2 with custom model:\n",
      "ViT Accuracy with 5-nearest neighbors: 0.8542, F1_score: 0.770804\n",
      "best-model_vit_large_dinoV2 in timmWrapper with custom model:\n",
      "ViT Accuracy with 5-nearest neighbors: 0.8542, F1_score: 0.770804\n"
     ]
    }
   ],
   "source": [
    "# evaluate with filtered embeddings\n",
    "\n",
    "filtered_embeddings_joscha, filtered_labels_joscha = filter_samples_by_threshold(custom_embeddings_joscha, test_labels, threshold=3)\n",
    "filtered_embeddings_robert, filtered_labels_robert = filter_samples_by_threshold(custom_embeddings_robert, test_labels, threshold=3)\n",
    "filtered_embeddings_vincent, filtered_labels_vincent = filter_samples_by_threshold(custom_embeddings_vincent, test_labels, threshold=3)\n",
    "filtered_embeddings_best, filtered_labels_best = filter_samples_by_threshold(custom_embeddings_best, test_labels, threshold=3)\n",
    "filtered_embeddings_wrapper, filtered_labels_wrapper = filter_samples_by_threshold(custom_embeddings_wrapper, test_labels, threshold=3)\n",
    "\n",
    "print(\"Evaluate with filtered embeddings\")\n",
    "classify_and_print_custom(model_name_joscha, custom_vit_joscha, filtered_embeddings_joscha, filtered_labels_joscha)\n",
    "classify_and_print_custom(model_name_robert, custom_vit_robert, filtered_embeddings_robert, filtered_labels_robert)\n",
    "classify_and_print_custom(model_name_vincent, custom_vit_vincent, filtered_embeddings_vincent, filtered_labels_vincent)\n",
    "classify_and_print_custom(model_name_best, custom_vit_best, filtered_embeddings_best, filtered_labels_best)\n",
    "classify_and_print_custom(model_name_wrapper, model_wrapper, filtered_embeddings_wrapper, filtered_labels_wrapper)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
