{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joscha Model (VIT Large Dinov2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Easy debug: pass in a few images adn check the output (embeddings + classification)\n",
    "\n",
    "2. Check which layers are added to the normal vit model (instead of removing weight params)\n",
    "\n",
    "3. datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from timm import create_model\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import torch\n",
    "from timm import create_model\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to model and data\n",
    "\n",
    "joscha_checkpoint_path = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/models/vit_large_dinov2_ssl_joscha.ckpt\"\n",
    "robert_checkpoint_path = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/models/supervised_dinov2_large.ckpt\"\n",
    "vincent_checkpoint_path = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/models/ssl_vincent_vit_large.ckpt\"\n",
    "\n",
    "#checkpoint_path = robert_checkpoint_path\n",
    "test_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_all_split_60-25-15/test\"\n",
    "train_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_all_split_60-25-15/train\"\n",
    "val_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_all_split_60-25-15/val\"\n",
    "\n",
    "all_folder = \"/workspaces/gorilla_watch/video_data/gorillawatch/gorillatracker/datasets/cxl_all_face\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load checkpoints (vit_large_dinov2_ssl_joscha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### inspect the checkpoints\n",
    "\n",
    "# checkpoint_path = joscha_checkpoint_path\n",
    "checkpoint_path = robert_checkpoint_path\n",
    "# checkpoint_path = vincent_checkpoint_path\n",
    "\n",
    "# Load checkpoint and extract state_dict\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda'))\n",
    "# print(checkpoint[\"hyper_parameters\"])\n",
    "# print(checkpoint[\"hparams_name\"])\n",
    "# print(checkpoint[\"state_dict\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compare checkpoints' state_dict with a standard model's state_dict\n",
    "\n",
    "\n",
    "# extract the state_dict from checkpoints\n",
    "state_dict = checkpoint.get('state_dict', checkpoint)  # Get 'state_dict' or use checkpoint directly if no wrapper exists\n",
    "new_state_dict = {k.replace('model_wrapper.', ''): v for k, v in state_dict.items()}\n",
    "new_state_dict2 = {k.replace('model.', ''): v for k, v in new_state_dict.items()}\n",
    "\n",
    "# Initialize the ViT model\n",
    "vit_model = create_model('vit_large_patch14_dinov2.lvd142m', pretrained=False, img_size=192)\n",
    "\n",
    "# Standard ViT model keys\n",
    "standard_vit_keys = vit_model.state_dict().keys()\n",
    "\n",
    "# Compare keys\n",
    "custom_keys = new_state_dict2.keys()\n",
    "missing_in_standard = [k for k in custom_keys if k not in standard_vit_keys]\n",
    "extra_in_standard = [k for k in standard_vit_keys if k not in custom_keys]\n",
    "\n",
    "# print(\"Missing keys in standard ViT:\", missing_in_standard)\n",
    "# print(\"Extra keys in standard ViT:\", extra_in_standard)\n",
    "\n",
    "# print(new_state_dict2)\n",
    "\n",
    "for key in missing_in_standard:\n",
    "    print(f\"{key}: {new_state_dict2[key].shape}\")\n",
    "    \n",
    "embedding_keys = [key for key in new_state_dict2.keys() if \"embedding_layer\" in key]\n",
    "# print(embedding_keys)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix pos_emd mismatch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def interpolate_positional_embedding(pretrained_state_dict, current_model, patch_size):\n",
    "    \"\"\"\n",
    "    Adjusts the positional embeddings from the pretrained model to fit the current model.\n",
    "\n",
    "    Args:\n",
    "        pretrained_state_dict (dict): State dictionary of the pretrained model.\n",
    "        current_model (torch.nn.Module): The current Vision Transformer model.\n",
    "        patch_size (int): Patch size of the ViT model.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state dictionary compatible with the current model.\n",
    "    \"\"\"\n",
    "    # Get the pretrained positional embeddings\n",
    "    pretrained_pos_embed = pretrained_state_dict['pos_embed']\n",
    "    current_pos_embed = current_model.state_dict()['pos_embed']\n",
    "    \n",
    "    # Exclude the CLS token (first token) if present\n",
    "    cls_token = pretrained_pos_embed[:, :1, :]\n",
    "    pretrained_grid = pretrained_pos_embed[:, 1:, :]\n",
    "    \n",
    "    # Get the grid dimensions\n",
    "    num_patches = current_pos_embed.shape[1] - 1  # Exclude CLS token\n",
    "    grid_size_pretrained = int((pretrained_grid.shape[1])**0.5)  # sqrt(num_patches)\n",
    "    grid_size_current = int(num_patches**0.5)\n",
    "\n",
    "    # Reshape the positional embeddings to a grid\n",
    "    pretrained_grid = pretrained_grid.reshape(1, grid_size_pretrained, grid_size_pretrained, -1).permute(0, 3, 1, 2)\n",
    "    \n",
    "    # Interpolate to the current grid size\n",
    "    interpolated_grid = F.interpolate(pretrained_grid, size=(grid_size_current, grid_size_current), mode='bilinear', align_corners=False)\n",
    "    interpolated_grid = interpolated_grid.permute(0, 2, 3, 1).reshape(1, -1, interpolated_grid.shape[1])\n",
    "    \n",
    "    # Combine CLS token and the new grid\n",
    "    new_pos_embed = torch.cat([cls_token, interpolated_grid], dim=1)\n",
    "    pretrained_state_dict['pos_embed'] = new_pos_embed\n",
    "\n",
    "    return pretrained_state_dict\n",
    "\n",
    "\n",
    "interpolate_positional_embedding(new_state_dict2, vit_model, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model supervised_dinov2_large into vit \n",
    "\n",
    "vit_model_robert = create_model('vit_large_patch14_dinov2.lvd142m', pretrained=False, img_size=192)\n",
    "\n",
    "vit_model_robert.load_state_dict(new_state_dict2, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the vit model with an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the VIT model with custom layers (vit_large_dinov2_ssl_joscha)\n",
    "\n",
    "class CustomVisionTransformer(nn.Module):\n",
    "    def __init__(self, base_vit):\n",
    "        super(CustomVisionTransformer, self).__init__()\n",
    "        self.base_vit = base_vit\n",
    "        \n",
    "        # Define embedding layers based on checkpoint dimensions\n",
    "        self.embedding_layer_0 = nn.BatchNorm1d(1024)  # Normalize input size 1024\n",
    "        self.embedding_layer_2 = nn.Linear(1024, 256)  # Linear layer: 1024 -> 256\n",
    "        self.embedding_layer_3 = nn.BatchNorm1d(256)  # Normalize input size 256\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the Vision Transformer backbone\n",
    "        x = self.base_vit.forward_features(x)  # Output size is [batch_size, 1024]\n",
    "        print(f\"output shape of base_vit: {x.shape}\")\n",
    "        \n",
    "        # Flatten the patch and token dimensions into batch\n",
    "        # x = x.view(-1, x.size(-1))  # Shape: [batch_size * 257, 1024]\n",
    "        x = x[:, 0, :]  # Shape: [batch_size, 1024]\n",
    "\n",
    "        \n",
    "        # Pass through the additional embedding layers\n",
    "        x = self.embedding_layer_0(x)  # BatchNorm1d for 1024\n",
    "        print(f\"output shape of embedding_layer_0: {x.shape}\")\n",
    "        x = self.embedding_layer_2(x)  # Linear transformation to 256\n",
    "        print(f\"output shape of embedding_layer_2: {x.shape}\")\n",
    "        x = self.embedding_layer_3(x)  # BatchNorm1d for 256\n",
    "        print(f\"output shape of embedding_layer_3: {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the base ViT model\n",
    "base_vit = create_model('vit_large_patch14_224', pretrained=False)\n",
    "\n",
    "# Wrap the base ViT model with the custom embedding layers\n",
    "custom_vit = CustomVisionTransformer(base_vit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the checkpoints' state_dict into the custom_vit \n",
    "\n",
    "# # Adjust the keys\n",
    "# adjusted_state_dict = {k.replace(\"model_wrapper.\", \"\"): v for k, v in state_dict.items()}\n",
    "# backbone_keys_adjusted = {k.replace(\"model.\", \"\"): v for k, v in adjusted_state_dict.items()}\n",
    "\n",
    "# # Separate backbone and embedding layers\n",
    "# backbone_state_dict = {k: v for k, v in backbone_keys_adjusted.items() if \"embedding_layer\" not in k}\n",
    "# filtered_backbone_state_dict = {k: v for k, v in backbone_state_dict.items() if \"ls1.gamma\" not in k and \"ls2.gamma\" not in k}\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # Resize the positional embeddings\n",
    "# checkpoint_pos_embed = backbone_state_dict[\"pos_embed\"]\n",
    "# checkpoint_pos_embed = checkpoint_pos_embed.view(1, 170, -1)  # Flatten\n",
    "# new_pos_embed = F.interpolate(checkpoint_pos_embed.permute(0, 2, 1), size=257, mode='linear')  # Interpolate\n",
    "# new_pos_embed = new_pos_embed.permute(0, 2, 1).view(1, 257, -1)\n",
    "\n",
    "# # Replace the key in the state_dict\n",
    "# filtered_backbone_state_dict[\"pos_embed\"] = new_pos_embed\n",
    "\n",
    "# embedding_layer_state_dict = {k: v for k, v in backbone_keys_adjusted.items() if \"embedding_layer\" in k}\n",
    "\n",
    "\n",
    "# # Load weights into the backbone (base_vit)\n",
    "# custom_vit.base_vit.load_state_dict(filtered_backbone_state_dict, strict=False)\n",
    "\n",
    "# # Load weights into the embedding layers\n",
    "# custom_vit.embedding_layer_0.load_state_dict({\n",
    "#     \"weight\": embedding_layer_state_dict[\"embedding_layer.0.weight\"],\n",
    "#     \"bias\": embedding_layer_state_dict[\"embedding_layer.0.bias\"],\n",
    "#     \"running_mean\": embedding_layer_state_dict[\"embedding_layer.0.running_mean\"],\n",
    "#     \"running_var\": embedding_layer_state_dict[\"embedding_layer.0.running_var\"],\n",
    "#     \"num_batches_tracked\": embedding_layer_state_dict[\"embedding_layer.0.num_batches_tracked\"],\n",
    "# })\n",
    "# custom_vit.embedding_layer_2.load_state_dict({\n",
    "#     \"weight\": embedding_layer_state_dict[\"embedding_layer.2.weight\"],\n",
    "#     \"bias\": embedding_layer_state_dict[\"embedding_layer.2.bias\"],\n",
    "# })\n",
    "# custom_vit.embedding_layer_3.load_state_dict({\n",
    "#     \"weight\": embedding_layer_state_dict[\"embedding_layer.3.weight\"],\n",
    "#     \"bias\": embedding_layer_state_dict[\"embedding_layer.3.bias\"],\n",
    "#     \"running_mean\": embedding_layer_state_dict[\"embedding_layer.3.running_mean\"],\n",
    "#     \"running_var\": embedding_layer_state_dict[\"embedding_layer.3.running_var\"],\n",
    "#     \"num_batches_tracked\": embedding_layer_state_dict[\"embedding_layer.3.num_batches_tracked\"],\n",
    "# })\n",
    "\n",
    "# # custom_vit.to(device)\n",
    "# custom_vit.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Max' approach to loading the state_dict\n",
    "\n",
    "# # Adjust the keys if necessary (remove any prefix like 'model.')\n",
    "# new_state_dict = {k.replace('model_wrapper.', ''): v for k, v in state_dict.items()}\n",
    "# new_state_dict2 = {k.replace('model.', ''): v for k, v in new_state_dict.items()}\n",
    "\n",
    "# # Filter out unexpected keys from the state_dict\n",
    "# model_keys = set(vit_model.state_dict().keys())\n",
    "# filtered_state_dict = {k: v for k, v in new_state_dict2.items() if k in model_keys}\n",
    "\n",
    "# # Load the filtered state_dict into the model\n",
    "# vit_model.load_state_dict(filtered_state_dict, strict=True)\n",
    "# vit_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# ### MAx' approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# Preprocessing function to resize and normalize images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Adjust based on model pretraining\n",
    "])\n",
    "\n",
    "# Load images and extract class codes from file names\n",
    "def load_images_and_labels(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            image = cv2.imread(os.path.join(folder, filename))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = transform(image)\n",
    "            images.append(image)\n",
    "            labels.append(filename[:4])  # Assuming first 4 chars are label\n",
    "            \n",
    "            filenames.append(filename)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Load data\n",
    "test_images, test_labels = load_images_and_labels(test_folder)\n",
    "# train_images, train_labels = load_images_and_labels(train_folder)\n",
    "# val_images, val_labels = load_images_and_labels(val_folder)\n",
    "# all_images, all_labels = load_images_and_labels(all_folder)\n",
    "\n",
    "test_images_tensor = torch.stack(test_images)\n",
    "# train_images_tensor = torch.stack(train_images)\n",
    "# val_images_tensor = torch.stack(val_images)\n",
    "# all_images_tensor = torch.stack(all_images)\n",
    "\n",
    "# train_val_images_tensor = torch.cat((train_images_tensor, val_images_tensor), 0)\n",
    "# train_val_labels = train_labels + val_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robert Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings from a model\n",
    "def generate_embeddings(model, images_tensor):\n",
    "    with torch.no_grad():\n",
    "        # Call the forward method to extract embeddings\n",
    "        embeddings = model(images_tensor)\n",
    "    return embeddings\n",
    "\n",
    "# test_images_tensor = test_images_tensor.to(device)\n",
    "# all_images_tensor = all_images_tensor.to(device)\n",
    "\n",
    "# Generate and flatten embeddings for ViT\n",
    "test_vit_embeddings = generate_embeddings(custom_vit, test_images_tensor)\n",
    "# train_val_vit_embeddings = generate_embeddings(custom_vit, train_val_images_tensor)\n",
    "# all_vit_embeddings = generate_embeddings(custom_vit, all_images_tensor)\n",
    "\n",
    "# Flatten the embeddings\n",
    "test_vit_embeddings_flat = test_vit_embeddings.view(test_vit_embeddings.size(0), -1).numpy()\n",
    "# train_val_vit_embeddings_flat = train_val_vit_embeddings.view(train_val_vit_embeddings.size(0), -1).numpy()\n",
    "# all_vit_embeddings_flat = all_vit_embeddings.view(all_vit_embeddings.size(0), -1).numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# Preprocessing function to resize and normalize images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((192, 192)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Adjust based on model pretraining\n",
    "])\n",
    "\n",
    "# Load images and extract class codes from file names\n",
    "def load_images_and_labels(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            image = cv2.imread(os.path.join(folder, filename))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = transform(image)\n",
    "            images.append(image)\n",
    "            labels.append(filename[:4])  # Assuming first 4 chars are label\n",
    "            \n",
    "            filenames.append(filename)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Load data\n",
    "test_images, test_labels = load_images_and_labels(test_folder)\n",
    "# train_images, train_labels = load_images_and_labels(train_folder)\n",
    "# val_images, val_labels = load_images_and_labels(val_folder)\n",
    "# all_images, all_labels = load_images_and_labels(all_folder)\n",
    "\n",
    "test_images_tensor = torch.stack(test_images)\n",
    "# train_images_tensor = torch.stack(train_images)\n",
    "# val_images_tensor = torch.stack(val_images)\n",
    "# all_images_tensor = torch.stack(all_images)\n",
    "\n",
    "# train_val_images_tensor = torch.cat((train_images_tensor, val_images_tensor), 0)\n",
    "# train_val_labels = train_labels + val_labels\n",
    "\n",
    "\n",
    "robert_test_embeddings = generate_embeddings(vit_model_robert, test_images_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try reducing smaples with insufficient data\n",
    "\n",
    "# from collections import Counter\n",
    "# threshold = 3\n",
    "\n",
    "# # Count occurrences of each label\n",
    "# label_counts = Counter(test_labels)\n",
    "# print(f\"Number of images before filtering: {len(test_labels)}\")\n",
    "\n",
    "# # \n",
    "# # Filter embeddings and labels for individuals with >= 3 samples\n",
    "# filtered_indices = [i for i, label in enumerate(test_labels) if label_counts[label] >= threshold]\n",
    "# test_vit_embeddings_filtered = test_vit_embeddings_flat[filtered_indices]\n",
    "# test_labels_filtered = [test_labels[i] for i in filtered_indices]\n",
    "\n",
    "# # check the number of images after filtering\n",
    "# print(f\"Number of images after filtering: {len(test_labels_filtered)}\")\n",
    "# print(f\"Number of images after filtering: {len(test_vit_embeddings_filtered)}\")\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def filter_samples_by_threshold(embeddings, labels, threshold=3):\n",
    "    # Count occurrences of each label\n",
    "    label_counts = Counter(labels)\n",
    "    print(f\"Number of images before filtering: {len(labels)}\")\n",
    "\n",
    "    # Filter embeddings and labels for labels with >= threshold occurrences\n",
    "    filtered_indices = [i for i, label in enumerate(labels) if label_counts[label] >= threshold]\n",
    "    filtered_embeddings = embeddings[filtered_indices]\n",
    "    filtered_labels = [labels[i] for i in filtered_indices]\n",
    "\n",
    "    # Log results\n",
    "    print(f\"Number of images after filtering: {len(filtered_labels)}\")\n",
    "    print(f\"Number of embeddings after filtering: {len(filtered_embeddings)}\")\n",
    "\n",
    "    return filtered_embeddings, filtered_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN classifier and excluding the closest neighbor (self-matching)\n",
    "def knn_classifier(embeddings, labels):\n",
    "    for k in range(1, 6):\n",
    "        # Use k+1 neighbors to account for excluding self\n",
    "        vit_knn = KNeighborsClassifier(n_neighbors=k + 1)\n",
    "        vit_knn.fit(embeddings, labels)  # Fit KNN on all data\n",
    "\n",
    "        vit_y_pred = []\n",
    "        for idx, test_embedding in enumerate(embeddings):\n",
    "            # Find k+1 neighbors (including the test sample itself)\n",
    "            neighbors = vit_knn.kneighbors(test_embedding.reshape(1, -1), return_distance=False)[0]\n",
    "\n",
    "            # Exclude the test sample itself (assumed to be the closest neighbor)\n",
    "            filtered_neighbors = neighbors[1:]  # Exclude the first neighbor\n",
    "        \n",
    "            # Predict based on the remaining neighbors (majority vote)\n",
    "            filtered_neighbor_labels = [labels[n] for n in filtered_neighbors]\n",
    "            predicted_label = max(set(filtered_neighbor_labels), key=filtered_neighbor_labels.count)\n",
    "            vit_y_pred.append(predicted_label)\n",
    "\n",
    "        # Calculate accuracy for the current k\n",
    "        vit_accuracy = accuracy_score(labels, vit_y_pred)\n",
    "        print(f\"ViT Accuracy with {k}-nearest neighbors (excluding self): {vit_accuracy:.4f}\")\n",
    "        \n",
    "\n",
    "# knn_classifier(all_vit_embeddings_reduced, all_labels, test_vit_embeddings_reduced, test_labels)\n",
    "# knn_classifier(all_vit_embeddings_filtered, all_labels_filtered, test_vit_embeddings_flat, test_labels)\n",
    "print(\"all test samples: joscha\")\n",
    "knn_classifier(test_vit_embeddings_flat, test_labels)\n",
    "print(\"all test samples: robert\")\n",
    "knn_classifier(robert_test_embeddings, test_labels)\n",
    "\n",
    "threshold = 3\n",
    "test_vit_embeddings_filtered, test_labels_filtered = filter_samples_by_threshold(\n",
    "    test_vit_embeddings_flat,\n",
    "    test_labels,\n",
    "    threshold=3\n",
    ")\n",
    "print(f\"filtered test samples (threshold = {threshold}):joscha\")\n",
    "knn_classifier(test_vit_embeddings_filtered, test_labels_filtered)\n",
    "\n",
    "robert_embeddings_filtered, robert_labels_filtered = filter_samples_by_threshold(\n",
    "    robert_test_embeddings,\n",
    "    test_labels,\n",
    "    threshold=3\n",
    ")\n",
    "print(f\"filtered test samples (threshold = {threshold}):robert\")\n",
    "knn_classifier(robert_embeddings_filtered, robert_labels_filtered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all test samples: joscha\n",
    "ViT Accuracy with 1-nearest neighbors (excluding self): 0.3704\n",
    "ViT Accuracy with 2-nearest neighbors (excluding self): 0.2963\n",
    "ViT Accuracy with 3-nearest neighbors (excluding self): 0.2222\n",
    "ViT Accuracy with 4-nearest neighbors (excluding self): 0.2407\n",
    "ViT Accuracy with 5-nearest neighbors (excluding self): 0.2222\n",
    "all test samples: robert\n",
    "ViT Accuracy with 1-nearest neighbors (excluding self): 0.8704\n",
    "ViT Accuracy with 2-nearest neighbors (excluding self): 0.7778\n",
    "ViT Accuracy with 3-nearest neighbors (excluding self): 0.7222\n",
    "ViT Accuracy with 4-nearest neighbors (excluding self): 0.6852\n",
    "ViT Accuracy with 5-nearest neighbors (excluding self): 0.6852\n",
    "Number of images before filtering: 54\n",
    "Number of images after filtering: 40\n",
    "Number of embeddings after filtering: 40\n",
    "filtered test samples (threshold = 3):joscha\n",
    "ViT Accuracy with 1-nearest neighbors (excluding self): 0.4000\n",
    "ViT Accuracy with 2-nearest neighbors (excluding self): 0.3250\n",
    "ViT Accuracy with 3-nearest neighbors (excluding self): 0.2750\n",
    "ViT Accuracy with 4-nearest neighbors (excluding self): 0.3250\n",
    "ViT Accuracy with 5-nearest neighbors (excluding self): 0.2500\n",
    "Number of images before filtering: 54\n",
    "Number of images after filtering: 40\n",
    "Number of embeddings after filtering: 40\n",
    "filtered test samples (threshold = 3):robert\n",
    "ViT Accuracy with 1-nearest neighbors (excluding self): 0.9750\n",
    "ViT Accuracy with 2-nearest neighbors (excluding self): 0.9500\n",
    "ViT Accuracy with 3-nearest neighbors (excluding self): 0.9750\n",
    "ViT Accuracy with 4-nearest neighbors (excluding self): 0.9250\n",
    "ViT Accuracy with 5-nearest neighbors (excluding self): 0.8750\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letztes mal:\n",
    "\n",
    "ViT Accuracy: 0.6667 (Mit test_size 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `all_vit_embeddings_flat` contains all embeddings\n",
    "# and `all_labels` contains corresponding labels\n",
    "\n",
    "# Using t-SNE to reduce dimensionality to 2D for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "# reduced_embeddings_tsne = tsne.fit_transform(all_vit_embeddings_flat)\n",
    "reduced_embeddings_tsne = tsne.fit_transform(test_vit_embeddings_flat)\n",
    "\n",
    "# Convert labels to numeric values for coloring\n",
    "unique_labels = list(set(test_labels))\n",
    "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "numeric_labels = [label_to_index[label] for label in test_labels]\n",
    "\n",
    "# Visualizing the reduced embeddings with a scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    reduced_embeddings_tsne[:, 0],\n",
    "    reduced_embeddings_tsne[:, 1],\n",
    "    c=numeric_labels,\n",
    "    cmap=\"tab10\",\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"k\"\n",
    ")\n",
    "plt.colorbar(scatter, ticks=range(len(unique_labels)), label=\"Classes\")\n",
    "plt.title(\"t-SNE Visualization of Embeddings\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
